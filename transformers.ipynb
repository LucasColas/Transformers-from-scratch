{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/Transformers-from-scratch/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        " Machine translation\n",
        "This project is originally from a course at Polytechnique Montr√©al called INF8225.\n",
        "The goal is to build a machine translation. Several architectures are used : RNN, GRU and Transformers.\n",
        "Do not forget to **select the runtime type as GPU!**\n",
        "\n",
        "**Sources**\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
        "\n",
        "*This notebook is quite big, use the table of contents to easily navigate through it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vLbVbH4lu4J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to use older versions of torchtext. Hence we need to install older versions of libraries."
      ],
      "metadata": {
        "id": "YVV4z5sSbzgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb > /dev/null\n",
        "import wandb"
      ],
      "metadata": {
        "id": "m27nUzQBthMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU2Ap1Vptj57",
        "outputId": "3c78918e-306a-4116-f19e-0af0e1841910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to run one of the two following cells. After you ran one of them, restart your session and run the same cell again. Then try to run the cell for the imports."
      ],
      "metadata": {
        "id": "6KIJl9afb9Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir > /dev/null # Downgrade numpy first\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "# Reinstall scipy and scikit-learn\n",
        "!pip install scipy --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install scikit-learn --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "!python3 -m spacy download en > /dev/null\n",
        "!python3 -m spacy download fr > /dev/null\n",
        "!pip install torchinfo > /dev/null\n",
        "!pip install einops > /dev/null\n",
        "!pip install wandb > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePmTpM8Ib5B9",
        "outputId": "cca6c731-6b58-4c76-91d2-3facb90623c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP\n",
        "\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "\n",
        "!python3 -m spacy download en > /dev/null\n",
        "!python3 -m spacy download fr > /dev/null\n",
        "!pip install torchinfo > /dev/null\n",
        "!pip install einops > /dev/null\n",
        "!pip install wandb > /dev/null\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxQLxOjRb1KY",
        "outputId": "6ec51350-f528-4715-841d-36f12f9af1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.40.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.40.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### !pip install --upgrade --force-reinstall scipy scikit-learn"
      ],
      "metadata": {
        "id": "qcfM-DOEvlw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJQfREvFUdoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aaa7ea0-17f0-4951-86fc-6cf72abcc3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "# cpal\n",
        "print(torch.__version__)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxNpMbkvUfGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58676897-f363-4be0-c1e7-5ebc327982c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 12:35:46--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‚Äòfra-eng.zip‚Äô\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  4.29MB/s    in 1.8s    \n",
            "\n",
            "2025-03-23 12:35:48 (4.29 MB/s) - ‚Äòfra-eng.zip‚Äô saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "209462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"fr\" could not be loaded, trying \"fr_core_news_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "\n",
        "en_tokenizer, fr_tokenizer = get_tokenizer('spacy', language='en'), get_tokenizer('spacy', language='fr')\n",
        "\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the install doesn't work, try this :\n",
        "Run the next cell and restart your session. Run again this cell after the restart of your session."
      ],
      "metadata": {
        "id": "6tSCXVB9bmw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install scikit-learn==1.1.3 --force-reinstall --no-cache-dir\n",
        "!pip install scipy==1.9.3 --force-reinstall --no-cache-dir !pip install spacy einops wandb torchinfo\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "YCRDEUTfbp-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdictimport numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pdimport torch\n",
        "# cpal\n",
        "print(torch.__version__)import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequenceimport torchtext\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016import spacyimport einops\n",
        "import wandb\n",
        "from torchinfo import summaryDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ViT75IqHbq_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zipdf = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))en_nlp = spacy.load('en_core_web_sm')\n",
        "fr_nlp = spacy.load('fr_core_news_sm')def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in en_nlp.tokenizer(text)]def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in fr_nlp.tokenizer(text)]SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "qRxfBCeubxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ],
      "metadata": {
        "id": "ppPj9CrnsSoW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "\n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "# Models architecture\n",
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse.\n",
        "\n",
        "---\n",
        "\n",
        "In pytorch, there is no dinstinction between an intermediate layer or a whole model having multiple layers in itself.\n",
        "Every layers or models inherit from the `torch.nn.Module`.\n",
        "This module needs to define the `__init__` method where you instanciate the layers,\n",
        "and the `forward` method where you decide how the inputs and the layers of the module interact between them.\n",
        "Thanks to the autograd computations of pytorch, you do not have\n",
        "to implement any backward method!\n",
        "\n",
        "A really important advice is to **always look at\n",
        "the shape of your input and your output.**\n",
        "From that, you can often guess how the layers should interact\n",
        "with the inputs to produce the right output.\n",
        "You can also easily detect if there's something wrong going on.\n",
        "\n",
        "You are more than advised to use the `einops` library and the `torch.einsum` function. This will require less operations than 'classical' code, but note that it's a bit trickier to use.\n",
        "This is a way of describing tensors manipulation with strings, bypassing the multiple tensor methods executed in the background.\n",
        "You can find a nice presentation of `einops` [here](https://einops.rocks/1-einops-basics/).\n",
        "A paper has just been released about einops [here](https://paperswithcode.com/paper/einops-clear-and-reliable-tensor).\n",
        "\n",
        "**A great tutorial on pytorch can be found [here](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).**\n",
        "Spending 3 hours on this tutorial is *no* waste of time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN models"
      ],
      "metadata": {
        "id": "xodRThXg2DHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN\n",
        "Here, the implementation of the RNN is provided as an example. Study this code and use it as an example for the GRU implementation, if needed.\n",
        "\n",
        "The `RNNCell` layer produce one hidden state vector for each sentence in the batch\n",
        "(useful for the output of the encoder), and also produce one embedding for each\n",
        "token in each sentence (useful for the output of the decoder).\n",
        "\n",
        "The `RNN` module is composed of a stack of `RNNCell`. Each token embeddings\n",
        "coming out from a previous `RNNCell` is used as an input for the next `RNNCell` layer.\n",
        "\n",
        "**Be careful !** Our `RNNCell` implementation is not exactly the same thing as\n",
        "the PyTorch's `nn.RNNCell`. PyTorch implements only the operations for one token\n",
        "(so you would need to loop through each tokens inside the `RNN` instead).\n",
        "\n",
        "The same thing apply for the `GRU` and `GRUCell`.\n"
      ],
      "metadata": {
        "id": "ZvfRVUKm1u8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A single RNN layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "\n",
        "    Important note: This layer does not exactly the same thing as nn.RNNCell does.\n",
        "    PyTorch implementation is only doing one simple pass over one token for each batch.\n",
        "    This implementation is taking the whole sequence of each batch and provide the\n",
        "    final hidden state along with the embeddings of each token in each sequence.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # See pytorch definition: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.Wih = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"Go through all the sequence in x, iteratively updating\n",
        "        the hidden state h.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_size = x.shape\n",
        "        y = torch.zeros([batch_size, seq_len, self.hidden_size], device=DEVICE)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "          input = x[:, t, :]\n",
        "          w_input = self.Wih(input)\n",
        "          w_hidden = self.Whh(h)\n",
        "          h = self.act(w_input + w_hidden)\n",
        "          y[:, t, :] = self.dropout(h)\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"Implementation of an RNN based\n",
        "    on https://pytorch.org/docs/stable/generated/torch.nn.RNN.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers (RNNCell or GRUCell).\n",
        "        dropout: Dropout rate.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "            This parameter can be removed if you decide to use the module `GRU`.\n",
        "            Indeed, `GRU` should have exactly the same code as this module,\n",
        "            but with `GRUCell` instead of `RNNCell`. We let the freedom for you\n",
        "            to decide at which level you want to specialise the modules (either\n",
        "            in `TranslationRNN` by creating a `GRU` or a `RNN`, or in `RNN`\n",
        "            by creating a `GRUCell` or a `RNNCell`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        model_class = RNNCell if model_type == 'RNN' else GRUCell\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(model_class(input_size, hidden_size, dropout))\n",
        "        for i in range(1, num_layers):\n",
        "          self.layers.append(model_class(hidden_size, hidden_size, dropout))\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"Pass the input sequence through all the RNN cells.\n",
        "        Returns the output and the final hidden state of each RNN layer\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Hidden state for each RNN layer.\n",
        "                Can be None, in which case an initial hidden state is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Output embeddings for each token after the RNN layers.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "        \"\"\"\n",
        "        input = x\n",
        "        h = torch.zeros([x.shape[0], len(self.layers), self.hidden_size], device=x.device) if h is None else h\n",
        "        final_h = torch.zeros_like(h, device=x.device)\n",
        "        for l in range(len(self.layers)):\n",
        "          input, h_out = self.layers[l](input, h[:, l, :])\n",
        "          final_h[:, l, :] = h_out\n",
        "\n",
        "        return input, final_h"
      ],
      "metadata": {
        "id": "RiNKnwScM5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "Here you have to implement a GRU-RNN. This architecture is close to the Vanilla RNN but perform different operations. Look up the pytorch documentation to figure out the differences."
      ],
      "metadata": {
        "id": "I0ciaamtvK0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"Implementation of a GRU based on https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        print(\"GRU !!\")\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create multiple GRU layers\n",
        "        self.layers = nn.ModuleList([GRUCell(input_size, hidden_size, dropout)])\n",
        "        self.layers.extend([GRUCell(hidden_size, hidden_size, dropout) for _ in range(1, num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state for each layer.\n",
        "                If 'None', then an initial hidden state (a zero filled tensor)\n",
        "                is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            output:\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h_n: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden size].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size = x.shape[0]\n",
        "        # If h is not provided, initialize with zeros\n",
        "        if h is None:\n",
        "            h = torch.zeros(batch_size, self.num_layers, self.hidden_size, device=x.device)\n",
        "        final_h = torch.zeros_like(h, device=x.device)\n",
        "\n",
        "        input_seq = x\n",
        "        # Pass through each GRU layer\n",
        "        for l, cell in enumerate(self.layers):\n",
        "            input_seq, h_out = cell(input_seq, h[:, l, :])\n",
        "            final_h[:, l, :] = h_out\n",
        "\n",
        "        return input_seq, final_h\n",
        "\n",
        "\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    \"\"\"A single GRU layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        print(\"GRUCell\")\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Gates for update (z) and reset (r)\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "\n",
        "        # Candidate hidden state\n",
        "        self.Win = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        y = torch.zeros(batch_size, seq_len, self.hidden_size, device=x.device)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Compute update and reset gates\n",
        "            z_t = self.sigmoid(self.Wiz(x_t) + self.Whz(h))\n",
        "            r_t = self.sigmoid(self.Wir(x_t) + self.Whr(h))\n",
        "\n",
        "            # Compute candidate hidden state\n",
        "            n_t = self.tanh(self.Win(x_t) + self.Whn(r_t * h))\n",
        "\n",
        "            # Update hidden state\n",
        "            h = (1 - z_t) * n_t + z_t * h\n",
        "\n",
        "            # Apply dropout and store the output for this time step\n",
        "            y[:, t, :] = self.dropout(h)\n",
        "\n",
        "        return y, h\n"
      ],
      "metadata": {
        "id": "xdAMSZ7EMrMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation RNN\n",
        "\n",
        "This module instanciates a vanilla RNN or a GRU-RNN and performs the translation task. This code des the following:\n",
        "* Encodes the source and target sequence\n",
        "* Passes the final hidden state of the encoder to the decoder (one for each layer)\n",
        "* Decodes the hidden state into the target sequence\n",
        "\n",
        "We use teacher forcing for training, meaning that when the next token is predicted, that prediction is based on the previous true target tokens."
      ],
      "metadata": {
        "id": "boIetZUy1f-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD-6N17xhuLy"
      },
      "outputs": [],
      "source": [
        "class TranslationRNN(nn.Module):\n",
        "    \"\"\"Basic RNN encoder and decoder for a translation task.\n",
        "    It can run as a vanilla RNN or a GRU-RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the hidden layers in the RNNs\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the RNNs.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.src_embeddings = nn.Embedding(n_tokens_src, dim_embedding, src_pad_idx)\n",
        "        self.tgt_embeddings = nn.Embedding(n_tokens_tgt, dim_embedding, tgt_pad_idx)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.norm = nn.LayerNorm(dim_hidden)\n",
        "        self.decoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.out_layer = nn.Linear(dim_hidden, n_tokens_tgt)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logits based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, tgt_seq_len, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        source = torch.fliplr(source)\n",
        "\n",
        "        src_emb = self.src_embeddings(source)\n",
        "        out, hidden = self.encoder(src_emb)\n",
        "\n",
        "        hidden = self.norm(hidden)\n",
        "\n",
        "        tgt_emb = self.tgt_embeddings(target)\n",
        "        y, hidden = self.decoder(tgt_emb, hidden)\n",
        "\n",
        "        y = self.out_layer(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XLrMrwpjeOq8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RI_8zCdm4YQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer models\n",
        "Here you have to code the Full Transformer and Decoder-Only Transformer architectures.\n",
        "It is divided in three parts:\n",
        "* Attention layers (done individually)\n",
        "* Encoder and decoder layers (done individually)\n",
        "* Full Transformer: gather the encoder and decoder layers (done individually)\n",
        "\n",
        "The Transformer (or \"Full Transformer\") is presented in the paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ],
      "metadata": {
        "id": "EZcGlRnZvOnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n"
      ],
      "metadata": {
        "id": "ZwaVFTTUlYwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    This PE module comes from:\n",
        "    Pytorch. (2021). LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1).to(DEVICE)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).to(DEVICE)\n",
        "        pe = torch.zeros(max_len, 1, d_model).to(DEVICE)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, \"b s e -> s b e\")\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x = rearrange(x, \"s b e -> b s e\")\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "lIqHye2Vl3gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ],
      "metadata": {
        "id": "OFxV-6M3402p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "\n",
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention between the keys and the queries.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "\n",
        "    head_dim = q.size(-1)\n",
        "    scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(head_dim)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "    output = torch.einsum(\"bhqk,bkhd->bqhd\", attn, v)\n",
        "    return output, attn\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        # TODO\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = dim // n_heads\n",
        "\n",
        "        # Projection layers for queries, keys, values.\n",
        "        self.w_q = nn.Linear(dim, dim)\n",
        "        self.w_k = nn.Linear(dim, dim)\n",
        "        self.w_v = nn.Linear(dim, dim)\n",
        "        # Output projection.\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size, seq_len_q, _ = q.shape\n",
        "        batch_size, seq_len_k, _ = k.shape\n",
        "\n",
        "        # Linear projections and reshape for multiple heads.\n",
        "        q = self.w_q(q)\n",
        "        k = self.w_k(k)\n",
        "        v = self.w_v(v)\n",
        "        # Rearranging: split the last dimension into (n_heads, head_dim)\n",
        "        q = rearrange(q, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "        k = rearrange(k, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "        v = rearrange(v, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "\n",
        "        # Prepare masks if provided.\n",
        "        mask = None\n",
        "        if key_padding_mask is not None:\n",
        "            # Expand mask to shape [batch, 1, 1, seq_len_k]\n",
        "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "        if attn_mask is not None:\n",
        "            # attn_mask: [seq_len_q, seq_len_k] -> [1, 1, seq_len_q, seq_len_k]\n",
        "            attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
        "            mask = attn_mask if mask is None else mask | attn_mask\n",
        "\n",
        "        # Compute attention.\n",
        "        attn_output, attn = attention(q, k, v, mask=mask, dropout=self.attn_dropout)\n",
        "        # Rearranging back to [batch, seq_len_q, dim]\n",
        "        attn_output = rearrange(attn_output, \"b t h d -> b t (h d)\")\n",
        "        output = self.fc(attn_output)\n",
        "        output = self.proj_dropout(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ],
      "metadata": {
        "id": "nIpHjOtK47DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.cross_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Self-attention over tgt (with causal mask)\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, key_padding_mask=tgt_key_padding_mask, attn_mask=tgt_mask_attn)\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        # Cross-attention: query=tgt, key/value=src (encoded source)\n",
        "        tgt2 = self.cross_attn(tgt, src, src, key_padding_mask=src_key_padding_mask)\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        # Feed-forward network\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layer:int ,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, d_ff, nhead, dropout)\n",
        "            for _ in range(num_decoder_layer)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        output = tgt\n",
        "        for layer in self.layers:\n",
        "            output = layer(src, output, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        src2 = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=None)\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm1(src)\n",
        "        # Feed-forward network\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, dim_feedforward, nheads, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, key_padding_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer\n",
        "This section gathers the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ],
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        self.encoder = TransformerEncoder(d_model, dim_feedforward, num_encoder_layers, nhead, dropout)\n",
        "        self.decoder = TransformerDecoder(d_model, dim_feedforward, num_decoder_layers, nhead, dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        memory = self.encoder(src, key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(memory, tgt, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            max_seq_length: int = 512\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Token embeddings\n",
        "        self.src_embedding = nn.Embedding(n_tokens_src, dim_embedding)\n",
        "        self.tgt_embedding = nn.Embedding(n_tokens_tgt, dim_embedding)\n",
        "        # Learnable positional embeddings\n",
        "        self.src_pos_embedding = nn.Embedding(max_seq_length, dim_embedding)\n",
        "        self.tgt_pos_embedding = nn.Embedding(max_seq_length, dim_embedding)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads,\n",
        "            num_encoder_layers=n_layers,\n",
        "            num_decoder_layers=n_layers,\n",
        "            dim_feedforward=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        # Final linear projection to vocab size\n",
        "        self.fc_out = nn.Linear(dim_embedding, n_tokens_tgt)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Create masks\n",
        "        tgt_mask_attn = self.generate_causal_mask(target)\n",
        "        src_key_padding_mask, tgt_key_padding_mask = self.generate_key_padding_mask(source, target)\n",
        "\n",
        "        batch_size, src_seq_len = source.shape\n",
        "        batch_size, tgt_seq_len = target.shape\n",
        "        # Create position indices\n",
        "        src_positions = torch.arange(0, src_seq_len, device=source.device).unsqueeze(0).expand(batch_size, src_seq_len)\n",
        "        tgt_positions = torch.arange(0, tgt_seq_len, device=target.device).unsqueeze(0).expand(batch_size, tgt_seq_len)\n",
        "\n",
        "        # Embed tokens and add position embeddings\n",
        "        src_emb = self.src_embedding(source) + self.src_pos_embedding(src_positions)\n",
        "        tgt_emb = self.tgt_embedding(target) + self.tgt_pos_embedding(tgt_positions)\n",
        "\n",
        "        # Pass through Transformer (encoder-decoder)\n",
        "        transformer_out = self.transformer(src_emb, tgt_emb, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        # Project to vocabulary\n",
        "        logits = self.fc_out(transformer_out)\n",
        "        return logits\n",
        "\n",
        "    def generate_causal_mask(\n",
        "            self,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending subsequent tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [seq_len_tgt, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = target.shape[1]\n",
        "\n",
        "        tgt_mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
        "        tgt_mask = torch.triu(tgt_mask, diagonal=1).to(target.device)\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def generate_key_padding_mask(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending padding tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        src_key_padding_mask = source == self.src_pad_idx\n",
        "        tgt_key_padding_mask = target == self.tgt_pad_idx\n",
        "\n",
        "        return src_key_padding_mask, tgt_key_padding_mask"
      ],
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search\n",
        "\n",
        "One idea to explore once you have your model working is to implement a geedy search to generate a target translation from a trained model and an input source string. The next token will simply be the most probable one. Compare this strategy of decoding with the beam search strategy below."
      ],
      "metadata": {
        "id": "ql6jv2lAK-nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_sentence_length):\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            next_token = predicted[:, -1].argmax(dim=-1).unsqueeze(dim=1)\n",
        "\n",
        "            tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
        "\n",
        "            if next_token.item() == EOS_IDX:\n",
        "                break\n",
        "\n",
        "    tgt_sentence = list(tgt_tokens.squeeze().cpu().numpy())[1:]  # Remove <bos> token\n",
        "    tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "    tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "\n",
        "    tgt_sentence = beautify(tgt_sentence)\n",
        "    return tgt_sentence"
      ],
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search\n",
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always chooses the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k¬≤ sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ],
      "metadata": {
        "id": "LgGFG-uXue6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "\n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  # [batch_size, n_beams, n_tokens]\n",
        "    beams = beams.unsqueeze(dim=2)  # [batch_size, n_beams, 1]\n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)  # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences orderer by their likelihood.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        while tgt_tokens.shape[1] < max_sentence_length:\n",
        "            batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "            # Get next beams\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            probs, predicted = predicted[:, -1].topk(k=beam_width, dim=-1)\n",
        "\n",
        "            # Separe between terminated sentences and the others\n",
        "            idx_terminated, idx_not_terminated = indices_terminated(tgt_tokens, EOS_IDX)\n",
        "            idx_terminated, idx_not_terminated = idx_terminated.to(device), idx_not_terminated.to(device)\n",
        "\n",
        "            tgt_terminated = torch.index_select(tgt_tokens, dim=0, index=idx_terminated)\n",
        "            tgt_probs_terminated = torch.index_select(target_probs, dim=0, index=idx_terminated)\n",
        "\n",
        "            filter_t = lambda t: torch.index_select(t, dim=0, index=idx_not_terminated)\n",
        "            tgt_others = filter_t(tgt_tokens)\n",
        "            tgt_probs_others = filter_t(target_probs)\n",
        "            predicted = filter_t(predicted)\n",
        "            probs = filter_t(probs)\n",
        "\n",
        "            # Add the top tokens to the previous target sentences\n",
        "            tgt_others = append_beams(tgt_others, predicted)\n",
        "\n",
        "            # Add padding to terminated target\n",
        "            padd = torch.zeros((len(tgt_terminated), 1), dtype=torch.long, device=device)\n",
        "            tgt_terminated = torch.cat(\n",
        "                (tgt_terminated, padd),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Update each target sentence probabilities\n",
        "            tgt_probs_others = torch.repeat_interleave(tgt_probs_others, beam_width)\n",
        "            tgt_probs_others *= probs.flatten()\n",
        "            tgt_probs_terminated *= 0.999  # Penalize short sequences overtime\n",
        "\n",
        "            # Group up the terminated and the others\n",
        "            target_probs = torch.cat(\n",
        "                (tgt_probs_others, tgt_probs_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "            tgt_tokens = torch.cat(\n",
        "                (tgt_others, tgt_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "\n",
        "            # Keep only the top `max_target` target sentences\n",
        "            if target_probs.shape[0] <= max_target:\n",
        "                continue\n",
        "\n",
        "            target_probs, indices = target_probs.topk(k=max_target, dim=0)\n",
        "            tgt_tokens = torch.index_select(tgt_tokens, dim=0, index=indices)\n",
        "\n",
        "    sentences = []\n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "        sentences.append(tgt_sentence)\n",
        "\n",
        "    sentences = [beautify(s) for s in sentences]\n",
        "\n",
        "    # Join the sentences with their likelihood\n",
        "    sentences = [(s, p.item()) for s, p in zip(sentences, target_probs)]\n",
        "    # Sort the sentences by their likelihood\n",
        "    sentences = [(s, p) for s, p in sorted(sentences, key=lambda k: k[1], reverse=True)]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb"
      ],
      "metadata": {
        "id": "77fE9LtfZayA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking GPU and logging to wandb\n",
        "\n",
        "!wandb login\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WriScTUEsRHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18acba54-5d96-4db0-a69c-3a3c24ecdd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Thu Mar 20 12:19:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "JkXpm6Unb5g6",
        "outputId": "da46bdb8-e77b-4b88-f687-15f3f65770e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdUA-rftb-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)  # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()  # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict, LRscheduler: bool=None):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            if LRscheduler:\n",
        "                LRscheduler.step()\n",
        "            optimizer.step()\n",
        "\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "\n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "\n",
        "        # Logs\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "        val_pred, val_prob = beam_search(\n",
        "            model,\n",
        "            val_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "\n",
        "        logs = {**train_logs, **val_logs}  # Merge dictionnaries\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        train_pred, train_prob = beam_search(\n",
        "            model,\n",
        "            train_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob,\n",
        "            val_source, val_target, val_pred, val_prob,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "\n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ],
      "metadata": {
        "id": "YImgxCWjlWni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate the datasets\n",
        "\n",
        "MAX_SEQ_LEN = 60\n",
        "MIN_TOK_FREQ = 2\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ],
      "metadata": {
        "id": "iqmpxnO1lgDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5ca9c2-f08b-426c-d0b7-99a9603f9cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 12,154\n",
            "French vocabulary size: 18,340\n",
            "\n",
            "Training examples: 209,459\n",
            "Validation examples: 23,274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywFEpplOU5dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e0d961-f655-4aff-bf7c-bfbb45bd9592"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄEmbedding: 1-2                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄEmbedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄEmbedding: 1-4                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄTransformer: 1-5                                 [128, 60, 196]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        768,108\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        1,232,628\n",
              "‚îú‚îÄLinear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 11,791,244\n",
              "Trainable params: 11,791,244\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.51\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1955.76\n",
              "Params size (MB): 47.16\n",
              "Estimated Total Size (MB): 2003.05\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "\"\"\"model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\"\"\"\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maOTVtk4acxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cd228ab-0acb-4ec3-882a-7d5513b0740a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_125105-wcs5ro9p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">basic architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.71     top-1: 0.66    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.54     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "It's in the fridge.\n",
            "C'est dans le frigo.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.43     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Are you sure you know what to do?\n",
            "√ätes-vous s√ªr de savoir quoi faire ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.26     top-1: 0.72    top-5: 0.90    top-10: 0.92\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Lend him as much money as he needs.\n",
            "Pr√™te-lui autant d'argent qu'il a besoin.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.17     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Eval -    loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I think it's impossible for him to solve the problem.\n",
            "Je pense qu'il est impossible de le r√©soudre.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom isn't the one who needs help.\n",
            "Tom n'est pas celui qui a besoin d'aide.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Are you productive?\n",
            "√ätes-vous productif ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "Because of the snow, I couldn't see anything.\n",
            "√Ä cause de la neige, je ne pouvais rien voir.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Nobody recognized you.\n",
            "Personne ne vous a reconnu.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I know how old you are.\n",
            "Je sais comment vous √™tes vieux.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Don't talk about my family.\n",
            "Ne parle pas de ma famille !\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.94289</td></tr><tr><td>Train - top-1</td><td>0.77542</td></tr><tr><td>Train - top-10</td><td>0.95308</td></tr><tr><td>Train - top-5</td><td>0.93074</td></tr><tr><td>Validation - loss</td><td>0.94087</td></tr><tr><td>Validation - top-1</td><td>0.78161</td></tr><tr><td>Validation - top-10</td><td>0.95074</td></tr><tr><td>Validation - top-5</td><td>0.93064</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">basic architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_125105-wcs5ro9p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online  # online / offline / disabled to activate, deactivate or turn off WandB logging\n",
        "\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Test',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='basic architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PFIyvKUefdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37422df2-311d-422f-82c3-ac73c5f5c340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. (29.39885%) \t Il est possible d'essayer votre travail ici.\n",
            "1. (22.23845%) \t Il est possible d'essayer ton travail ici.\n",
            "2. (4.08434%) \t Il est possible de tenter ton travail ici.\n",
            "3. (3.98461%) \t Il est possible d'essayer ta travail ici.\n",
            "4. (3.38965%) \t Il est possible de tenter votre travail ici.\n"
          ]
        }
      ],
      "source": [
        "sentence = \"It is possible to try your work here.\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:5]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Questions\n",
        "1. Explain the differences between Vanilla RNN, GRU-RNN, Encoder-Decoder Transformer and Decoder-Only Transformer.\n",
        "\n",
        " Vanilla RNN process each sequence step-by-step. Vanilla RNN have a hidden state that is updated after each time step. The main limitation is that they suffer from vanishing and exploding gradients. Therefore it's hard for Vanilla RNN to learn long-term dependencies. That's why GRU-RNN were invented.\n",
        "GRU-RNN are a variation of RNNs. They include gating mechanisms to control information flow. They have reset and update gates that help address the vanishing gradient problem by preserving important information across longer sequences.\n",
        "Encoder-Decoder Transformer is an architecture that has two main components. The first component is an encoder that processes the input sequence. The second component is a  decoder that generates the output sequence. Transformers use self-attention mechanisms. The latter are highly parallelizable, allowing them to process entire sequences simultaneously unlike RNNs.\n",
        "Decoder-Only Transformers use only the decoder part of the transformer. It generates sequences by autoregression. It means each token is predicted based on previously generated tokens.\n",
        "\n",
        "2. Why is positionnal encoding necessary in Transformers and not in RNNs?\n",
        "\n",
        "RNNs process sequences step-by-step. The order of inputs is implicitly handled by the sequential nature of the architecture. Transformers process entire sequences at once using attention mechanisms, which have no inherent sense of order. Positional encoding allows transformer to have information about the position of tokens in the sequence, allowing the transformer to understand the relative order of tokens.\n",
        "\n",
        "3. Describe the preprocessing process. Detail how the initial dataset is processed before being fed to the translation models.\n",
        "\n",
        "First of all, there's tokenizaition. The input sentences are first tokenized into subwords, words, or characters depending on the tokenization strategy.\n",
        "\n",
        "After tokenization, each token is mapped into a dense vector representation (called embedding). The embedding layer converts tokens into vectors that capture syntactic and semantic meanings. In transformers, this embedding layer is learned during training.\n",
        "\n",
        "Since transformers process sequences in parallel, positional encodings are added to the embeddings to provide information about the position of each token in the sequence. This is crucial because a self-attention mechanism doesn't inherently understand token order.\n",
        "\n",
        "Sentences may have different lengths, so shorter sentences are padded to match the longest sentence in the batch. Padding tokens are added to ensure a uniform length for sequences.\n",
        "\n",
        "then, the tokenized sentences (along with their embeddings and positional encodings) are transformed into tensors that our model can process.\n",
        "\n",
        "Masks are then generated to ensure that padded tokens are ignored during attention calculations, preventing the model from focusing on irrelevant padding positions.\n",
        "\n",
        "And finally.The processed sequences are grouped into batches to make training more efficient. Batching helps leverage parallel computation.\n",
        "\n",
        "4. What is teacher forcing, and how is it used in Transformer training? How does the decoder input differ?\n",
        "\n",
        "Teacher forcing is a technique where during training the model uses the actual target sequence as input for the decoder instead of model output from the previous step. Teacher forcing can help the model to converge faster by providing the correct context. The decoder input differs in that during training. The model sees the actual target, while during inference, it sees its own previous outputs.\n",
        "\n",
        "5. How are the two types of mask important to the attention mechanism (causal and padding) and how do they work? How do they differ between the encoder and decoder?\n",
        "\n",
        "Causal Mask is used in the decoder to prevent future tokens from being visible to the current token. This ensures the model generates tokens one step at a time and based only on the previous tokens. It doesn't give \"spoilers\" to the model.\n",
        "Padding Mask is used to mask padded positions in both the encoder and decoder, ensuring the model doesn‚Äôt attend to padded positions that are irrelevant.\n",
        "The encoder uses only padding masks, while the decoder uses both causal and padding masks. Causal masking is critical in the decoder to ensure autoregressive generation.\n",
        "\n",
        "6. What is a causal mask, and why is it only used in the decoder?\n",
        "\n",
        "The causal mask ensures that each token in the sequence only attends to past tokens and not future tokens during generation. This ensures the model predicts the next token based on the previous tokens and only the previous tokens. It is only used in the decoder because the decoder is responsible for autoregressive output generation, where tokens are generated one by one.\n",
        "\n",
        "7. Why does the decoder use both self-attention and encoder-decoder attention?\n",
        "\n",
        "The self-attention mechanism in the decoder helps the model to consider the relationships between previously generated tokens. The encoder-decoder attention helps the decoder focus on relevant parts of the input sequence when generating each output token.\n",
        "\n",
        "8. Why is the Transformer model parallelizable, and how does this improve efficiency compared to RNNs?\n",
        "\n",
        "Unlike RNNs, which process tokens sequentially, transformers can process entire sequences in parallel. This is because self-attention mechanism allows the model to process different parts of an input sequence simultaneously, rather than sequentially. And also a head of self attention does not need the other heads. It can work independently and in parallel.\n",
        "\n",
        "9. How does multi-head self-attention allow the model to capture different aspects of a sentence?\n",
        "\n",
        "Multi-head attention allows the model to apply multiple attention mechanisms in parallel. The interesting thing is that each head can focus on different parts of the sentence, capturing various relationships : syntax, semantics, or long-range dependencies. This helps the model gain a richer understanding of the input.\n",
        "\n",
        "10. What does the decoder's final output represent before the projection layer? What does the encoder's final output represent?\n",
        "\n",
        "the decoder‚Äôs output is a set of contextualized token embeddings that represent the understanding of the model of the sequence and the input from the encoder.\n",
        "The final output of the encoder is a set of embeddings representing the entire input sequence, which are used by the decoder to generate the output.\n",
        "\n",
        "11. What is the role of the final linear projection layer in the decoder?\n",
        "How does the decoder output differ between training (parallel processing) and inference (sequential generation)?\n",
        "\n",
        "The role of the final linear projection layer is to map the decoder's output embeddings to the size of the target vocabulary. It transforms the model‚Äôs contextualized representations into logits for each possible output token. In training, the decoder can process entire sequences in parallel, while during inference, tokens are generated sequentially, using the previously generated tokens as input.\n",
        "\n",
        "12. Why does the decoder recompute all outputs at each inference step instead of appending new outputs incrementally?\n",
        "\n",
        "Transformers recompute the entire sequence during each inference because it ensures consistency in the attention mechanism. And also each new token influences the attention distribution. Recomputing allows the model to incorporate the newly generated token in the context when predicting the next token."
      ],
      "metadata": {
        "id": "uHhixEEGzWRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small report - experiments"
      ],
      "metadata": {
        "id": "Y3tQdusIjPCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "This section presents the other metrics I used to evaluate a model."
      ],
      "metadata": {
        "id": "JZkYhzv8Rlgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics used :\n",
        "\n",
        "\n",
        "*   [BLEU](https://en.wikipedia.org/wiki/BLEU) : evaluates how closely a machine-generated translation matches reference translations by comparing overlapping n-grams. BLEU compares the n-grams (sequences of words) in the machine-translated output to the n-grams in a reference translation. It focuses on precision : how many of the generated n-grams appear in the reference. BLEU also includes a brevity penalty to account for overly short translations.\n",
        "*   [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric) : set of metrics that evaluates text based on recall by measuring the overlap between machine-generated content and reference text, often used in summarization tasks.\n",
        "* [METEOR](https://en.wikipedia.org/wiki/METEOR) : designed to address some of BLEU‚Äôs limitations by focusing on both precision and recall, and it also incorporates synonyms and stemming. METEOR aligns words in the generated translation and reference text based on exact matches, synonyms, and paraphrases. It then computes precision and recall scores, with recall weighted higher than precision. METEOR includes a penalty for fragmented alignments (when matches are far apart in the sentence).\n",
        "\n"
      ],
      "metadata": {
        "id": "beF_wQV5TWkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "z-XQ3dxjFIR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ff4ab2-b8a0-439e-908e-48d1d590bec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=042495c18222755ed58fdd4c958e1b30d9dcf9f9ef9de0c15d0d76b5ae9acd47\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "eSe0pZAIRs-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49393fba-29a4-4a6a-d191-2e98edc0eff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes BLEU score between the reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (List[str]): List of reference sentences.\n",
        "        hypothesis (List[str]): The predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score.\n",
        "    \"\"\"\n",
        "    return sentence_bleu([reference], hypothesis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_rouge(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes ROUGE scores between reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (str): Reference sentence.\n",
        "        hypothesis (str): Predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE scores (1, 2, L).\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    return scorer.score(reference, hypothesis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_meteor(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes METEOR score between the reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (List[str]): List of reference sentences.\n",
        "        hypothesis (str): Predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        float: METEOR score.\n",
        "    \"\"\"\n",
        "    return meteor_score([reference], hypothesis)\n",
        "\n",
        "def compute_metrics(reference, hypothesis):\n",
        "    bleu = compute_bleu(reference, hypothesis)\n",
        "    rouge = compute_rouge(' '.join(reference), ' '.join(hypothesis))\n",
        "    meteor = compute_meteor(reference, hypothesis)\n",
        "\n",
        "    metrics = {\n",
        "        'BLEU': bleu,\n",
        "        'ROUGE-1': rouge['rouge1'].fmeasure,\n",
        "        'ROUGE-2': rouge['rouge2'].fmeasure,\n",
        "        'ROUGE-L': rouge['rougeL'].fmeasure,\n",
        "        'METEOR': meteor\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DiOfqI8EOi65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(model, sentence, reference, config, k=5, beam_width=10, max_target=100):\n",
        "  preds = beam_search(\n",
        "      model,\n",
        "      sentence,\n",
        "      config['src_vocab'],\n",
        "      config['tgt_vocab'],\n",
        "      config['src_tokenizer'],\n",
        "      config['device'],\n",
        "      beam_width=beam_width,\n",
        "      max_target=max_target,\n",
        "      max_sentence_length=config['max_sequence_length']\n",
        "  )[:k]\n",
        "  print(\"Beam Search : \")\n",
        "  for i, (translation, likelihood) in enumerate(preds):\n",
        "      print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')\n",
        "      metrics = compute_metrics(reference, translation.split())\n",
        "      for metric, value in metrics.items():\n",
        "          print(f\"{metric}: {value}\")\n",
        "      print()\n",
        "\n",
        "\n",
        "  pred_greedy_search = greedy_search(\n",
        "      model,\n",
        "      sentence,\n",
        "      config['src_vocab'],\n",
        "      config['tgt_vocab'],\n",
        "      config['src_tokenizer'],\n",
        "      config['device'],\n",
        "      max_sentence_length=config['max_sequence_length']\n",
        "  )\n",
        "\n",
        "  print(f'Greedy search: \\t {pred_greedy_search}')\n",
        "\n",
        "  metrics_greedy = compute_metrics(reference, pred_greedy_search.split())\n",
        "  for metric, value in metrics_greedy.items():\n",
        "      print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "id": "d5qvbt1Y6yg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate scheduler"
      ],
      "metadata": {
        "id": "uZ5LPtB2E8Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A learning rate scheduler is a tool used in training deep learning models to adjust the learning rate over time. This tool is used in the article Attention is all you need."
      ],
      "metadata": {
        "id": "mo6X5Kut_xMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLRScheduler:\n",
        "    def __init__(self, optimizer : optim, d_model : int, warmup_steps : int):\n",
        "        \"\"\" Initialize the scheduler\n",
        "        Args:\n",
        "            optimizer: The optimizer whose learning rate needs to be adjusted.\n",
        "            d_model: The model's dimensionality.\n",
        "            warmup_steps: The number of warmup steps.\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the learning rate based on the current step number.\"\"\"\n",
        "        self.step_num += 1\n",
        "        lr = self._compute_lr()\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def _compute_lr(self):\n",
        "        \"\"\"Compute the learning rate according to the formula.\"\"\"\n",
        "        lr = (self.d_model ** -0.5) * min(\n",
        "            self.step_num ** -0.5,\n",
        "            self.step_num * (self.warmup_steps ** -1.5)\n",
        "        )\n",
        "        return lr"
      ],
      "metadata": {
        "id": "2UJ8NmnxE-FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "bjQ17dWRjAMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 5,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "id": "VRR07kL92B7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0293f10-4811-473a-8550-738ea42e0706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TranslationRNN                           [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                         [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄRNN: 1-2                               [128, 60, 256]            --\n",
              "‚îÇ    ‚îî‚îÄModuleList: 2-1                   --                        --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-1                 [128, 60, 256]            116,224\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-2                 [128, 60, 256]            131,584\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-3                 [128, 60, 256]            131,584\n",
              "‚îú‚îÄLayerNorm: 1-3                         [128, 3, 256]             512\n",
              "‚îú‚îÄEmbedding: 1-4                         [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄRNN: 1-5                               [128, 60, 256]            --\n",
              "‚îÇ    ‚îî‚îÄModuleList: 2-2                   --                        --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-4                 [128, 60, 256]            116,224\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-5                 [128, 60, 256]            131,584\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄRNNCell: 3-6                 [128, 60, 256]            131,584\n",
              "‚îú‚îÄLinear: 1-6                            [128, 60, 18340]          4,713,380\n",
              "==========================================================================================\n",
              "Total params: 11,449,500\n",
              "Trainable params: 11,449,500\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 7.20\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1340.42\n",
              "Params size (MB): 45.80\n",
              "Estimated Total Size (MB): 1386.35\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='RNN',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='RNN',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QvkYM1i52L9u",
        "outputId": "e630cc00-a202-4b17-ef01-c9ad87acbfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_130114-ubskng3g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">RNN</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 5 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.77     top-1: 0.49    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.68     top-1: 0.50    top-5: 0.68    top-10: 0.74\n",
            "Whose room is this?\n",
            "√Ä quelle heure est-ce ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.48     top-1: 0.53    top-5: 0.71    top-10: 0.77\n",
            "Eval -    loss: 2.41     top-1: 0.53    top-5: 0.72    top-10: 0.78\n",
            "The house was on fire.\n",
            "La maison √©tait ouverte.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.35     top-1: 0.54    top-5: 0.73    top-10: 0.79\n",
            "Eval -    loss: 2.28     top-1: 0.54    top-5: 0.74    top-10: 0.79\n",
            "There isn't enough coffee for everyone.\n",
            "Il n'y a pas de travail √† faire.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.29     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "Eval -    loss: 2.19     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "He lost his eyesight.\n",
            "Il perdit ses promesses.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.19     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.14     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "She is used to staying up all night.\n",
            "Elle est all√© √† l'√©cole aujourd'hui.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.19423</td></tr><tr><td>Train - top-1</td><td>0.55791</td></tr><tr><td>Train - top-10</td><td>0.80902</td></tr><tr><td>Train - top-5</td><td>0.75152</td></tr><tr><td>Validation - loss</td><td>2.14071</td></tr><tr><td>Validation - top-1</td><td>0.56359</td></tr><tr><td>Validation - top-10</td><td>0.81431</td></tr><tr><td>Validation - top-5</td><td>0.75849</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">RNN</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_130114-ubskng3g/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "uz8nKzh32nHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51762329-f176-4ef0-d645-ec7381a60000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00007%) \t Le Japon est la capitale de la gare.\n",
            "BLEU: 1.0832677820940877e-231\n",
            "ROUGE-1: 0.25\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.125\n",
            "METEOR: 0.125\n",
            "\n",
            "1. (0.00005%) \t Le Japon est la troisi√®me source de classe.\n",
            "BLEU: 1.0832677820940877e-231\n",
            "ROUGE-1: 0.23529411764705882\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.11764705882352941\n",
            "METEOR: 0.125\n",
            "\n",
            "2. (0.00005%) \t Le monde est la capitale de la gare.\n",
            "BLEU: 1.2882297539194154e-231\n",
            "ROUGE-1: 0.375\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.25\n",
            "METEOR: 0.1875\n",
            "\n",
            "3. (0.00004%) \t Le monde est le plus grand fleuve du monde.\n",
            "BLEU: 6.7393716283177006e-155\n",
            "ROUGE-1: 0.35294117647058826\n",
            "ROUGE-2: 0.13333333333333333\n",
            "ROUGE-L: 0.35294117647058826\n",
            "METEOR: 0.3155006858710563\n",
            "\n",
            "4. (0.00003%) \t Les gens ont vu la diff√©rence entre les deux filles.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.06097560975609756\n",
            "\n",
            "Greedy search: \t Les gens sont la m√™me source de la nourriture de la vie.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.05952380952380953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the work is done\"\n",
        "reference = [\"le\", \"travail\", \"est\", \"fait\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "sYchP9WAtyQq",
        "outputId": "5b64c0f5-7cce-4c50-b915-77b1142e6698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (1.14373%) \t Le t√©l√©phone sonne.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.22222222222222224\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.22222222222222224\n",
            "METEOR: 0.12820512820512822\n",
            "\n",
            "1. (0.18161%) \t Le chien sonne.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.28571428571428575\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.28571428571428575\n",
            "METEOR: 0.12820512820512822\n",
            "\n",
            "2. (0.14133%) \t La r√©ponse.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.13488%) \t Le chien s'est produit.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.4444444444444445\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.4444444444444445\n",
            "METEOR: 0.125\n",
            "\n",
            "4. (0.09769%) \t Je l'aime.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t Le chien est en train de r√™ver.\n",
            "BLEU: 1.1200407237786664e-231\n",
            "ROUGE-1: 0.3333333333333333\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.3333333333333333\n",
            "METEOR: 0.23255813953488375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "JI7UXoByjBwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 5,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'GRU',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "id": "Z0xhfY01jCzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f4fe8c-8a5e-4fcd-e7de-da00625e4cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TranslationRNN                           [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                         [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄRNN: 1-2                               [128, 60, 256]            --\n",
              "‚îÇ    ‚îî‚îÄModuleList: 2-1                   --                        --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-1                 [128, 60, 256]            348,672\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-2                 [128, 60, 256]            394,752\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-3                 [128, 60, 256]            394,752\n",
              "‚îú‚îÄLayerNorm: 1-3                         [128, 3, 256]             512\n",
              "‚îú‚îÄEmbedding: 1-4                         [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄRNN: 1-5                               [128, 60, 256]            --\n",
              "‚îÇ    ‚îî‚îÄModuleList: 2-2                   --                        --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-4                 [128, 60, 256]            348,672\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-5                 [128, 60, 256]            394,752\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄGRUCell: 3-6                 [128, 60, 256]            394,752\n",
              "‚îú‚îÄLinear: 1-6                            [128, 60, 18340]          4,713,380\n",
              "==========================================================================================\n",
              "Total params: 12,967,068\n",
              "Trainable params: 12,967,068\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 18.85\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1717.91\n",
              "Params size (MB): 51.87\n",
              "Estimated Total Size (MB): 1769.90\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='RNN',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='GRU',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MidOn3oqkAp",
        "outputId": "939b038e-5190-4776-ea06-0041e50800b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_122208-ai3x7sks</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">GRU</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 5 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.90     top-1: 0.63    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.79     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "Whose room is this?\n",
            "√Ä qui est cette pi√®ce ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.49     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.45     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "The house was on fire.\n",
            "La maison √©tait feu.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.34     top-1: 0.70    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.32     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "There isn't enough coffee for everyone.\n",
            "Il n'y a pas assez de caf√© pour tout le monde.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.25     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "He lost his eyesight.\n",
            "Il perdit la vue.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.17     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "She is used to staying up all night.\n",
            "Elle est habitu√© √† rester debout toute la nuit.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.17065</td></tr><tr><td>Train - top-1</td><td>0.7324</td></tr><tr><td>Train - top-10</td><td>0.93063</td></tr><tr><td>Train - top-5</td><td>0.90248</td></tr><tr><td>Validation - loss</td><td>1.21256</td></tr><tr><td>Validation - top-1</td><td>0.72795</td></tr><tr><td>Validation - top-10</td><td>0.92509</td></tr><tr><td>Validation - top-5</td><td>0.89587</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_122208-ai3x7sks/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkCZPRrjf6KJ",
        "outputId": "b2de753c-4957-4797-daef-d52fe797c92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.04919%) \t Le beurre est le meilleur gar√ßon du monde ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "1. (0.01414%) \t Le pain est le meilleur gar√ßon du monde ?\n",
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "2. (0.01374%) \t Le beurre est le meilleur gar√ßon du monde du monde.\n",
            "BLEU: 3.965294799986402e-78\n",
            "ROUGE-1: 0.4210526315789474\n",
            "ROUGE-2: 0.23529411764705882\n",
            "ROUGE-L: 0.4210526315789474\n",
            "METEOR: 0.45731707317073167\n",
            "\n",
            "3. (0.01289%) \t Le bl√¢mer est le meilleur gar√ßon du monde ?\n",
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.4444444444444445\n",
            "ROUGE-2: 0.25\n",
            "ROUGE-L: 0.4444444444444445\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "4. (0.01022%) \t Le beurre est le meilleur gar√ßon du monde.\n",
            "BLEU: 4.4646672960328985e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.3680555555555556\n",
            "\n",
            "Greedy search: \t Le beurre est le plus grand de la meilleure source du monde.\n",
            "BLEU: 5.233427736988301e-155\n",
            "ROUGE-1: 0.4\n",
            "ROUGE-2: 0.1111111111111111\n",
            "ROUGE-L: 0.4\n",
            "METEOR: 0.30423280423280424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the work is done\"\n",
        "reference = [\"le\", \"travail\", \"est\", \"fait\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZ_ln-Hodo1",
        "outputId": "534bd4eb-5c71-47e3-d506-c6d2c099a111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (2.46919%) \t Tout est le travail ?\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "1. (2.34901%) \t Tout est le travail.\n",
            "BLEU: 1.5319719891192393e-231\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.25\n",
            "\n",
            "2. (1.99484%) \t Le travail est le travail.\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.6666666666666665\n",
            "ROUGE-2: 0.5714285714285715\n",
            "ROUGE-L: 0.6666666666666665\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "3. (1.65739%) \t Le travail est fait.\n",
            "BLEU: 9.53091075863908e-155\n",
            "ROUGE-1: 1.0\n",
            "ROUGE-2: 1.0\n",
            "ROUGE-L: 1.0\n",
            "METEOR: 0.7361111111111112\n",
            "\n",
            "4. (1.23024%) \t Tout est le travail ¬† ?\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "Greedy search: \t Le travail est fait du travail.\n",
            "BLEU: 5.775353993361614e-78\n",
            "ROUGE-1: 0.8\n",
            "ROUGE-2: 0.7499999999999999\n",
            "ROUGE-L: 0.8\n",
            "METEOR: 0.9449404761904763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "EvGynA5MCY5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YwDbJVPmarJrGhXBDAffl8jF_BVgcWKJ\" alt=\"RNN\" width=\"700\"/>\n"
      ],
      "metadata": {
        "id": "ze0Ec_P-Clpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1tdgNIwexsdyd8aSAf_4fF9tBdRup19Eg\" alt=\"RNN\" width=\"700\"/>"
      ],
      "metadata": {
        "id": "rocP6A4rCzB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU seems to be a better model for translation. However the model struggles to generalize because the translations of the instances are not well translated."
      ],
      "metadata": {
        "id": "8A0bMTJiC_w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architectures\n",
        "This section presents a few of the best architectures I created."
      ],
      "metadata": {
        "id": "18bYK_8IRhM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First architecture"
      ],
      "metadata": {
        "id": "TF8HYEqIUlab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Xnr3idFeByP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c741d0-d9ea-4108-a982-938bf932ce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄEmbedding: 1-2                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄEmbedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄEmbedding: 1-4                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄTransformer: 1-5                                 [128, 60, 196]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        256,036\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        410,876\n",
              "‚îú‚îÄLinear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 10,457,420\n",
              "Trainable params: 10,457,420\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.34\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1435.24\n",
              "Params size (MB): 41.83\n",
              "Estimated Total Size (MB): 1477.19\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='first architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sFdByui95wEM",
        "outputId": "a5d38db7-62f6-416e-fba6-f09c80c348c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_222559-eyc47829</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">first architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "Eval -    loss: 1.74     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You never asked what I wanted.\n",
            "Tu n'as jamais demand√© ce que je voulais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.60     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Have you read this book already?\n",
            "Avez-vous d√©j√† lu ce livre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.43     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.31     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "He is playing in his room.\n",
            "Il joue dans sa chambre.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.35     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.23     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I wish my girlfriend would spend more time with me.\n",
            "J'aimerais que ma petite amie passe davantage de temps avec moi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.24     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.18     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "He was so sad that he almost went mad.\n",
            "Il √©tait si triste qu'il est parti.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "You can get a loan from a bank.\n",
            "Tu peux obtenir un pr√™t de banque.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "I swear I didn't make this up.\n",
            "Je jure que je ne faisais pas √ßa.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.11     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I had to stop.\n",
            "J'ai d√ª arr√™ter.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.08     top-1: 0.76    top-5: 0.91    top-10: 0.94\n",
            "CEO's of American corporations are paid several times their Japanese counterparts.\n",
            "Les habitants de l'immobilier a √©t√© pay√© plusieurs fois leurs japonaise.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom is trying to make sure that everything is ready.\n",
            "Tom essaie d'essayer de assurer que tout est pr√™t.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.08052</td></tr><tr><td>Train - top-1</td><td>0.74934</td></tr><tr><td>Train - top-10</td><td>0.94146</td></tr><tr><td>Train - top-5</td><td>0.91612</td></tr><tr><td>Validation - loss</td><td>1.06362</td></tr><tr><td>Validation - top-1</td><td>0.75878</td></tr><tr><td>Validation - top-10</td><td>0.94018</td></tr><tr><td>Validation - top-5</td><td>0.91688</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">first architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_222559-eyc47829/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "CUPXLcm5iezz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598aadfc-a862-4863-a3c8-9d39d10bfd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (2.13144%) \t Le fer est le meilleur groupe du monde.\n",
            "BLEU: 0.345720784641941\n",
            "ROUGE-1: 0.625\n",
            "ROUGE-2: 0.42857142857142855\n",
            "ROUGE-L: 0.625\n",
            "METEOR: 0.49609375\n",
            "\n",
            "1. (1.38691%) \t Le caf√© est le meilleur groupe du monde.\n",
            "BLEU: 0.345720784641941\n",
            "ROUGE-1: 0.625\n",
            "ROUGE-2: 0.42857142857142855\n",
            "ROUGE-L: 0.625\n",
            "METEOR: 0.49609375\n",
            "\n",
            "2. (0.69926%) \t Le fer est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.3896604938271605\n",
            "\n",
            "3. (0.54538%) \t Le fer est le meilleur groupe du monde dans le monde.\n",
            "BLEU: 0.24808415001701817\n",
            "ROUGE-1: 0.5263157894736842\n",
            "ROUGE-2: 0.3529411764705882\n",
            "ROUGE-L: 0.5263157894736842\n",
            "METEOR: 0.4481927710843373\n",
            "\n",
            "4. (0.50393%) \t Le fer est le meilleur groupe au monde.\n",
            "BLEU: 0.5169731539571706\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.7142857142857143\n",
            "ROUGE-L: 0.75\n",
            "METEOR: 0.6225\n",
            "\n",
            "Greedy search: \t Le fer est le meilleur groupe de la Terre.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.48996913580246915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second architecture"
      ],
      "metadata": {
        "id": "CC8bFeacvCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 14,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 6,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bejQMT75LY-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acce5911-b94e-472f-bbc3-95cfa02354b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "TranslationTransformer                                  [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                        [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄEmbedding: 1-2                                        [128, 60, 196]            100,352\n",
              "‚îú‚îÄEmbedding: 1-3                                        [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄEmbedding: 1-4                                        [128, 60, 196]            100,352\n",
              "‚îú‚îÄTransformer: 1-5                                      [128, 60, 196]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                          [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                             --                        1,536,216\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                          [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                             --                        2,465,256\n",
              "‚îú‚îÄLinear: 1-6                                           [128, 60, 18340]          3,612,980\n",
              "=========================================================================================================\n",
              "Total params: 13,791,980\n",
              "Trainable params: 13,791,980\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.77\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 2736.54\n",
              "Params size (MB): 55.17\n",
              "Estimated Total Size (MB): 2791.83\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='second architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wZejgZTb_rRO",
        "outputId": "ef9e4cf3-b4b1-472b-a769-b61ae107901d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_224843-q0ioo85p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">second architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.66     top-1: 0.67    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.50     top-1: 0.69    top-5: 0.86    top-10: 0.90\n",
            "Turn on the TV.\n",
            "√âteins la t√©l√©vision.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.33     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.22     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I was overweight.\n",
            "J'ai √©t√© en surpoids.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.22     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "Don't give him any ideas.\n",
            "Ne lui donne pas d'id√©es.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.09     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Are you leaving now?\n",
            "Allez-vous partir maintenant ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "A party of scientists were on board with them.\n",
            "Une f√™te de scientifiques √©taient au bord d'eux.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 0.99     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm sick of your excuses.\n",
            "J'en ai marre de vos excuses.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.96     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm really glad to hear it.\n",
            "Je suis tr√®s heureux de l'entendre.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.90     top-1: 0.78    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.91     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "I have to do something else first.\n",
            "Je dois faire quelque chose d'autre.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.87     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.89     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "I think it's impossible for him to solve the problem.\n",
            "Je pense qu'il est impossible de lui r√©soudre le probl√®me.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.86     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.88     top-1: 0.79    top-5: 0.94    top-10: 0.95\n",
            "What makes you laugh?\n",
            "Qu'est-ce qui te fait rire ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.85579</td></tr><tr><td>Train - top-1</td><td>0.7909</td></tr><tr><td>Train - top-10</td><td>0.95955</td></tr><tr><td>Train - top-5</td><td>0.94017</td></tr><tr><td>Validation - loss</td><td>0.88038</td></tr><tr><td>Validation - top-1</td><td>0.79193</td></tr><tr><td>Validation - top-10</td><td>0.95467</td></tr><tr><td>Validation - top-5</td><td>0.93557</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">second architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_224843-q0ioo85p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GiW8ipY7Jl_",
        "outputId": "ca7c7237-d27f-4bb7-bdf3-0844c390975e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.60861%) \t Le meilleur groupe est le meilleur groupe au monde.\n",
            "BLEU: 0.44632361378533286\n",
            "ROUGE-1: 0.7058823529411765\n",
            "ROUGE-2: 0.6666666666666666\n",
            "ROUGE-L: 0.7058823529411765\n",
            "METEOR: 0.6148148148148148\n",
            "\n",
            "1. (0.55541%) \t Le meilleur groupe est le meilleur groupe du monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.48996913580246915\n",
            "\n",
            "2. (0.46842%) \t Le meilleur groupe est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2626909894424158\n",
            "ROUGE-1: 0.5555555555555556\n",
            "ROUGE-2: 0.375\n",
            "ROUGE-L: 0.5555555555555556\n",
            "METEOR: 0.38490853658536583\n",
            "\n",
            "3. (0.20985%) \t Le meilleur pays est le meilleur groupe du monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.48996913580246915\n",
            "\n",
            "4. (0.18202%) \t Le meilleur groupe est le meilleur groupe dans le monde du monde.\n",
            "BLEU: 0.22416933501922293\n",
            "ROUGE-1: 0.5\n",
            "ROUGE-2: 0.33333333333333326\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.44285714285714284\n",
            "\n",
            "Greedy search: \t Le meilleur groupe est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2626909894424158\n",
            "ROUGE-1: 0.5555555555555556\n",
            "ROUGE-2: 0.375\n",
            "ROUGE-L: 0.5555555555555556\n",
            "METEOR: 0.38490853658536583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third architecture"
      ],
      "metadata": {
        "id": "-8So852v6qel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 256,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 6,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "warmup_steps = 200\n",
        "scheduler = CustomLRScheduler(config['optimizer'], config['dim_embedding'], warmup_steps)\n",
        "\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOJ-wcio6qGq",
        "outputId": "22267e20-94f8-405e-c4d0-9870bef7caae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "TranslationTransformer                                  [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                        [128, 60, 256]            3,111,424\n",
              "‚îú‚îÄEmbedding: 1-2                                        [128, 60, 256]            131,072\n",
              "‚îú‚îÄEmbedding: 1-3                                        [128, 60, 256]            4,695,040\n",
              "‚îú‚îÄEmbedding: 1-4                                        [128, 60, 256]            131,072\n",
              "‚îú‚îÄTransformer: 1-5                                      [128, 60, 256]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                          [128, 60, 256]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                             --                        2,374,656\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                          [128, 60, 256]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                             --                        3,956,736\n",
              "‚îú‚îÄLinear: 1-6                                           [128, 60, 18340]          4,713,380\n",
              "=========================================================================================================\n",
              "Total params: 19,113,380\n",
              "Trainable params: 19,113,380\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 2.45\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 3171.53\n",
              "Params size (MB): 76.45\n",
              "Estimated Total Size (MB): 3248.11\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='third architecture',\n",
        "    ):\n",
        "    train_model(model, config, scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GKPYlUqhJglS",
        "outputId": "971be404-f300-4b4c-c60f-6bb377cb577f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_223858-egy0a07w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">third architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "May I ask another question?\n",
            "id√©e arpenteuse m√©dicaments V√©rifiez langue fain√©ant accordez Devinez fain√©ant fain√©ant accordez Devinez fain√©ant accordez attrap√©e chienne canettes √©teigne V√©rifiez mis√©rable hospitalis√©es fain√©ant accordez b√¢tie d√©tritus reposons fain√©ant fain√©ant comportent voulions figur√© astucieux payerons d√©cod√© d√©tritus reviendrons figure all√©gation d√©couvertes croyait accordez V√©rifiez excit√©e F Devinez prisonni√®re prisonni√®re remporter embarras travers√¢mes croyait couvercle tra√Æn√© collation sucre serais √©pais principale sh√©rif\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He hesitated for a while.\n",
            "s√©ries suspendit quant pr√©f√®rerais Pr√©sentez accordez accordez Devinez prisonni√®re √©pais b√¢cl√© accordez cygnes pseudo-science programmes recontacterai concept √©claira posant accordez cygnes cygnes accordez climatiseur accordez Vide √©claira tombai moulins fain√©ant √©gar√© sous-sol suppl√©mentaire continuera accordez cygnes conducteur occup√©s boursier suspendit Devinez fain√©ant accordez port√®rent accordez circonstance dangereuse travailleur continua accordez joyeuses pension recontacterai d√©cideront accordez savais tondue V√©rifiez score\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I know I made a mistake.\n",
            "s√©ries suspendit sous-sol incomp√©tents emm√©nager allusion chiqu√© troupeau prisonni√®re √©pais b√¢cl√© Cendrillon croyait accordez indienne √©claira envoyer exemples marches insinuer su correcte karat√© incorrecte d√©tritus embauch√©es d√©clin√© d√©barrasserai compl√®tement accrochait V√©rifiez agr√©ablement dormi effraction puisqu'levais conducteur puisqu'V√©rifiez Coupe accordez V√©rifiez Cambodge port√®rent astucieux Saisissez sous-sol cordes injuste empilables d√©cod√© d√©cidai recontacterai d√©cideront accordez V√©rifiez br√©silienne oubliez ob√©ira\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I think you ought to pay more attention in class.\n",
            "syllabe r√©sidentiel chiqu√© √©ruption Hong fain√©ant accordez Devinez fain√©ant fain√©ant accordez Devinez fain√©ant accordez indienne reproduisent modestes refaire quatre-vingt-dix-sept accordez cygnes yoga n√©gociations tennis signatures puisqu'pique-niques r√©sonn√© devrais rompu r√©sidentiel Nombre musicien croyait accordez cygnes empilables accordez canettes accordez Devinez fain√©ant accordez saisit d√©baller chronom√®tre Quelques cordes cr√©dule programmes enleva fassions n√©gociations inspecteur consid√©rer Cookie pr√©visions forc√©s go√ªte\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Did I hurt you?\n",
            "s√©ries suspendit sous-sol incomp√©tents emm√©nager allusion chiqu√© troupeau prisonni√®re √©pais b√¢cl√© Cendrillon croyait accordez cygnes pseudo-science extraordinaire accordez cygnes v√©n√®re cygnes yoga empilables rompu exemples Jetons moment souhaiterions croyait incorrect sh√©rif volent souffrez r√©gl√©e leurs d√©cod√© conducteur prisonni√®re remarquablement furent hommes cin√©mas commen√ßait cuisinez d√©cod√© thon leurs originales plais changements √©galement tasses leurs id√©ogrammes insinu√© cahiers √©claira enregistr√©e devenez\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I'd be disappointed if I saw you doing that.\n",
            "s√©ries changements Donne-le-moi r√©sidentiel emm√©nager registre Hong Signez casser Davantage b√¢cl√© accordez servez sponsors √©pais fromage Parlez r√©fugi√©s joyeuses chapeau entendis Mieux tra√ßa adress√©e joyeuses trait√©s Change √©cout√¢mes ordres enseigna r√©sidentiel Nombre musicien croyait accordez vari√©t√© coupez V√©rifiez payai croyait accordez saisit inconsid√©r√© traversent fatigant consid√©rer menacez Question injuste empilables mangue passionnante possession formellement croyait bouch√©s trag√©die sursaut retrouve\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Why did you guys break up?\n",
            "s√©ries suspendit arm√©es √©pais bras parvint accordez rougi pr√©para √©pais b√¢cl√© Cendrillon croyait accordez attrap√©e fers organise partag√®rent compromette d√©couvrira impressionn√©es joui bachot√© marches mention cow persistant temp√™tes spray travers√¢mes pr√©cision √©coles traditionnel croyait ferme d√©cod√© criait-t appr√©cieriez adopt√®rent d√©placer chenille excit√©e remplis d√©cod√© thon mini-jupe d√©tritus embauch√©es confiants fun√©railles ent√™tant sh√©rif expuls√© accordez √©touffait pr√©visions forc√©s pr√Æmes\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "The room was very untidy.\n",
            "√©tatsunien lessive europ√©ens souffrez langue fain√©ant accordez rougi pr√©para √©pais b√¢cl√© accordez Reconnais accordez cygnes pseudo-science extraordinaire accordez cygnes chapeau figur√© mesure concerne honor√© d√©tritus embauch√©es r√©sidentiel torturez Entrez fain√©ant programmeur sh√©rif arpenteuse essayez compliment sous-sol suppl√©mentaire insoutenable sous-estim√©e traduit accordez saisit conservateur remplis d√©cod√© thon leurs cerisiers injuste empilables honor√© oubliez emprunt√© interrompue allusion mus√©es accordez oubliez ob√©ira\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He didn't say anything.\n",
            "s√©ries suspendit occup√©s type contentez fr√¥l√© accordez tenterai industrielle accordez joyeuses chapeau Constitution cinquantaine attrap√©e pleurniches joyeuses chapeau contentez enseigna √©gaux concevoir accordez climatiseur accordez conducteur r√©sidentiel craignez moulins Apollo go√ªte Reconnaissez musicien douleurs accordez savais leva suspendit gorge Bravo Devinez fain√©ant accordez port√®rent difficile d√©brouille allonge roula sh√©rif vieillissait croyait couvercle recontacterai d√©cideront accordez savais tondue rus√© f√™tes\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "After work, I go right home.\n",
            "s√©ries changements Donne-le-moi sac pleurait dissuader accordez Devinez prisonni√®re g√¢chis nocturne Retenez conducteur journ√©es f√¢che chienne fleuves croyait √©meute accordez cygnes yoga n√©gociations tennis d√©tritus embauch√©es d√©clin√© d√©barrasserai compl√®tement accrochait V√©rifiez humilit√© embauch√©es boursier puisqu'levais conducteur puisqu'baguette pleura puisqu'refuse puisqu'Service jean emm√©nager calice m√©moriser canettes empilables fun√©railles comptais √©chec bambin marcherons jalouses astucieux √©touffa idioties\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ</td></tr><tr><td>Train - top-1</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>Train - top-10</td><td>‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>Train - top-5</td><td>‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ</td></tr><tr><td>Validation - loss</td><td>‚ñÖ‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñà</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÑ</td></tr><tr><td>Validation - top-10</td><td>‚ñÇ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñÖ</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñÉ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>9.87689</td></tr><tr><td>Train - top-1</td><td>2e-05</td></tr><tr><td>Train - top-10</td><td>0.00056</td></tr><tr><td>Train - top-5</td><td>0.00019</td></tr><tr><td>Validation - loss</td><td>9.87546</td></tr><tr><td>Validation - top-1</td><td>0.0</td></tr><tr><td>Validation - top-10</td><td>0.00018</td></tr><tr><td>Validation - top-5</td><td>5e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">third architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_223858-egy0a07w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL3AfEPI0lOM",
        "outputId": "f6e2d77c-3f38-4cb5-8c36-561abb80dc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel gencives √©pais accordez Devinez fain√©ant document√©e enseigna accordez derni√®rement accordez attrap√©e conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde d√©clin√© perdez plais sh√©rif arpenteuse infinies programmes perdez bambin portail exercice attrap√©s luxueux Lib√©rez concept √©claira joyeuses chapeau exercice cordes injuste empilables fun√©railles comptais √©chec bambin pr√©cipiter bourrer enquise pr√™teriez virent\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel gencives √©pais accordez Devinez fain√©ant document√©e enseigna accordez derni√®rement accordez attrap√©e conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde d√©clin√© perdez plais sh√©rif arpenteuse infinies programmes perdez bambin portail exercice attrap√©s luxueux Lib√©rez concept √©claira joyeuses chapeau exercice cordes injuste empilables fun√©railles comptais √©chec bambin pr√©cipiter bourrer enquise pr√™teriez bicyclette\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel gencives √©pais accordez Devinez fain√©ant document√©e enseigna accordez derni√®rement accordez attrap√©e conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde d√©clin√© perdez plais sh√©rif arpenteuse infinies programmes perdez bambin portail exercice attrap√©s luxueux Lib√©rez concept √©claira joyeuses chapeau exercice cordes injuste empilables fun√©railles comptais √©chec bambin pr√©cipiter bourrer enquise pr√™teriez d√©tritus\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.02380952380952381\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.02380952380952381\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel gencives √©pais accordez Devinez fain√©ant document√©e enseigna accordez derni√®rement accordez attrap√©e conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde d√©clin√© perdez plais sh√©rif arpenteuse infinies programmes perdez bambin portail exercice attrap√©s luxueux Lib√©rez concept √©claira joyeuses chapeau exercice cordes injuste empilables fun√©railles comptais √©chec bambin pr√©cipiter bourrer enquise pr√™teriez bloqu√©e\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.02380952380952381\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.02380952380952381\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel gencives √©pais accordez Devinez fain√©ant document√©e enseigna accordez derni√®rement accordez attrap√©e conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde d√©clin√© perdez plais sh√©rif arpenteuse infinies programmes perdez bambin portail exercice attrap√©s luxueux Lib√©rez concept √©claira joyeuses chapeau exercice cordes injuste empilables fun√©railles comptais √©chec bambin pr√©cipiter bourrer enquise pr√™teriez score\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t s√©ries suspendit chienne outil professionnels pr√©venus cr√©dule rougi pr√©para √©pais b√¢cl√© accordez derni√®rement sermonna r√¢teau mourons joyeuses chapeau contentez Hong nuage luxueux dettes payerons tripe hibernent trahis fain√©ant d√©n√©gations enseigna r√©sidentiel raconterez arpenteuse infinies embauch√©es r√©sidentiel empilables accordez canettes veniez emprunt√© cin√©mas commen√ßait cuisinez absurde tasses leurs girafe accordez empilables universel fantastiques recontacterai d√©cideront cimeti√®re immobile Sale offre jouons Rencontrez\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is the result of this work ?\"\n",
        "reference = [\"Quel\", \"est\", \"le\", \"r√©sultat\", \"de\", \"ce\", \"travail\", \"?\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "YcTJiPnk8ziK",
        "outputId": "1d53522a-892f-4138-f4cd-af3fb374aa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel ni√© √©pais accordez Devinez fain√©ant document√©e enseigna accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables √©pargn√©e Baissez score virgule programmes d√©cod√© immerg√©e rattraperai Reprenons refuse Devinez fain√©ant document√©e indienne espion tasses leurs cerisiers melons fain√©ant chanceuse commen√ßait traversent retrait√© croyait talons enquise Attache ob√©ira\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04819277108433735\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04819277108433735\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel ni√© √©pais accordez Devinez fain√©ant document√©e enseigna accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables √©pargn√©e Baissez score virgule programmes d√©cod√© immerg√©e rattraperai Reprenons refuse Devinez fain√©ant document√©e indienne espion tasses leurs cerisiers melons fain√©ant chanceuse commen√ßait traversent retrait√© croyait talons enquise Attache saigneras\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel ni√© √©pais accordez Devinez fain√©ant document√©e enseigna accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables √©pargn√©e Baissez score virgule programmes d√©cod√© immerg√©e rattraperai Reprenons refuse Devinez fain√©ant document√©e indienne espion tasses leurs cerisiers melons fain√©ant chanceuse commen√ßait traversent retrait√© croyait talons enquise Attache estime\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel ni√© √©pais accordez Devinez fain√©ant document√©e enseigna accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables √©pargn√©e Baissez score virgule programmes d√©cod√© immerg√©e rattraperai Reprenons refuse Devinez fain√©ant document√©e indienne espion tasses leurs cerisiers melons fain√©ant chanceuse commen√ßait traversent retrait√© croyait talons enquise Attache organisons\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t s√©ries changements Donne-le-moi r√©sidentiel ni√© √©pais accordez Devinez fain√©ant document√©e enseigna accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables √©pargn√©e Baissez score virgule programmes d√©cod√© immerg√©e rattraperai Reprenons refuse Devinez fain√©ant document√©e indienne espion tasses leurs cerisiers melons fain√©ant chanceuse commen√ßait traversent retrait√© croyait talons enquise Attache applaudissait\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t s√©ries changements Donne-le-moi r√©sidentiel torturez aimeras chiqu√© nia croyait m√©moriser dauphins accordez croyait accordez indienne espion excit√©e remplis empilables Autrefois √©pais bras accordez devenez d√©tritus embauch√©es d√©clin√© d√©barrasserai industrielle accordez V√©rifiez veng√© embauch√©es croyait puisqu'refuse prospectus ravaler roses veniez emprunt√© poupon Devinez fous joyeuses tasses leurs score √©ducateur empilables universel Tes croyait dix-neuf-cent-cinquante-sept manteaux cimeti√®re enquise imprimer choisis cordes\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.047058823529411764\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.047058823529411764\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth architecture\n",
        "Big architecture. Values of the hyperparameters taken from the article Attention is all you need (name in the article : big)."
      ],
      "metadata": {
        "id": "od4A6GETEamU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "warmup_steps = 200\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 256,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 8,\n",
        "    'dim_embedding': 512,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.2,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "scheduler = CustomLRScheduler(config['optimizer'], config['dim_embedding'], warmup_steps)\n",
        "\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "dL8AVTo18zVS",
        "outputId": "df02c01a-65db-41fe-e67b-995580ca8e1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [256, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [256, 60, 512]            6,222,848\n",
              "‚îú‚îÄEmbedding: 1-2                                   [256, 60, 512]            262,144\n",
              "‚îú‚îÄEmbedding: 1-3                                   [256, 60, 512]            9,390,080\n",
              "‚îú‚îÄEmbedding: 1-4                                   [256, 60, 512]            262,144\n",
              "‚îú‚îÄTransformer: 1-5                                 [256, 60, 512]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [256, 60, 512]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        3,946,752\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [256, 60, 512]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        7,101,696\n",
              "‚îú‚îÄLinear: 1-6                                      [256, 60, 18340]          9,408,420\n",
              "====================================================================================================\n",
              "Total params: 36,594,084\n",
              "Trainable params: 36,594,084\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 9.37\n",
              "====================================================================================================\n",
              "Input size (MB): 0.25\n",
              "Forward/backward pass size (MB): 6280.15\n",
              "Params size (MB): 146.38\n",
              "Estimated Total Size (MB): 6426.77\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='fourth architecture',\n",
        "    ):\n",
        "    train_model(model, config,scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xkIY6luZPZKs",
        "outputId": "9a196ff0-445b-42fa-c0ef-bc6f4d346f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_231610-zaztjkmh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">fourth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Show me where it happened.\n",
            "toqu√© abattre accusez jur√©s remise aigre amen√©s paris souffre repassait r√©bellion allonger douche marque triomphe amusais assurance radio raval√© Profite venait Renvoie publicitaire int√©rieure sortirais satisfaisants test d√©clencher arr√™t√®rent achats enjeux handicap√© du profond diapo biologiste Achetez auto-stoppeurs Informez ferons serrurier moment d√©tritus essayai lyc√©enne hypocondriaque subir embrassais amie abuse affam√©e Arm√©nie tenez techniques f√©minine ratons √©quitable aimons soulign√©s\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I've been waiting all day for you.\n",
            "affili√© J‚Äô allonger d√©tourna envol√© courant tristes pistolets Hey opposants radios allonger douche marque triomphe Seize essaya repassait anodin Bas humiliation visions d√©barrasser d√©couragez crocodiles t√©moignerai graves prier musicien pigeon sabots Comporte gomme ch√¢teau commode cr√©dit Suisses trimestre choisi prouva intimidez lou√©e attaquent thermom√®tre d√©butent colons lois am√©lior√©s chr√©tiens douche M√™le morts concentr√©s douche aise fers √©quitable aimons soulign√©s\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Where do you usually go fishing?\n",
            "amus√®rent fis adoraient attirer athl√®tes aigre amen√©s elle atome fichez dangereux allonger douche marque triomphe venait r√©pondeur attaque v√©g√©tariennes robe toqu√© van Servez vacances fortement serr√©es divin brut musicien cl√¥ture pr√©cis√©ment Ton n√©gociations le√ßons habitons biologiste Achetez auto-stoppeurs Informez maline capitaine d√©sol√©es attaquent thermom√®tre d√©butent colons lapins mourant foutu fiert√© Seconde surestimez appr√©cierais modifi√© emm√®ne Chicago attendirent excuse nettoies\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "The doctor says that I'll never be able to swim again.\n",
            "parlera √©pris amen√©s Indiens toqu√© Effectuez expliquerais buissons manipulateur entrant dangereux allonger douche marque triomphe venait r√©pondeur attaque v√©g√©tariennes robe toqu√© expos√©s √âpluche compl√©ter repart Qu'sources n√©gatifs M√™le morts La obsession cor√©en connaissance comm√®re biologiste Achetez auto-stoppeurs Informez ferons nettoy√©s originale approcha an√©antie sida re√ßue dira embrassais amie connaissances plaisantent gagnions descendue ma emm√®ne souviens √©quitable van Marseille\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I'll let you know when it's done.\n",
            "parlera √©pris accusez jur√©s remise aigre amen√©s elle atome fichez dangereux allonger douche marque triomphe amusais assurance bougie influent sauter additionnelle √©couler r√©nov√© vacances fortement patiemment veiller satisfait gorge cl√¥ture puzzle Bois retir√©e Partez faufil√© partit doctorat contusionn√© pr√©parerai perdrai chatte chamade Autre mourrez sauter acupuncture repos r√©cup√©r√© Prendre br√ªl√© Serre d√©bris pr√©parerait modifi√© emm√®ne souviens soucies van Marseille\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "She taught him the tricks of the trade.\n",
            "amus√®rent fis pardonn√© entour√© date toqu√© amen√©s elle atome fichez dangereux allonger douche marque triomphe venait r√©pondeur attaque v√©g√©tariennes robe toqu√© expos√©s distribution significations invasion rendras reparler portier chanceuses musicien Barri√®re amie majeur canons coinc√© anthropologie achet√© abandonne colombe prisonni√®res changerons nourrissent cuisine toucher cirer sous-estim√©e quelques admira pr√©occuperais √©cout√© potentielles handball CD chass√© d√©licate enl√®vera √©quitable aimons soulign√©s\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "We eat to live, not live to eat.\n",
            "affili√© inqui√©tudes accusez survenait toqu√© nerveuse am√©liorer d√©√ßu Hey opposants radios allonger douche marque triomphe amusais assurance bougie trahirai consommez pilules dangereux aventures rentre d√©c√®de plombs aval√© munitions bruiner vint r√©p√©ta appr√©cieras insiste serra pr√©sidentielle biologiste Achetez auto-stoppeurs Informez ferons nettoy√©s √©preuves gaspiller spectateurs chang√©e ant√©c√©dents d√©sorganis√©e Jetons Pelez √©vacu√© Personnellement remarquable prouve distribuer dissuada Bell √©quitable aimons soulign√©s\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "You are taller than me.\n",
            "toqu√© monopoles prenait critiqu√© entra√Ænons aigre amen√©s perdante hydrog√®ne occuperai radios allonger douche marque triomphe Seize hypoxie repassait anodin lib√®rerai chanter J‚Äô Payez vacances fortement contr√¥le su donation musicien agonie Ils √©tudies majeur dussiez d√©sintoxication avocat employ√© m√©lang√© 60 per√ßante dormit soleil diff√®re souciait biologiste pointait chauffard batte √©gay√© avari√©e tenez matchs existent spaghetti d√©licate enl√®vera √©quitable van Marseille\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Tom got fired.\n",
            "amus√®rent fis adoraient attirer athl√®tes aigre amen√©s elle atome fichez dangereux allonger douche marque effronterie choisi hypoxie journ√©e montiez mec tondrai repos√© siffler r√©gl√©e prier abandonnera veiller production anglaise Pardonne explosion fantasque terrifi√©e douche aise fromage bienvenu tante agonie cowboy avis√©s d√©sol√©es attaquent thermom√®tre d√©butent colons tourn√© descendants amie connaissances divertissante teinturier maigrichon Ferme symphonies laverie persuada Retourne mentir\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He died a few days before his hundredth birthday.\n",
            "parlera √©pris suspendis deviendras flic Apportez-le-moi S√©cession bloquait verglac√© propri√©t√© dangereux allonger douche marque disons troisi√®me √©pais tienne J‚Äô fantasque tra√Æn√© athl√®tes fermes vacances fortement informer r√©sonna avoua morceau Interdit moment Ton n√©gociations le√ßons corneilles ironique instrument assise int√©rieure mah l√®che r√©daction h√©rita vieillissons locuteurs dormais grenouille attaquent Voyager pardonn√© √©prouve fanfare examiner tra√Ætresse cirer sous-estim√©e arr√™tions posent bordent\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>Train - top-10</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>Train - top-5</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÖ</td></tr><tr><td>Validation - loss</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñÉ</td></tr><tr><td>Validation - top-1</td><td>‚ñà‚ñá‚ñÇ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÅ‚ñÇ</td></tr><tr><td>Validation - top-10</td><td>‚ñá‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñà‚ñÉ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>9.99004</td></tr><tr><td>Train - top-1</td><td>7e-05</td></tr><tr><td>Train - top-10</td><td>0.00048</td></tr><tr><td>Train - top-5</td><td>0.00025</td></tr><tr><td>Validation - loss</td><td>10.00034</td></tr><tr><td>Validation - top-1</td><td>2e-05</td></tr><tr><td>Validation - top-10</td><td>0.00042</td></tr><tr><td>Validation - top-5</td><td>0.00018</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fourth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_231610-zaztjkmh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "7GU_eHJXilmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bf5a47-23de-46c7-eb66-a8e25f4b42d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t toqu√© proc√©der r√©imprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisi√®me √©pais tienne J‚Äô fantasque terrifi√©e expos√©s √âpluche compl√©ter Serait bowling J‚Äô envoient musicien indiens coinc√©s dureront remarquable √©charde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons m√©prise paisible d√©clare dira formellement exag√©r√© sortirais irr√©parable adh√©sif appuyer piq√ªre d√©licate enl√®vera √©quitable aimons soulign√©s\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t toqu√© proc√©der r√©imprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisi√®me √©pais tienne J‚Äô fantasque terrifi√©e expos√©s √âpluche compl√©ter Serait bowling J‚Äô envoient musicien indiens coinc√©s dureront remarquable √©charde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons m√©prise paisible d√©clare dira formellement exag√©r√© sortirais irr√©parable adh√©sif appuyer piq√ªre d√©licate enl√®vera √©quitable aimons soup√©\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t toqu√© proc√©der r√©imprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisi√®me √©pais tienne J‚Äô fantasque terrifi√©e expos√©s √âpluche compl√©ter Serait bowling J‚Äô envoient musicien indiens coinc√©s dureront remarquable √©charde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons m√©prise paisible d√©clare dira formellement exag√©r√© sortirais irr√©parable adh√©sif appuyer piq√ªre d√©licate enl√®vera √©quitable aimons dose\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t toqu√© proc√©der r√©imprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisi√®me √©pais tienne J‚Äô fantasque terrifi√©e expos√©s √âpluche compl√©ter Serait bowling J‚Äô envoient musicien indiens coinc√©s dureront remarquable √©charde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons m√©prise paisible d√©clare dira formellement exag√©r√© sortirais irr√©parable adh√©sif appuyer piq√ªre d√©licate enl√®vera √©quitable aimons viendront\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t toqu√© proc√©der r√©imprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisi√®me √©pais tienne J‚Äô fantasque terrifi√©e expos√©s √âpluche compl√©ter Serait bowling J‚Äô envoient musicien indiens coinc√©s dureront remarquable √©charde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons m√©prise paisible d√©clare dira formellement exag√©r√© sortirais irr√©parable adh√©sif appuyer piq√ªre d√©licate enl√®vera √©quitable aimons deviens\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t toqu√© proc√©der pointez souvient concrets perdus soleil agence corr√©lation √©chou√©e essayai s√©curit√© chaleureusement accepterais survivrons noix anorexique survivantes bancale flottant chanter dangereux aventures elle aptitude vint opposent envoient musicien indiens coinc√©s dureront remarquable √©charde couper ironique instrument assise int√©rieure nettoya magazine grossiers r√©√©lu refroidit com√©die essayiez oignons int√©ressais pr√©vention obscur Rome explique sp√©cialis√©e jardinage angles √©chang√®rent confectionn√© tue √âgypte poussaient\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the third and fourth architecture have a loss that is too high. The main possible reason is the number of parameters. Also the learning rate scheduler doesn't help either. Let's stick to smaller architectures."
      ],
      "metadata": {
        "id": "i2JLYsUvZw4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fifth architecture"
      ],
      "metadata": {
        "id": "reg8cpu0aJbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 14,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 4,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZP1Sqx9aAcN",
        "outputId": "300b9315-ad91-4f25-d73f-b153cf1745fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄEmbedding: 1-2                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄEmbedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄEmbedding: 1-4                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄTransformer: 1-5                                 [128, 60, 196]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        1,024,144\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        1,643,504\n",
              "‚îú‚îÄLinear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 12,458,156\n",
              "Trainable params: 12,458,156\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.59\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 2216.02\n",
              "Params size (MB): 49.83\n",
              "Estimated Total Size (MB): 2265.97\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='fifth architecture',\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3IjYl67saVwc",
        "outputId": "aa441d14-6827-4d2c-95ba-2a3b262edf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250318_133659-merbp2h8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">fifth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.65     top-1: 0.67    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.51     top-1: 0.69    top-5: 0.86    top-10: 0.89\n",
            "He wrote to me from time to time.\n",
            "Il m'a √©crit de temps √† temps.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.35     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.25     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Have you ever gone skinny dipping?\n",
            "Avez-vous jamais √©t√© aussi maigre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.20     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "Tom believed it.\n",
            "Tom le croyait.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.11     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.05     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "What was the weather like yesterday?\n",
            "Quel temps √©tait le temps, hier ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.07     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Who saw me?\n",
            "Qui m'a vu ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.01     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "They are talking about what they will sing.\n",
            "Ils parlent de ce qu'ils vont chanter.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Comfort Tom.\n",
            "<unk> Tom.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm starting to get bored.\n",
            "Je commence √† m'ennuyer.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.90     top-1: 0.78    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "What did you think I'd do?\n",
            "Qu'avez-vous pens√© que je ferais ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.87     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.92     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "Tom didn't get home until after 2:30 this morning.\n",
            "Tom n'est pas arriv√© jusqu'√† la maison ce matin.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.86598</td></tr><tr><td>Train - top-1</td><td>0.78602</td></tr><tr><td>Train - top-10</td><td>0.95928</td></tr><tr><td>Train - top-5</td><td>0.93873</td></tr><tr><td>Validation - loss</td><td>0.91573</td></tr><tr><td>Validation - top-1</td><td>0.78578</td></tr><tr><td>Validation - top-10</td><td>0.95304</td></tr><tr><td>Validation - top-5</td><td>0.93345</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fifth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250318_133659-merbp2h8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sixth architecture"
      ],
      "metadata": {
        "id": "PWXizdV2fOyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous architectures were big architectures. Let's try with a smaller architecture"
      ],
      "metadata": {
        "id": "qssBVVz4f-Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 3,\n",
        "    'dim_embedding': 30,\n",
        "    'dim_hidden': 64,\n",
        "    'n_layers': 2,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoKLQQqGleHf",
        "outputId": "72a37b1d-f47f-463a-e276-09321ab30404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [128, 60, 30]             364,620\n",
              "‚îú‚îÄEmbedding: 1-2                                   [128, 60, 30]             15,360\n",
              "‚îú‚îÄEmbedding: 1-3                                   [128, 60, 30]             550,200\n",
              "‚îú‚îÄEmbedding: 1-4                                   [128, 60, 30]             15,360\n",
              "‚îú‚îÄTransformer: 1-5                                 [128, 60, 30]             --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [128, 60, 30]             --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        15,548\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [128, 60, 30]             --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        23,108\n",
              "‚îú‚îÄLinear: 1-6                                      [128, 60, 18340]          568,540\n",
              "====================================================================================================\n",
              "Total params: 1,552,736\n",
              "Trainable params: 1,552,736\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 198.75\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1219.95\n",
              "Params size (MB): 6.21\n",
              "Estimated Total Size (MB): 1226.29\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='sixth architecture',\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m6BbA3bqfdX-",
        "outputId": "f45a8645-4f81-4e4f-d6f1-98e01ef9d4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250318_141218-yx2opww2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">sixth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 3.45     top-1: 0.42    top-5: 0.59    top-10: 0.65\n",
            "Eval -    loss: 3.28     top-1: 0.44    top-5: 0.61    top-10: 0.67\n",
            "She arranged the flowers beautifully.\n",
            "Elle sait le fran√ßais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.93     top-1: 0.48    top-5: 0.67    top-10: 0.73\n",
            "Eval -    loss: 2.71     top-1: 0.51    top-5: 0.69    top-10: 0.75\n",
            "Beef is very expensive.\n",
            "Peu.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.61     top-1: 0.53    top-5: 0.72    top-10: 0.77\n",
            "Eval -    loss: 2.40     top-1: 0.55    top-5: 0.74    top-10: 0.79\n",
            "He decided to submit his resignation.\n",
            "Il a d√©cid√© de sa fille.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.43     top-1: 0.55    top-5: 0.74    top-10: 0.79\n",
            "Eval -    loss: 2.19     top-1: 0.58    top-5: 0.77    top-10: 0.81\n",
            "The cats are safe.\n",
            "Les chats sont diff√©rents.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.26     top-1: 0.57    top-5: 0.77    top-10: 0.81\n",
            "Eval -    loss: 2.06     top-1: 0.60    top-5: 0.79    top-10: 0.83\n",
            "I was on my way to work.\n",
            "J'√©tais sur mon chemin de travail.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.15     top-1: 0.59    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 1.96     top-1: 0.62    top-5: 0.80    top-10: 0.84\n",
            "No one knows what to say.\n",
            "Personne ne sait ce qui veut dire.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.06     top-1: 0.60    top-5: 0.79    top-10: 0.84\n",
            "Eval -    loss: 1.88     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "If for some reason that happened, what would you do?\n",
            "Si vous trouve une raison de quoi que tu ferais ¬† ?\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.99     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.82     top-1: 0.63    top-5: 0.82    top-10: 0.86\n",
            "What do you consider your greatest achievement?\n",
            "Que penses-tu ton album ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.77     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "Many students go to Europe for the purpose of studying music.\n",
            "Beaucoup d'√©tudiants en Europe pour apprendre la musique.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.89     top-1: 0.62    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.73     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You didn't keep your word.\n",
            "Vous n'avez pas tenu votre mot.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.89182</td></tr><tr><td>Train - top-1</td><td>0.6234</td></tr><tr><td>Train - top-10</td><td>0.85933</td></tr><tr><td>Train - top-5</td><td>0.81553</td></tr><tr><td>Validation - loss</td><td>1.73038</td></tr><tr><td>Validation - top-1</td><td>0.64691</td></tr><tr><td>Validation - top-10</td><td>0.86973</td></tr><tr><td>Validation - top-5</td><td>0.83049</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sixth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250318_141218-yx2opww2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A smaller architecture fails to do good translation"
      ],
      "metadata": {
        "id": "2SlPH5CFqHAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "rloGZXI0FfQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_Qx5gffKckj0IY959hygnMm0f-KWrCX-\" alt=\"bayes_net\" width=\"900\"/>"
      ],
      "metadata": {
        "id": "bn2lo-UeFgnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=12h360IOUV1cg7hjlGF9lNK0CbjA_rciC\" alt=\"bayes_net\" width=\"900\"/>\n"
      ],
      "metadata": {
        "id": "Kze4nGtqs7ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best architecture\n",
        "The best architecture seems to be the second transformer architecture (based on its loss).\n",
        "But the first architecture seems to generalize better. Because the translation of the sentence \"Iron Maiden is the best band in the world\" is better with the first architecture. So the second architecture may be overfitting. Increase the number of heads doesn't necessarily lead to better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qcE_JAk9Vq8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "0sAIeu7FWARJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 12,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UmGgeBGOD4Js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70776e86-343d-48ea-b8a0-285116189baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "‚îú‚îÄEmbedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "‚îú‚îÄEmbedding: 1-2                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄEmbedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "‚îú‚îÄEmbedding: 1-4                                   [128, 60, 196]            100,352\n",
              "‚îú‚îÄTransformer: 1-5                                 [128, 60, 196]            --\n",
              "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-1                        --                        256,036\n",
              "‚îÇ    ‚îî‚îÄTransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                        --                        410,876\n",
              "‚îú‚îÄLinear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 10,457,420\n",
              "Trainable params: 10,457,420\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.34\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1435.24\n",
              "Params size (MB): 41.83\n",
              "Estimated Total Size (MB): 1477.19\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Best architecture',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='first architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rLH1ryQjAcYg",
        "outputId": "f808abc4-b08f-428f-9d0e-a67d82d129a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250323_123743-pjp8tc3r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">first architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "Eval -    loss: 1.74     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You never asked what I wanted.\n",
            "Tu n'as jamais demand√© ce que je voulais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.60     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Have you read this book already?\n",
            "Avez-vous d√©j√† lu ce livre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.43     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.31     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "He is playing in his room.\n",
            "Il joue dans sa chambre.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.35     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.23     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I wish my girlfriend would spend more time with me.\n",
            "J'aimerais que ma petite amie passe davantage de temps avec moi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.24     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.18     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "He was so sad that he almost went mad.\n",
            "Il √©tait si triste qu'il est parti.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "You can get a loan from a bank.\n",
            "Tu peux obtenir un pr√™t de banque.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "I swear I didn't make this up.\n",
            "Je jure que je ne faisais pas √ßa.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.11     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I had to stop.\n",
            "J'ai d√ª arr√™ter.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.08     top-1: 0.76    top-5: 0.91    top-10: 0.94\n",
            "CEO's of American corporations are paid several times their Japanese counterparts.\n",
            "Les habitants de l'immobilier a √©t√© pay√© plusieurs fois leurs japonaise.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom is trying to make sure that everything is ready.\n",
            "Tom essaie d'essayer de assurer que tout est pr√™t.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.05     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I have to stop you from doing that. \"Stop me from doing what?\"\n",
            "¬´ ¬† Je dois t'emp√™cher de faire √ßa. ¬† ¬ª ¬´ ¬† Pas de quoi ¬† ? ¬† ¬ª\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I'm only going to show you once.\n",
            "Je ne vais te montrer tout de suite.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train - top-1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-10</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train - top-5</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation - top-1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-10</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation - top-5</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.0026</td></tr><tr><td>Train - top-1</td><td>0.76263</td></tr><tr><td>Train - top-10</td><td>0.94921</td></tr><tr><td>Train - top-5</td><td>0.92455</td></tr><tr><td>Validation - loss</td><td>1.03892</td></tr><tr><td>Validation - top-1</td><td>0.76393</td></tr><tr><td>Validation - top-10</td><td>0.94266</td></tr><tr><td>Validation - top-5</td><td>0.91919</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">first architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250323_123743-pjp8tc3r/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1pd1UTt__FOzQNR2Py9jkqERuHNTOeStp\" alt=\"bayes_net\" width=\"700\"/>"
      ],
      "metadata": {
        "id": "H8eXrpKjKdjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1NqdYPzqIOTi1SVRBIDoJrDi88cnQ44Zn\" alt=\"bayes_net\" width=\"700\"/>\n"
      ],
      "metadata": {
        "id": "rEW8IhX_J0nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "PonKtwzKWBe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def evaluate_model_on_dataset(model, sentences, references, config, method='greedy', beam_width=10, k=5):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a list of sentences and references.\n",
        "\n",
        "    Args:\n",
        "        model: The translation/generation model.\n",
        "        sentences (List[str]): List of source sentences.\n",
        "        references (List[List[str]]): List of reference token lists.\n",
        "        config (dict): Configuration parameters.\n",
        "        method (str): Either 'greedy' or 'beam' search.\n",
        "        beam_width (int): Beam width if using beam search.\n",
        "        k (int): Number of beam candidates to consider.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of metrics dictionaries for each sentence.\n",
        "    \"\"\"\n",
        "    all_metrics = []\n",
        "    for sentence, reference in zip(sentences, references):\n",
        "        if method == 'greedy':\n",
        "            translation = greedy_search(\n",
        "                model,\n",
        "                sentence,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                config['device'],\n",
        "                max_sentence_length=config['max_sequence_length']\n",
        "            )\n",
        "            print(\"sentence : \", sentence)\n",
        "            print(\"translation : \", translation.split())\n",
        "            print(\"reference : \", reference)\n",
        "            print()\n",
        "            metrics = compute_metrics(reference, translation.split())\n",
        "        elif method == 'beam':\n",
        "            preds = beam_search(\n",
        "                model,\n",
        "                sentence,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                config['device'],\n",
        "                beam_width=beam_width,\n",
        "                max_target=100,\n",
        "                max_sentence_length=config['max_sequence_length']\n",
        "            )[:k]\n",
        "            # Pick the best translation (first candidate)\n",
        "            translation, likelihood = preds[0]\n",
        "            print(\"translation : \", translation.split())\n",
        "            print(\"reference : \", reference)\n",
        "            metrics = compute_metrics(reference, translation.split())\n",
        "        else:\n",
        "            raise ValueError(\"Invalid method: choose 'greedy' or 'beam'\")\n",
        "        all_metrics.append(metrics)\n",
        "    return all_metrics\n",
        "\n",
        "def aggregate_metrics(all_metrics):\n",
        "    \"\"\"\n",
        "    Aggregates metrics by computing the average for each metric.\n",
        "\n",
        "    Args:\n",
        "        all_metrics (List[dict]): List of metrics for each sentence.\n",
        "\n",
        "    Returns:\n",
        "        dict: Average metrics.\n",
        "    \"\"\"\n",
        "    avg_metrics = {}\n",
        "    keys = all_metrics[0].keys()\n",
        "    for key in keys:\n",
        "        avg_metrics[key] = np.mean([m[key] for m in all_metrics])\n",
        "    return avg_metrics\n",
        "\n",
        "def plot_metrics_distribution(all_metrics):\n",
        "    \"\"\"\n",
        "    Plots histograms for the distribution of each metric.\n",
        "\n",
        "    Args:\n",
        "        all_metrics (List[dict]): List of metrics for each sentence.\n",
        "    \"\"\"\n",
        "    metrics_names = list(all_metrics[0].keys())\n",
        "    data = {metric: [m[metric] for m in all_metrics] for metric in metrics_names}\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, metric in enumerate(metrics_names):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.hist(data[metric], bins=20, color='skyblue', edgecolor='black')\n",
        "        plt.title(f\"Distribution of {metric}\")\n",
        "        plt.xlabel(metric)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_average_metrics(avg_metrics):\n",
        "    \"\"\"\n",
        "    Plots a bar chart of average metrics.\n",
        "\n",
        "    Args:\n",
        "        avg_metrics (dict): Dictionary of average metrics.\n",
        "    \"\"\"\n",
        "    metrics = list(avg_metrics.keys())\n",
        "    values = list(avg_metrics.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(metrics, values, color='lightgreen', edgecolor='black')\n",
        "    plt.title(\"Average Metrics over the Dataset\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Average Score\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AGL9mjszWDYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "        {\n",
        "            \"sentence\": \"Iron Maiden is the best band in the world\",\n",
        "            \"reference\": [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"The quick brown fox jumps over the lazy dog\",\n",
        "            \"reference\": [\"Le\", \"renard\", \"brun\", \"rapide\", \"saute\", \"par-dessus\", \"le\", \"chien\", \"paresseux\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"Artificial intelligence is transforming our world\",\n",
        "            \"reference\": [\"L'intelligence\", \"artificielle\", \"transforme\", \"notre\", \"monde\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"Hello world\",\n",
        "            \"reference\": [\"Bonjour\", \"le\", \"monde\"]\n",
        "        },\n",
        "         {\n",
        "            \"sentence\": \"This is a test sentence\",\n",
        "            \"reference\": [\"Ceci\", \"est\", \"une\", \"phrase\", \"de\", \"test\"]\n",
        "        },\n",
        "        {\"sentence\": \"The weather is nice today.\", \"reference\": \"Le temps est agr√©able aujourd'hui.\".split()},\n",
        "        {\"sentence\": \"I enjoy reading books in the park.\", \"reference\": \"J'aime lire des livres dans le parc.\".split()},\n",
        "        {\"sentence\": \"She loves to play the piano.\", \"reference\": \"Elle aime jouer du piano.\".split()},\n",
        "        {\"sentence\": \"The children are playing in the garden.\", \"reference\": \"Les enfants jouent dans le jardin.\".split()},\n",
        "        {\"sentence\": \"He is working on a new project.\", \"reference\": \"Il travaille sur un nouveau projet.\".split()},\n",
        "        {\"sentence\": \"They visited the museum last weekend.\", \"reference\": \"Ils ont visit√© le mus√©e le week-end dernier.\".split()},\n",
        "        {\"sentence\": \"Our team won the championship.\", \"reference\": \"Notre √©quipe a gagn√© le championnat.\".split()},\n",
        "        {\"sentence\": \"The food at that restaurant is delicious.\", \"reference\": \"La nourriture dans ce restaurant est d√©licieuse.\".split()},\n",
        "        {\"sentence\": \"I will travel to France next summer.\", \"reference\": \"Je voyagerai en France l'√©t√© prochain.\".split()},\n",
        "        {\"sentence\": \"Technology is changing the world rapidly.\", \"reference\": \"La technologie change le monde rapidement.\".split()},\n",
        "        {\"sentence\": \"She is learning to speak Spanish.\", \"reference\": \"Elle apprend √† parler espagnol.\".split()},\n",
        "        {\"sentence\": \"Music brings joy to our lives.\", \"reference\": \"La musique apporte de la joie √† nos vies.\".split()},\n",
        "        {\"sentence\": \"He enjoys hiking in the mountains.\", \"reference\": \"Il aime faire de la randonn√©e dans les montagnes.\".split()},\n",
        "        {\"sentence\": \"The movie was both entertaining and thought-provoking.\", \"reference\": \"Le film √©tait √† la fois divertissant et stimulant.\".split()},\n",
        "        {\"sentence\": \"They are planning a surprise party for their friend.\", \"reference\": \"Ils pr√©voient une f√™te surprise pour leur ami.\".split()}\n",
        "\n",
        "    ]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4eK_V0yKXOZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sample_data\n",
        "sentences = [item[\"sentence\"] for item in dataset]\n",
        "references = [item[\"reference\"] for item in dataset]\n",
        "\n",
        "# Evaluate the model on the dataset using greedy search.\n",
        "all_metrics = evaluate_model_on_dataset(model, sentences, references, config, method='greedy')\n",
        "avg_metrics = aggregate_metrics(all_metrics)\n",
        "print(\"Average Metrics over the Dataset:\")\n",
        "for metric, value in avg_metrics.items():\n",
        "  print(f\"{metric}: {value}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6fR2Zl5HkiL",
        "outputId": "2af69435-ed31-4b48-c769-f3e7e44d9d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence :  Iron Maiden is the best band in the world\n",
            "translation :  ['Le', 'Le', 'vin', 'est', 'le', 'meilleur', 'groupe', 'dans', 'le', 'monde.']\n",
            "reference :  ['Iron', 'Maiden', 'est', 'le', 'meilleur', 'groupe', 'au', 'monde']\n",
            "\n",
            "sentence :  The quick brown fox jumps over the lazy dog\n",
            "translation :  ['Le', 'centre-ville', 'marron', 'le', 'renard.']\n",
            "reference :  ['Le', 'renard', 'brun', 'rapide', 'saute', 'par-dessus', 'le', 'chien', 'paresseux']\n",
            "\n",
            "sentence :  Artificial intelligence is transforming our world\n",
            "translation :  [\"L'intelligence\", 'artificielle', 'est', 'plus', 'grande', 'que', 'le', 'monde', 'soit', 'au', 'monde.']\n",
            "reference :  [\"L'intelligence\", 'artificielle', 'transforme', 'notre', 'monde']\n",
            "\n",
            "sentence :  Hello world\n",
            "translation :  ['Salut', 'le', 'monde.']\n",
            "reference :  ['Bonjour', 'le', 'monde']\n",
            "\n",
            "sentence :  This is a test sentence\n",
            "translation :  [\"C'est\", 'un', 'examen', 'd‚Äô', 'une', 'phrase.']\n",
            "reference :  ['Ceci', 'est', 'une', 'phrase', 'de', 'test']\n",
            "\n",
            "sentence :  The weather is nice today.\n",
            "translation :  ['Le', 'temps', 'est', 'beau', \"aujourd'hui.\"]\n",
            "reference :  ['Le', 'temps', 'est', 'agr√©able', \"aujourd'hui.\"]\n",
            "\n",
            "sentence :  I enjoy reading books in the park.\n",
            "translation :  [\"J'aime\", 'lire', 'des', 'livres', 'dans', 'le', 'parc.']\n",
            "reference :  [\"J'aime\", 'lire', 'des', 'livres', 'dans', 'le', 'parc.']\n",
            "\n",
            "sentence :  She loves to play the piano.\n",
            "translation :  ['Elle', 'aime', 'jouer', 'du', 'piano.']\n",
            "reference :  ['Elle', 'aime', 'jouer', 'du', 'piano.']\n",
            "\n",
            "sentence :  The children are playing in the garden.\n",
            "translation :  ['Les', 'enfants', 'jouent', 'dans', 'le', 'jardin.']\n",
            "reference :  ['Les', 'enfants', 'jouent', 'dans', 'le', 'jardin.']\n",
            "\n",
            "sentence :  He is working on a new project.\n",
            "translation :  ['Il', 'travaille', 'sur', 'un', 'nouveau', 'projet.']\n",
            "reference :  ['Il', 'travaille', 'sur', 'un', 'nouveau', 'projet.']\n",
            "\n",
            "sentence :  They visited the museum last weekend.\n",
            "translation :  ['Ils', 'ont', 'visit√©', 'le', 'mus√©e', 'le', 'week-end', 'dernier.']\n",
            "reference :  ['Ils', 'ont', 'visit√©', 'le', 'mus√©e', 'le', 'week-end', 'dernier.']\n",
            "\n",
            "sentence :  Our team won the championship.\n",
            "translation :  ['Notre', '√©quipe', 'a', 'gagn√©', 'le', 'championnat.']\n",
            "reference :  ['Notre', '√©quipe', 'a', 'gagn√©', 'le', 'championnat.']\n",
            "\n",
            "sentence :  The food at that restaurant is delicious.\n",
            "translation :  ['La', 'nourriture', 'au', 'restaurant', 'est', 'd√©licieuse.']\n",
            "reference :  ['La', 'nourriture', 'dans', 'ce', 'restaurant', 'est', 'd√©licieuse.']\n",
            "\n",
            "sentence :  I will travel to France next summer.\n",
            "translation :  ['Je', 'vais', 'voyager', '√†', 'la', 'France', \"l'√©t√©\", 'prochain.']\n",
            "reference :  ['Je', 'voyagerai', 'en', 'France', \"l'√©t√©\", 'prochain.']\n",
            "\n",
            "sentence :  Technology is changing the world rapidly.\n",
            "translation :  ['La', 'technologie', 'change', 'du', 'monde', 'est', 'rapidement.']\n",
            "reference :  ['La', 'technologie', 'change', 'le', 'monde', 'rapidement.']\n",
            "\n",
            "sentence :  She is learning to speak Spanish.\n",
            "translation :  ['Elle', 'apprend', '√†', 'parler', 'espagnol.']\n",
            "reference :  ['Elle', 'apprend', '√†', 'parler', 'espagnol.']\n",
            "\n",
            "sentence :  Music brings joy to our lives.\n",
            "translation :  ['La', 'musique', 'classique', 'de', 'notre', 'vie', '√†', 'notre', 'vies.']\n",
            "reference :  ['La', 'musique', 'apporte', 'de', 'la', 'joie', '√†', 'nos', 'vies.']\n",
            "\n",
            "sentence :  He enjoys hiking in the mountains.\n",
            "translation :  ['Il', 'aime', 'la', 'randonn√©e', 'dans', 'les', 'montagnes.']\n",
            "reference :  ['Il', 'aime', 'faire', 'de', 'la', 'randonn√©e', 'dans', 'les', 'montagnes.']\n",
            "\n",
            "sentence :  The movie was both entertaining and thought-provoking.\n",
            "translation :  ['Le', 'film', '√©tait', '√†', 'la', 'fois,', 'et', 'pensais', '¬´', '¬ª']\n",
            "reference :  ['Le', 'film', '√©tait', '√†', 'la', 'fois', 'divertissant', 'et', 'stimulant.']\n",
            "\n",
            "sentence :  They are planning a surprise party for their friend.\n",
            "translation :  ['Ils', 'ont', \"l'intention\", \"d'une\", 'surprise', 'pour', 'leur', 'amie.']\n",
            "reference :  ['Ils', 'pr√©voient', 'une', 'f√™te', 'surprise', 'pour', 'leur', 'ami.']\n",
            "\n",
            "Average Metrics over the Dataset:\n",
            "BLEU: 0.41025996480920907\n",
            "ROUGE-1: 0.7648611111111111\n",
            "ROUGE-2: 0.61386322011322\n",
            "ROUGE-L: 0.7586111111111111\n",
            "METEOR: 0.6578715069174248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of metrics and the average metrics.\n",
        "plot_metrics_distribution(all_metrics)\n",
        "plot_average_metrics(avg_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4CHAx6hWH17c",
        "outputId": "8dc4341d-2dde-4d98-9d5e-22bb49976a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "L7EyXM4BH341"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can perform good translations of small sentences with few ambiguities. The model has some difficulties translating words like ‚Äúat‚Äù, as they can have different translations and meanings in French. METEOR score is slight above 0.65 which indicates that the machine-generated translation is fairly close to alignment with human reference translations. However BLEU indicates that the translations are not so similar to the references."
      ],
      "metadata": {
        "id": "mD0BFuSXH58X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Conclusion"
      ],
      "metadata": {
        "id": "mJS7OsHXKoEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several types of architecture for translation. RNN does not seem to be sufficiently efficient for translation. GRU is an improvement on RNN, but still fails to generalize well. Transformers have potential for translation. But with a larger data set, they could lead to better performance."
      ],
      "metadata": {
        "id": "boMh0aq4KqdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Not Part of TP3, But A Potential Project Idea: Understanding the Architecture of a Decoder-Only Transformer\n",
        "\n",
        "Step 1: In a project group of 3-4 create a high level plan for a Decoder-Only model for how you would need to modify this code to implement a Decoder-Only Transformer. Key components of the implementation should be split up and each member of the group should present the pseudo-code (or actual code) for one component of the full model to one another, and in a report. Then create the working model and perform experiments comparing it with your TP3 encoder-decoder model.\n",
        "\n",
        "For more details on the Decoder-Only Transformer see [this blog post](https://medium.com/international-school-of-ai-data-science/building-custom-gpt-with-pytorch-59e5ba8102d4). The [first \"GPT\" paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), and the paper cited by this GPT-1 paper for the Decoder Only architecture used for GPT, [i.e. this paper](https://arxiv.org/abs/1801.10198)"
      ],
      "metadata": {
        "id": "U6-OpYG_hz2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxFnLwN1zI-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}