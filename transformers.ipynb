{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/Transformers-from-scratch/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        " Machine translation\n",
        "This project is originally from a course at Polytechnique Montréal called INF8225.\n",
        "The goal is to build a machine translation. Several architectures are used : RNN, GRU and Transformers.\n",
        "Do not forget to **select the runtime type as GPU!**\n",
        "\n",
        "**Sources**\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
        "\n",
        "*This notebook is quite big, use the table of contents to easily navigate through it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vLbVbH4lu4J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to use older versions of torchtext. Hence we need to install older versions of libraries."
      ],
      "metadata": {
        "id": "YVV4z5sSbzgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb > /dev/null\n",
        "import wandb"
      ],
      "metadata": {
        "id": "m27nUzQBthMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU2Ap1Vptj57",
        "outputId": "3c78918e-306a-4116-f19e-0af0e1841910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to run one of the two following cells. After you ran one of them, restart your session and run the same cell again. Then try to run the cell for the imports."
      ],
      "metadata": {
        "id": "6KIJl9afb9Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir > /dev/null # Downgrade numpy first\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "# Reinstall scipy and scikit-learn\n",
        "!pip install scipy --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install scikit-learn --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "!python3 -m spacy download en > /dev/null\n",
        "!python3 -m spacy download fr > /dev/null\n",
        "!pip install torchinfo > /dev/null\n",
        "!pip install einops > /dev/null\n",
        "!pip install wandb > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePmTpM8Ib5B9",
        "outputId": "cca6c731-6b58-4c76-91d2-3facb90623c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP\n",
        "\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir > /dev/null\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir > /dev/null\n",
        "\n",
        "\n",
        "!python3 -m spacy download en > /dev/null\n",
        "!python3 -m spacy download fr > /dev/null\n",
        "!pip install torchinfo > /dev/null\n",
        "!pip install einops > /dev/null\n",
        "!pip install wandb > /dev/null\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxQLxOjRb1KY",
        "outputId": "6ec51350-f528-4715-841d-36f12f9af1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.40.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.40.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### !pip install --upgrade --force-reinstall scipy scikit-learn"
      ],
      "metadata": {
        "id": "qcfM-DOEvlw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJQfREvFUdoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aaa7ea0-17f0-4951-86fc-6cf72abcc3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "# cpal\n",
        "print(torch.__version__)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxNpMbkvUfGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58676897-f363-4be0-c1e7-5ebc327982c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 12:35:46--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  4.29MB/s    in 1.8s    \n",
            "\n",
            "2025-03-23 12:35:48 (4.29 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "209462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"fr\" could not be loaded, trying \"fr_core_news_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "\n",
        "en_tokenizer, fr_tokenizer = get_tokenizer('spacy', language='en'), get_tokenizer('spacy', language='fr')\n",
        "\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the install doesn't work, try this :\n",
        "Run the next cell and restart your session. Run again this cell after the restart of your session."
      ],
      "metadata": {
        "id": "6tSCXVB9bmw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install scikit-learn==1.1.3 --force-reinstall --no-cache-dir\n",
        "!pip install scipy==1.9.3 --force-reinstall --no-cache-dir !pip install spacy einops wandb torchinfo\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "YCRDEUTfbp-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdictimport numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pdimport torch\n",
        "# cpal\n",
        "print(torch.__version__)import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequenceimport torchtext\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016import spacyimport einops\n",
        "import wandb\n",
        "from torchinfo import summaryDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ViT75IqHbq_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zipdf = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))en_nlp = spacy.load('en_core_web_sm')\n",
        "fr_nlp = spacy.load('fr_core_news_sm')def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in en_nlp.tokenizer(text)]def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in fr_nlp.tokenizer(text)]SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "qRxfBCeubxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ],
      "metadata": {
        "id": "ppPj9CrnsSoW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "\n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "# Models architecture\n",
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse.\n",
        "\n",
        "---\n",
        "\n",
        "In pytorch, there is no dinstinction between an intermediate layer or a whole model having multiple layers in itself.\n",
        "Every layers or models inherit from the `torch.nn.Module`.\n",
        "This module needs to define the `__init__` method where you instanciate the layers,\n",
        "and the `forward` method where you decide how the inputs and the layers of the module interact between them.\n",
        "Thanks to the autograd computations of pytorch, you do not have\n",
        "to implement any backward method!\n",
        "\n",
        "A really important advice is to **always look at\n",
        "the shape of your input and your output.**\n",
        "From that, you can often guess how the layers should interact\n",
        "with the inputs to produce the right output.\n",
        "You can also easily detect if there's something wrong going on.\n",
        "\n",
        "You are more than advised to use the `einops` library and the `torch.einsum` function. This will require less operations than 'classical' code, but note that it's a bit trickier to use.\n",
        "This is a way of describing tensors manipulation with strings, bypassing the multiple tensor methods executed in the background.\n",
        "You can find a nice presentation of `einops` [here](https://einops.rocks/1-einops-basics/).\n",
        "A paper has just been released about einops [here](https://paperswithcode.com/paper/einops-clear-and-reliable-tensor).\n",
        "\n",
        "**A great tutorial on pytorch can be found [here](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).**\n",
        "Spending 3 hours on this tutorial is *no* waste of time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN models"
      ],
      "metadata": {
        "id": "xodRThXg2DHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN\n",
        "Here, the implementation of the RNN is provided as an example. Study this code and use it as an example for the GRU implementation, if needed.\n",
        "\n",
        "The `RNNCell` layer produce one hidden state vector for each sentence in the batch\n",
        "(useful for the output of the encoder), and also produce one embedding for each\n",
        "token in each sentence (useful for the output of the decoder).\n",
        "\n",
        "The `RNN` module is composed of a stack of `RNNCell`. Each token embeddings\n",
        "coming out from a previous `RNNCell` is used as an input for the next `RNNCell` layer.\n",
        "\n",
        "**Be careful !** Our `RNNCell` implementation is not exactly the same thing as\n",
        "the PyTorch's `nn.RNNCell`. PyTorch implements only the operations for one token\n",
        "(so you would need to loop through each tokens inside the `RNN` instead).\n",
        "\n",
        "The same thing apply for the `GRU` and `GRUCell`.\n"
      ],
      "metadata": {
        "id": "ZvfRVUKm1u8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A single RNN layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "\n",
        "    Important note: This layer does not exactly the same thing as nn.RNNCell does.\n",
        "    PyTorch implementation is only doing one simple pass over one token for each batch.\n",
        "    This implementation is taking the whole sequence of each batch and provide the\n",
        "    final hidden state along with the embeddings of each token in each sequence.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # See pytorch definition: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.Wih = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"Go through all the sequence in x, iteratively updating\n",
        "        the hidden state h.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_size = x.shape\n",
        "        y = torch.zeros([batch_size, seq_len, self.hidden_size], device=DEVICE)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "          input = x[:, t, :]\n",
        "          w_input = self.Wih(input)\n",
        "          w_hidden = self.Whh(h)\n",
        "          h = self.act(w_input + w_hidden)\n",
        "          y[:, t, :] = self.dropout(h)\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"Implementation of an RNN based\n",
        "    on https://pytorch.org/docs/stable/generated/torch.nn.RNN.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers (RNNCell or GRUCell).\n",
        "        dropout: Dropout rate.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "            This parameter can be removed if you decide to use the module `GRU`.\n",
        "            Indeed, `GRU` should have exactly the same code as this module,\n",
        "            but with `GRUCell` instead of `RNNCell`. We let the freedom for you\n",
        "            to decide at which level you want to specialise the modules (either\n",
        "            in `TranslationRNN` by creating a `GRU` or a `RNN`, or in `RNN`\n",
        "            by creating a `GRUCell` or a `RNNCell`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        model_class = RNNCell if model_type == 'RNN' else GRUCell\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(model_class(input_size, hidden_size, dropout))\n",
        "        for i in range(1, num_layers):\n",
        "          self.layers.append(model_class(hidden_size, hidden_size, dropout))\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"Pass the input sequence through all the RNN cells.\n",
        "        Returns the output and the final hidden state of each RNN layer\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Hidden state for each RNN layer.\n",
        "                Can be None, in which case an initial hidden state is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Output embeddings for each token after the RNN layers.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "        \"\"\"\n",
        "        input = x\n",
        "        h = torch.zeros([x.shape[0], len(self.layers), self.hidden_size], device=x.device) if h is None else h\n",
        "        final_h = torch.zeros_like(h, device=x.device)\n",
        "        for l in range(len(self.layers)):\n",
        "          input, h_out = self.layers[l](input, h[:, l, :])\n",
        "          final_h[:, l, :] = h_out\n",
        "\n",
        "        return input, final_h"
      ],
      "metadata": {
        "id": "RiNKnwScM5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "Here you have to implement a GRU-RNN. This architecture is close to the Vanilla RNN but perform different operations. Look up the pytorch documentation to figure out the differences."
      ],
      "metadata": {
        "id": "I0ciaamtvK0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"Implementation of a GRU based on https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        print(\"GRU !!\")\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create multiple GRU layers\n",
        "        self.layers = nn.ModuleList([GRUCell(input_size, hidden_size, dropout)])\n",
        "        self.layers.extend([GRUCell(hidden_size, hidden_size, dropout) for _ in range(1, num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state for each layer.\n",
        "                If 'None', then an initial hidden state (a zero filled tensor)\n",
        "                is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            output:\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h_n: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden size].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size = x.shape[0]\n",
        "        # If h is not provided, initialize with zeros\n",
        "        if h is None:\n",
        "            h = torch.zeros(batch_size, self.num_layers, self.hidden_size, device=x.device)\n",
        "        final_h = torch.zeros_like(h, device=x.device)\n",
        "\n",
        "        input_seq = x\n",
        "        # Pass through each GRU layer\n",
        "        for l, cell in enumerate(self.layers):\n",
        "            input_seq, h_out = cell(input_seq, h[:, l, :])\n",
        "            final_h[:, l, :] = h_out\n",
        "\n",
        "        return input_seq, final_h\n",
        "\n",
        "\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    \"\"\"A single GRU layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        print(\"GRUCell\")\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Gates for update (z) and reset (r)\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "\n",
        "        # Candidate hidden state\n",
        "        self.Win = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        y = torch.zeros(batch_size, seq_len, self.hidden_size, device=x.device)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Compute update and reset gates\n",
        "            z_t = self.sigmoid(self.Wiz(x_t) + self.Whz(h))\n",
        "            r_t = self.sigmoid(self.Wir(x_t) + self.Whr(h))\n",
        "\n",
        "            # Compute candidate hidden state\n",
        "            n_t = self.tanh(self.Win(x_t) + self.Whn(r_t * h))\n",
        "\n",
        "            # Update hidden state\n",
        "            h = (1 - z_t) * n_t + z_t * h\n",
        "\n",
        "            # Apply dropout and store the output for this time step\n",
        "            y[:, t, :] = self.dropout(h)\n",
        "\n",
        "        return y, h\n"
      ],
      "metadata": {
        "id": "xdAMSZ7EMrMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation RNN\n",
        "\n",
        "This module instanciates a vanilla RNN or a GRU-RNN and performs the translation task. This code des the following:\n",
        "* Encodes the source and target sequence\n",
        "* Passes the final hidden state of the encoder to the decoder (one for each layer)\n",
        "* Decodes the hidden state into the target sequence\n",
        "\n",
        "We use teacher forcing for training, meaning that when the next token is predicted, that prediction is based on the previous true target tokens."
      ],
      "metadata": {
        "id": "boIetZUy1f-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD-6N17xhuLy"
      },
      "outputs": [],
      "source": [
        "class TranslationRNN(nn.Module):\n",
        "    \"\"\"Basic RNN encoder and decoder for a translation task.\n",
        "    It can run as a vanilla RNN or a GRU-RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the hidden layers in the RNNs\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the RNNs.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.src_embeddings = nn.Embedding(n_tokens_src, dim_embedding, src_pad_idx)\n",
        "        self.tgt_embeddings = nn.Embedding(n_tokens_tgt, dim_embedding, tgt_pad_idx)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.norm = nn.LayerNorm(dim_hidden)\n",
        "        self.decoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.out_layer = nn.Linear(dim_hidden, n_tokens_tgt)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logits based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, tgt_seq_len, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        source = torch.fliplr(source)\n",
        "\n",
        "        src_emb = self.src_embeddings(source)\n",
        "        out, hidden = self.encoder(src_emb)\n",
        "\n",
        "        hidden = self.norm(hidden)\n",
        "\n",
        "        tgt_emb = self.tgt_embeddings(target)\n",
        "        y, hidden = self.decoder(tgt_emb, hidden)\n",
        "\n",
        "        y = self.out_layer(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XLrMrwpjeOq8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RI_8zCdm4YQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer models\n",
        "Here you have to code the Full Transformer and Decoder-Only Transformer architectures.\n",
        "It is divided in three parts:\n",
        "* Attention layers (done individually)\n",
        "* Encoder and decoder layers (done individually)\n",
        "* Full Transformer: gather the encoder and decoder layers (done individually)\n",
        "\n",
        "The Transformer (or \"Full Transformer\") is presented in the paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ],
      "metadata": {
        "id": "EZcGlRnZvOnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n"
      ],
      "metadata": {
        "id": "ZwaVFTTUlYwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    This PE module comes from:\n",
        "    Pytorch. (2021). LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1).to(DEVICE)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).to(DEVICE)\n",
        "        pe = torch.zeros(max_len, 1, d_model).to(DEVICE)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, \"b s e -> s b e\")\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x = rearrange(x, \"s b e -> b s e\")\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "lIqHye2Vl3gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ],
      "metadata": {
        "id": "OFxV-6M3402p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "\n",
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention between the keys and the queries.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "\n",
        "    head_dim = q.size(-1)\n",
        "    scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(head_dim)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "    output = torch.einsum(\"bhqk,bkhd->bqhd\", attn, v)\n",
        "    return output, attn\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        # TODO\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = dim // n_heads\n",
        "\n",
        "        # Projection layers for queries, keys, values.\n",
        "        self.w_q = nn.Linear(dim, dim)\n",
        "        self.w_k = nn.Linear(dim, dim)\n",
        "        self.w_v = nn.Linear(dim, dim)\n",
        "        # Output projection.\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size, seq_len_q, _ = q.shape\n",
        "        batch_size, seq_len_k, _ = k.shape\n",
        "\n",
        "        # Linear projections and reshape for multiple heads.\n",
        "        q = self.w_q(q)\n",
        "        k = self.w_k(k)\n",
        "        v = self.w_v(v)\n",
        "        # Rearranging: split the last dimension into (n_heads, head_dim)\n",
        "        q = rearrange(q, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "        k = rearrange(k, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "        v = rearrange(v, \"b t (h d) -> b t h d\", h=self.n_heads)\n",
        "\n",
        "        # Prepare masks if provided.\n",
        "        mask = None\n",
        "        if key_padding_mask is not None:\n",
        "            # Expand mask to shape [batch, 1, 1, seq_len_k]\n",
        "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "        if attn_mask is not None:\n",
        "            # attn_mask: [seq_len_q, seq_len_k] -> [1, 1, seq_len_q, seq_len_k]\n",
        "            attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
        "            mask = attn_mask if mask is None else mask | attn_mask\n",
        "\n",
        "        # Compute attention.\n",
        "        attn_output, attn = attention(q, k, v, mask=mask, dropout=self.attn_dropout)\n",
        "        # Rearranging back to [batch, seq_len_q, dim]\n",
        "        attn_output = rearrange(attn_output, \"b t h d -> b t (h d)\")\n",
        "        output = self.fc(attn_output)\n",
        "        output = self.proj_dropout(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ],
      "metadata": {
        "id": "nIpHjOtK47DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.cross_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Self-attention over tgt (with causal mask)\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, key_padding_mask=tgt_key_padding_mask, attn_mask=tgt_mask_attn)\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        # Cross-attention: query=tgt, key/value=src (encoded source)\n",
        "        tgt2 = self.cross_attn(tgt, src, src, key_padding_mask=src_key_padding_mask)\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        # Feed-forward network\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layer:int ,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, d_ff, nhead, dropout)\n",
        "            for _ in range(num_decoder_layer)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        output = tgt\n",
        "        for layer in self.layers:\n",
        "            output = layer(src, output, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        src2 = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=None)\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm1(src)\n",
        "        # Feed-forward network\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, dim_feedforward, nheads, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, key_padding_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer\n",
        "This section gathers the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ],
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        self.encoder = TransformerEncoder(d_model, dim_feedforward, num_encoder_layers, nhead, dropout)\n",
        "        self.decoder = TransformerDecoder(d_model, dim_feedforward, num_decoder_layers, nhead, dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        memory = self.encoder(src, key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(memory, tgt, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            max_seq_length: int = 512\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Token embeddings\n",
        "        self.src_embedding = nn.Embedding(n_tokens_src, dim_embedding)\n",
        "        self.tgt_embedding = nn.Embedding(n_tokens_tgt, dim_embedding)\n",
        "        # Learnable positional embeddings\n",
        "        self.src_pos_embedding = nn.Embedding(max_seq_length, dim_embedding)\n",
        "        self.tgt_pos_embedding = nn.Embedding(max_seq_length, dim_embedding)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads,\n",
        "            num_encoder_layers=n_layers,\n",
        "            num_decoder_layers=n_layers,\n",
        "            dim_feedforward=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        # Final linear projection to vocab size\n",
        "        self.fc_out = nn.Linear(dim_embedding, n_tokens_tgt)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Create masks\n",
        "        tgt_mask_attn = self.generate_causal_mask(target)\n",
        "        src_key_padding_mask, tgt_key_padding_mask = self.generate_key_padding_mask(source, target)\n",
        "\n",
        "        batch_size, src_seq_len = source.shape\n",
        "        batch_size, tgt_seq_len = target.shape\n",
        "        # Create position indices\n",
        "        src_positions = torch.arange(0, src_seq_len, device=source.device).unsqueeze(0).expand(batch_size, src_seq_len)\n",
        "        tgt_positions = torch.arange(0, tgt_seq_len, device=target.device).unsqueeze(0).expand(batch_size, tgt_seq_len)\n",
        "\n",
        "        # Embed tokens and add position embeddings\n",
        "        src_emb = self.src_embedding(source) + self.src_pos_embedding(src_positions)\n",
        "        tgt_emb = self.tgt_embedding(target) + self.tgt_pos_embedding(tgt_positions)\n",
        "\n",
        "        # Pass through Transformer (encoder-decoder)\n",
        "        transformer_out = self.transformer(src_emb, tgt_emb, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        # Project to vocabulary\n",
        "        logits = self.fc_out(transformer_out)\n",
        "        return logits\n",
        "\n",
        "    def generate_causal_mask(\n",
        "            self,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending subsequent tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [seq_len_tgt, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = target.shape[1]\n",
        "\n",
        "        tgt_mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
        "        tgt_mask = torch.triu(tgt_mask, diagonal=1).to(target.device)\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def generate_key_padding_mask(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending padding tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        src_key_padding_mask = source == self.src_pad_idx\n",
        "        tgt_key_padding_mask = target == self.tgt_pad_idx\n",
        "\n",
        "        return src_key_padding_mask, tgt_key_padding_mask"
      ],
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search\n",
        "\n",
        "One idea to explore once you have your model working is to implement a geedy search to generate a target translation from a trained model and an input source string. The next token will simply be the most probable one. Compare this strategy of decoding with the beam search strategy below."
      ],
      "metadata": {
        "id": "ql6jv2lAK-nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_sentence_length):\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            next_token = predicted[:, -1].argmax(dim=-1).unsqueeze(dim=1)\n",
        "\n",
        "            tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
        "\n",
        "            if next_token.item() == EOS_IDX:\n",
        "                break\n",
        "\n",
        "    tgt_sentence = list(tgt_tokens.squeeze().cpu().numpy())[1:]  # Remove <bos> token\n",
        "    tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "    tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "\n",
        "    tgt_sentence = beautify(tgt_sentence)\n",
        "    return tgt_sentence"
      ],
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search\n",
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always chooses the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k² sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ],
      "metadata": {
        "id": "LgGFG-uXue6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "\n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  # [batch_size, n_beams, n_tokens]\n",
        "    beams = beams.unsqueeze(dim=2)  # [batch_size, n_beams, 1]\n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)  # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences orderer by their likelihood.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        while tgt_tokens.shape[1] < max_sentence_length:\n",
        "            batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "            # Get next beams\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            probs, predicted = predicted[:, -1].topk(k=beam_width, dim=-1)\n",
        "\n",
        "            # Separe between terminated sentences and the others\n",
        "            idx_terminated, idx_not_terminated = indices_terminated(tgt_tokens, EOS_IDX)\n",
        "            idx_terminated, idx_not_terminated = idx_terminated.to(device), idx_not_terminated.to(device)\n",
        "\n",
        "            tgt_terminated = torch.index_select(tgt_tokens, dim=0, index=idx_terminated)\n",
        "            tgt_probs_terminated = torch.index_select(target_probs, dim=0, index=idx_terminated)\n",
        "\n",
        "            filter_t = lambda t: torch.index_select(t, dim=0, index=idx_not_terminated)\n",
        "            tgt_others = filter_t(tgt_tokens)\n",
        "            tgt_probs_others = filter_t(target_probs)\n",
        "            predicted = filter_t(predicted)\n",
        "            probs = filter_t(probs)\n",
        "\n",
        "            # Add the top tokens to the previous target sentences\n",
        "            tgt_others = append_beams(tgt_others, predicted)\n",
        "\n",
        "            # Add padding to terminated target\n",
        "            padd = torch.zeros((len(tgt_terminated), 1), dtype=torch.long, device=device)\n",
        "            tgt_terminated = torch.cat(\n",
        "                (tgt_terminated, padd),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Update each target sentence probabilities\n",
        "            tgt_probs_others = torch.repeat_interleave(tgt_probs_others, beam_width)\n",
        "            tgt_probs_others *= probs.flatten()\n",
        "            tgt_probs_terminated *= 0.999  # Penalize short sequences overtime\n",
        "\n",
        "            # Group up the terminated and the others\n",
        "            target_probs = torch.cat(\n",
        "                (tgt_probs_others, tgt_probs_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "            tgt_tokens = torch.cat(\n",
        "                (tgt_others, tgt_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "\n",
        "            # Keep only the top `max_target` target sentences\n",
        "            if target_probs.shape[0] <= max_target:\n",
        "                continue\n",
        "\n",
        "            target_probs, indices = target_probs.topk(k=max_target, dim=0)\n",
        "            tgt_tokens = torch.index_select(tgt_tokens, dim=0, index=indices)\n",
        "\n",
        "    sentences = []\n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "        sentences.append(tgt_sentence)\n",
        "\n",
        "    sentences = [beautify(s) for s in sentences]\n",
        "\n",
        "    # Join the sentences with their likelihood\n",
        "    sentences = [(s, p.item()) for s, p in zip(sentences, target_probs)]\n",
        "    # Sort the sentences by their likelihood\n",
        "    sentences = [(s, p) for s, p in sorted(sentences, key=lambda k: k[1], reverse=True)]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb"
      ],
      "metadata": {
        "id": "77fE9LtfZayA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking GPU and logging to wandb\n",
        "\n",
        "!wandb login\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WriScTUEsRHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18acba54-5d96-4db0-a69c-3a3c24ecdd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Thu Mar 20 12:19:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "JkXpm6Unb5g6",
        "outputId": "da46bdb8-e77b-4b88-f687-15f3f65770e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdUA-rftb-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)  # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()  # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict, LRscheduler: bool=None):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            if LRscheduler:\n",
        "                LRscheduler.step()\n",
        "            optimizer.step()\n",
        "\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "\n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "\n",
        "        # Logs\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "        val_pred, val_prob = beam_search(\n",
        "            model,\n",
        "            val_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "\n",
        "        logs = {**train_logs, **val_logs}  # Merge dictionnaries\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        train_pred, train_prob = beam_search(\n",
        "            model,\n",
        "            train_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob,\n",
        "            val_source, val_target, val_pred, val_prob,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "\n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ],
      "metadata": {
        "id": "YImgxCWjlWni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate the datasets\n",
        "\n",
        "MAX_SEQ_LEN = 60\n",
        "MIN_TOK_FREQ = 2\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ],
      "metadata": {
        "id": "iqmpxnO1lgDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5ca9c2-f08b-426c-d0b7-99a9603f9cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 12,154\n",
            "French vocabulary size: 18,340\n",
            "\n",
            "Training examples: 209,459\n",
            "Validation examples: 23,274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywFEpplOU5dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e0d961-f655-4aff-bf7c-bfbb45bd9592"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "├─Embedding: 1-2                                   [128, 60, 196]            100,352\n",
              "├─Embedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "├─Embedding: 1-4                                   [128, 60, 196]            100,352\n",
              "├─Transformer: 1-5                                 [128, 60, 196]            --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-1                        --                        768,108\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-2                        --                        1,232,628\n",
              "├─Linear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 11,791,244\n",
              "Trainable params: 11,791,244\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.51\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1955.76\n",
              "Params size (MB): 47.16\n",
              "Estimated Total Size (MB): 2003.05\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "\"\"\"model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\"\"\"\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maOTVtk4acxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cd228ab-0acb-4ec3-882a-7d5513b0740a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_125105-wcs5ro9p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">basic architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.71     top-1: 0.66    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.54     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "It's in the fridge.\n",
            "C'est dans le frigo.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.43     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Are you sure you know what to do?\n",
            "Êtes-vous sûr de savoir quoi faire ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.26     top-1: 0.72    top-5: 0.90    top-10: 0.92\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Lend him as much money as he needs.\n",
            "Prête-lui autant d'argent qu'il a besoin.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.17     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Eval -    loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I think it's impossible for him to solve the problem.\n",
            "Je pense qu'il est impossible de le résoudre.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom isn't the one who needs help.\n",
            "Tom n'est pas celui qui a besoin d'aide.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Are you productive?\n",
            "Êtes-vous productif ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "Because of the snow, I couldn't see anything.\n",
            "À cause de la neige, je ne pouvais rien voir.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Nobody recognized you.\n",
            "Personne ne vous a reconnu.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I know how old you are.\n",
            "Je sais comment vous êtes vieux.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Don't talk about my family.\n",
            "Ne parle pas de ma famille !\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▁▁▁▁▂▁▂</td></tr><tr><td>Train - top-1</td><td>▁▂▅▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██████</td></tr><tr><td>Train - top-10</td><td>▁▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇███▇▇▇█████</td></tr><tr><td>Train - top-5</td><td>▁▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.94289</td></tr><tr><td>Train - top-1</td><td>0.77542</td></tr><tr><td>Train - top-10</td><td>0.95308</td></tr><tr><td>Train - top-5</td><td>0.93074</td></tr><tr><td>Validation - loss</td><td>0.94087</td></tr><tr><td>Validation - top-1</td><td>0.78161</td></tr><tr><td>Validation - top-10</td><td>0.95074</td></tr><tr><td>Validation - top-5</td><td>0.93064</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">basic architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/wcs5ro9p</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_125105-wcs5ro9p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online  # online / offline / disabled to activate, deactivate or turn off WandB logging\n",
        "\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Test',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='basic architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PFIyvKUefdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37422df2-311d-422f-82c3-ac73c5f5c340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. (29.39885%) \t Il est possible d'essayer votre travail ici.\n",
            "1. (22.23845%) \t Il est possible d'essayer ton travail ici.\n",
            "2. (4.08434%) \t Il est possible de tenter ton travail ici.\n",
            "3. (3.98461%) \t Il est possible d'essayer ta travail ici.\n",
            "4. (3.38965%) \t Il est possible de tenter votre travail ici.\n"
          ]
        }
      ],
      "source": [
        "sentence = \"It is possible to try your work here.\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:5]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Questions\n",
        "1. Explain the differences between Vanilla RNN, GRU-RNN, Encoder-Decoder Transformer and Decoder-Only Transformer.\n",
        "\n",
        " Vanilla RNN process each sequence step-by-step. Vanilla RNN have a hidden state that is updated after each time step. The main limitation is that they suffer from vanishing and exploding gradients. Therefore it's hard for Vanilla RNN to learn long-term dependencies. That's why GRU-RNN were invented.\n",
        "GRU-RNN are a variation of RNNs. They include gating mechanisms to control information flow. They have reset and update gates that help address the vanishing gradient problem by preserving important information across longer sequences.\n",
        "Encoder-Decoder Transformer is an architecture that has two main components. The first component is an encoder that processes the input sequence. The second component is a  decoder that generates the output sequence. Transformers use self-attention mechanisms. The latter are highly parallelizable, allowing them to process entire sequences simultaneously unlike RNNs.\n",
        "Decoder-Only Transformers use only the decoder part of the transformer. It generates sequences by autoregression. It means each token is predicted based on previously generated tokens.\n",
        "\n",
        "2. Why is positionnal encoding necessary in Transformers and not in RNNs?\n",
        "\n",
        "RNNs process sequences step-by-step. The order of inputs is implicitly handled by the sequential nature of the architecture. Transformers process entire sequences at once using attention mechanisms, which have no inherent sense of order. Positional encoding allows transformer to have information about the position of tokens in the sequence, allowing the transformer to understand the relative order of tokens.\n",
        "\n",
        "3. Describe the preprocessing process. Detail how the initial dataset is processed before being fed to the translation models.\n",
        "\n",
        "First of all, there's tokenizaition. The input sentences are first tokenized into subwords, words, or characters depending on the tokenization strategy.\n",
        "\n",
        "After tokenization, each token is mapped into a dense vector representation (called embedding). The embedding layer converts tokens into vectors that capture syntactic and semantic meanings. In transformers, this embedding layer is learned during training.\n",
        "\n",
        "Since transformers process sequences in parallel, positional encodings are added to the embeddings to provide information about the position of each token in the sequence. This is crucial because a self-attention mechanism doesn't inherently understand token order.\n",
        "\n",
        "Sentences may have different lengths, so shorter sentences are padded to match the longest sentence in the batch. Padding tokens are added to ensure a uniform length for sequences.\n",
        "\n",
        "then, the tokenized sentences (along with their embeddings and positional encodings) are transformed into tensors that our model can process.\n",
        "\n",
        "Masks are then generated to ensure that padded tokens are ignored during attention calculations, preventing the model from focusing on irrelevant padding positions.\n",
        "\n",
        "And finally.The processed sequences are grouped into batches to make training more efficient. Batching helps leverage parallel computation.\n",
        "\n",
        "4. What is teacher forcing, and how is it used in Transformer training? How does the decoder input differ?\n",
        "\n",
        "Teacher forcing is a technique where during training the model uses the actual target sequence as input for the decoder instead of model output from the previous step. Teacher forcing can help the model to converge faster by providing the correct context. The decoder input differs in that during training. The model sees the actual target, while during inference, it sees its own previous outputs.\n",
        "\n",
        "5. How are the two types of mask important to the attention mechanism (causal and padding) and how do they work? How do they differ between the encoder and decoder?\n",
        "\n",
        "Causal Mask is used in the decoder to prevent future tokens from being visible to the current token. This ensures the model generates tokens one step at a time and based only on the previous tokens. It doesn't give \"spoilers\" to the model.\n",
        "Padding Mask is used to mask padded positions in both the encoder and decoder, ensuring the model doesn’t attend to padded positions that are irrelevant.\n",
        "The encoder uses only padding masks, while the decoder uses both causal and padding masks. Causal masking is critical in the decoder to ensure autoregressive generation.\n",
        "\n",
        "6. What is a causal mask, and why is it only used in the decoder?\n",
        "\n",
        "The causal mask ensures that each token in the sequence only attends to past tokens and not future tokens during generation. This ensures the model predicts the next token based on the previous tokens and only the previous tokens. It is only used in the decoder because the decoder is responsible for autoregressive output generation, where tokens are generated one by one.\n",
        "\n",
        "7. Why does the decoder use both self-attention and encoder-decoder attention?\n",
        "\n",
        "The self-attention mechanism in the decoder helps the model to consider the relationships between previously generated tokens. The encoder-decoder attention helps the decoder focus on relevant parts of the input sequence when generating each output token.\n",
        "\n",
        "8. Why is the Transformer model parallelizable, and how does this improve efficiency compared to RNNs?\n",
        "\n",
        "Unlike RNNs, which process tokens sequentially, transformers can process entire sequences in parallel. This is because self-attention mechanism allows the model to process different parts of an input sequence simultaneously, rather than sequentially. And also a head of self attention does not need the other heads. It can work independently and in parallel.\n",
        "\n",
        "9. How does multi-head self-attention allow the model to capture different aspects of a sentence?\n",
        "\n",
        "Multi-head attention allows the model to apply multiple attention mechanisms in parallel. The interesting thing is that each head can focus on different parts of the sentence, capturing various relationships : syntax, semantics, or long-range dependencies. This helps the model gain a richer understanding of the input.\n",
        "\n",
        "10. What does the decoder's final output represent before the projection layer? What does the encoder's final output represent?\n",
        "\n",
        "the decoder’s output is a set of contextualized token embeddings that represent the understanding of the model of the sequence and the input from the encoder.\n",
        "The final output of the encoder is a set of embeddings representing the entire input sequence, which are used by the decoder to generate the output.\n",
        "\n",
        "11. What is the role of the final linear projection layer in the decoder?\n",
        "How does the decoder output differ between training (parallel processing) and inference (sequential generation)?\n",
        "\n",
        "The role of the final linear projection layer is to map the decoder's output embeddings to the size of the target vocabulary. It transforms the model’s contextualized representations into logits for each possible output token. In training, the decoder can process entire sequences in parallel, while during inference, tokens are generated sequentially, using the previously generated tokens as input.\n",
        "\n",
        "12. Why does the decoder recompute all outputs at each inference step instead of appending new outputs incrementally?\n",
        "\n",
        "Transformers recompute the entire sequence during each inference because it ensures consistency in the attention mechanism. And also each new token influences the attention distribution. Recomputing allows the model to incorporate the newly generated token in the context when predicting the next token."
      ],
      "metadata": {
        "id": "uHhixEEGzWRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small report - experiments"
      ],
      "metadata": {
        "id": "Y3tQdusIjPCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "This section presents the other metrics I used to evaluate a model."
      ],
      "metadata": {
        "id": "JZkYhzv8Rlgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics used :\n",
        "\n",
        "\n",
        "*   [BLEU](https://en.wikipedia.org/wiki/BLEU) : evaluates how closely a machine-generated translation matches reference translations by comparing overlapping n-grams. BLEU compares the n-grams (sequences of words) in the machine-translated output to the n-grams in a reference translation. It focuses on precision : how many of the generated n-grams appear in the reference. BLEU also includes a brevity penalty to account for overly short translations.\n",
        "*   [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric) : set of metrics that evaluates text based on recall by measuring the overlap between machine-generated content and reference text, often used in summarization tasks.\n",
        "* [METEOR](https://en.wikipedia.org/wiki/METEOR) : designed to address some of BLEU’s limitations by focusing on both precision and recall, and it also incorporates synonyms and stemming. METEOR aligns words in the generated translation and reference text based on exact matches, synonyms, and paraphrases. It then computes precision and recall scores, with recall weighted higher than precision. METEOR includes a penalty for fragmented alignments (when matches are far apart in the sentence).\n",
        "\n"
      ],
      "metadata": {
        "id": "beF_wQV5TWkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "z-XQ3dxjFIR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ff4ab2-b8a0-439e-908e-48d1d590bec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=042495c18222755ed58fdd4c958e1b30d9dcf9f9ef9de0c15d0d76b5ae9acd47\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "eSe0pZAIRs-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49393fba-29a4-4a6a-d191-2e98edc0eff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes BLEU score between the reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (List[str]): List of reference sentences.\n",
        "        hypothesis (List[str]): The predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score.\n",
        "    \"\"\"\n",
        "    return sentence_bleu([reference], hypothesis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_rouge(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes ROUGE scores between reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (str): Reference sentence.\n",
        "        hypothesis (str): Predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE scores (1, 2, L).\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    return scorer.score(reference, hypothesis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_meteor(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Computes METEOR score between the reference and hypothesis.\n",
        "\n",
        "    Args:\n",
        "        reference (List[str]): List of reference sentences.\n",
        "        hypothesis (str): Predicted sentence.\n",
        "\n",
        "    Returns:\n",
        "        float: METEOR score.\n",
        "    \"\"\"\n",
        "    return meteor_score([reference], hypothesis)\n",
        "\n",
        "def compute_metrics(reference, hypothesis):\n",
        "    bleu = compute_bleu(reference, hypothesis)\n",
        "    rouge = compute_rouge(' '.join(reference), ' '.join(hypothesis))\n",
        "    meteor = compute_meteor(reference, hypothesis)\n",
        "\n",
        "    metrics = {\n",
        "        'BLEU': bleu,\n",
        "        'ROUGE-1': rouge['rouge1'].fmeasure,\n",
        "        'ROUGE-2': rouge['rouge2'].fmeasure,\n",
        "        'ROUGE-L': rouge['rougeL'].fmeasure,\n",
        "        'METEOR': meteor\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DiOfqI8EOi65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(model, sentence, reference, config, k=5, beam_width=10, max_target=100):\n",
        "  preds = beam_search(\n",
        "      model,\n",
        "      sentence,\n",
        "      config['src_vocab'],\n",
        "      config['tgt_vocab'],\n",
        "      config['src_tokenizer'],\n",
        "      config['device'],\n",
        "      beam_width=beam_width,\n",
        "      max_target=max_target,\n",
        "      max_sentence_length=config['max_sequence_length']\n",
        "  )[:k]\n",
        "  print(\"Beam Search : \")\n",
        "  for i, (translation, likelihood) in enumerate(preds):\n",
        "      print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')\n",
        "      metrics = compute_metrics(reference, translation.split())\n",
        "      for metric, value in metrics.items():\n",
        "          print(f\"{metric}: {value}\")\n",
        "      print()\n",
        "\n",
        "\n",
        "  pred_greedy_search = greedy_search(\n",
        "      model,\n",
        "      sentence,\n",
        "      config['src_vocab'],\n",
        "      config['tgt_vocab'],\n",
        "      config['src_tokenizer'],\n",
        "      config['device'],\n",
        "      max_sentence_length=config['max_sequence_length']\n",
        "  )\n",
        "\n",
        "  print(f'Greedy search: \\t {pred_greedy_search}')\n",
        "\n",
        "  metrics_greedy = compute_metrics(reference, pred_greedy_search.split())\n",
        "  for metric, value in metrics_greedy.items():\n",
        "      print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "id": "d5qvbt1Y6yg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate scheduler"
      ],
      "metadata": {
        "id": "uZ5LPtB2E8Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A learning rate scheduler is a tool used in training deep learning models to adjust the learning rate over time. This tool is used in the article Attention is all you need."
      ],
      "metadata": {
        "id": "mo6X5Kut_xMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLRScheduler:\n",
        "    def __init__(self, optimizer : optim, d_model : int, warmup_steps : int):\n",
        "        \"\"\" Initialize the scheduler\n",
        "        Args:\n",
        "            optimizer: The optimizer whose learning rate needs to be adjusted.\n",
        "            d_model: The model's dimensionality.\n",
        "            warmup_steps: The number of warmup steps.\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the learning rate based on the current step number.\"\"\"\n",
        "        self.step_num += 1\n",
        "        lr = self._compute_lr()\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def _compute_lr(self):\n",
        "        \"\"\"Compute the learning rate according to the formula.\"\"\"\n",
        "        lr = (self.d_model ** -0.5) * min(\n",
        "            self.step_num ** -0.5,\n",
        "            self.step_num * (self.warmup_steps ** -1.5)\n",
        "        )\n",
        "        return lr"
      ],
      "metadata": {
        "id": "2UJ8NmnxE-FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "bjQ17dWRjAMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 5,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "id": "VRR07kL92B7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0293f10-4811-473a-8550-738ea42e0706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TranslationRNN                           [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                         [128, 60, 196]            2,382,184\n",
              "├─RNN: 1-2                               [128, 60, 256]            --\n",
              "│    └─ModuleList: 2-1                   --                        --\n",
              "│    │    └─RNNCell: 3-1                 [128, 60, 256]            116,224\n",
              "│    │    └─RNNCell: 3-2                 [128, 60, 256]            131,584\n",
              "│    │    └─RNNCell: 3-3                 [128, 60, 256]            131,584\n",
              "├─LayerNorm: 1-3                         [128, 3, 256]             512\n",
              "├─Embedding: 1-4                         [128, 60, 196]            3,594,640\n",
              "├─RNN: 1-5                               [128, 60, 256]            --\n",
              "│    └─ModuleList: 2-2                   --                        --\n",
              "│    │    └─RNNCell: 3-4                 [128, 60, 256]            116,224\n",
              "│    │    └─RNNCell: 3-5                 [128, 60, 256]            131,584\n",
              "│    │    └─RNNCell: 3-6                 [128, 60, 256]            131,584\n",
              "├─Linear: 1-6                            [128, 60, 18340]          4,713,380\n",
              "==========================================================================================\n",
              "Total params: 11,449,500\n",
              "Trainable params: 11,449,500\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 7.20\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1340.42\n",
              "Params size (MB): 45.80\n",
              "Estimated Total Size (MB): 1386.35\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='RNN',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='RNN',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QvkYM1i52L9u",
        "outputId": "e630cc00-a202-4b17-ef01-c9ad87acbfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_130114-ubskng3g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">RNN</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 5 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.77     top-1: 0.49    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.68     top-1: 0.50    top-5: 0.68    top-10: 0.74\n",
            "Whose room is this?\n",
            "À quelle heure est-ce ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.48     top-1: 0.53    top-5: 0.71    top-10: 0.77\n",
            "Eval -    loss: 2.41     top-1: 0.53    top-5: 0.72    top-10: 0.78\n",
            "The house was on fire.\n",
            "La maison était ouverte.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.35     top-1: 0.54    top-5: 0.73    top-10: 0.79\n",
            "Eval -    loss: 2.28     top-1: 0.54    top-5: 0.74    top-10: 0.79\n",
            "There isn't enough coffee for everyone.\n",
            "Il n'y a pas de travail à faire.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.29     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "Eval -    loss: 2.19     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "He lost his eyesight.\n",
            "Il perdit ses promesses.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.19     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.14     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "She is used to staying up all night.\n",
            "Elle est allé à l'école aujourd'hui.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▄▄▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▂▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>Train - top-5</td><td>▁▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Validation - loss</td><td>█▄▃▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▆▇█</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▇█</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.19423</td></tr><tr><td>Train - top-1</td><td>0.55791</td></tr><tr><td>Train - top-10</td><td>0.80902</td></tr><tr><td>Train - top-5</td><td>0.75152</td></tr><tr><td>Validation - loss</td><td>2.14071</td></tr><tr><td>Validation - top-1</td><td>0.56359</td></tr><tr><td>Validation - top-10</td><td>0.81431</td></tr><tr><td>Validation - top-5</td><td>0.75849</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">RNN</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ubskng3g</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_130114-ubskng3g/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "uz8nKzh32nHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51762329-f176-4ef0-d645-ec7381a60000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00007%) \t Le Japon est la capitale de la gare.\n",
            "BLEU: 1.0832677820940877e-231\n",
            "ROUGE-1: 0.25\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.125\n",
            "METEOR: 0.125\n",
            "\n",
            "1. (0.00005%) \t Le Japon est la troisième source de classe.\n",
            "BLEU: 1.0832677820940877e-231\n",
            "ROUGE-1: 0.23529411764705882\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.11764705882352941\n",
            "METEOR: 0.125\n",
            "\n",
            "2. (0.00005%) \t Le monde est la capitale de la gare.\n",
            "BLEU: 1.2882297539194154e-231\n",
            "ROUGE-1: 0.375\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.25\n",
            "METEOR: 0.1875\n",
            "\n",
            "3. (0.00004%) \t Le monde est le plus grand fleuve du monde.\n",
            "BLEU: 6.7393716283177006e-155\n",
            "ROUGE-1: 0.35294117647058826\n",
            "ROUGE-2: 0.13333333333333333\n",
            "ROUGE-L: 0.35294117647058826\n",
            "METEOR: 0.3155006858710563\n",
            "\n",
            "4. (0.00003%) \t Les gens ont vu la différence entre les deux filles.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.06097560975609756\n",
            "\n",
            "Greedy search: \t Les gens sont la même source de la nourriture de la vie.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.05952380952380953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the work is done\"\n",
        "reference = [\"le\", \"travail\", \"est\", \"fait\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "sYchP9WAtyQq",
        "outputId": "5b64c0f5-7cce-4c50-b915-77b1142e6698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (1.14373%) \t Le téléphone sonne.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.22222222222222224\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.22222222222222224\n",
            "METEOR: 0.12820512820512822\n",
            "\n",
            "1. (0.18161%) \t Le chien sonne.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.28571428571428575\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.28571428571428575\n",
            "METEOR: 0.12820512820512822\n",
            "\n",
            "2. (0.14133%) \t La réponse.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.13488%) \t Le chien s'est produit.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.4444444444444445\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.4444444444444445\n",
            "METEOR: 0.125\n",
            "\n",
            "4. (0.09769%) \t Je l'aime.\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t Le chien est en train de rêver.\n",
            "BLEU: 1.1200407237786664e-231\n",
            "ROUGE-1: 0.3333333333333333\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.3333333333333333\n",
            "METEOR: 0.23255813953488375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "JI7UXoByjBwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 5,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'GRU',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "id": "Z0xhfY01jCzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f4fe8c-8a5e-4fcd-e7de-da00625e4cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n",
            "GRUCell\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TranslationRNN                           [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                         [128, 60, 196]            2,382,184\n",
              "├─RNN: 1-2                               [128, 60, 256]            --\n",
              "│    └─ModuleList: 2-1                   --                        --\n",
              "│    │    └─GRUCell: 3-1                 [128, 60, 256]            348,672\n",
              "│    │    └─GRUCell: 3-2                 [128, 60, 256]            394,752\n",
              "│    │    └─GRUCell: 3-3                 [128, 60, 256]            394,752\n",
              "├─LayerNorm: 1-3                         [128, 3, 256]             512\n",
              "├─Embedding: 1-4                         [128, 60, 196]            3,594,640\n",
              "├─RNN: 1-5                               [128, 60, 256]            --\n",
              "│    └─ModuleList: 2-2                   --                        --\n",
              "│    │    └─GRUCell: 3-4                 [128, 60, 256]            348,672\n",
              "│    │    └─GRUCell: 3-5                 [128, 60, 256]            394,752\n",
              "│    │    └─GRUCell: 3-6                 [128, 60, 256]            394,752\n",
              "├─Linear: 1-6                            [128, 60, 18340]          4,713,380\n",
              "==========================================================================================\n",
              "Total params: 12,967,068\n",
              "Trainable params: 12,967,068\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 18.85\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1717.91\n",
              "Params size (MB): 51.87\n",
              "Estimated Total Size (MB): 1769.90\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='RNN',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='GRU',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MidOn3oqkAp",
        "outputId": "939b038e-5190-4776-ea06-0041e50800b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_122208-ai3x7sks</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">GRU</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 5 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.90     top-1: 0.63    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.79     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "Whose room is this?\n",
            "À qui est cette pièce ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.49     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.45     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "The house was on fire.\n",
            "La maison était feu.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.34     top-1: 0.70    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.32     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "There isn't enough coffee for everyone.\n",
            "Il n'y a pas assez de café pour tout le monde.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.25     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "He lost his eyesight.\n",
            "Il perdit la vue.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.17     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "She is used to staying up all night.\n",
            "Elle est habitué à rester debout toute la nuit.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▂▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▄▆▆▆▆▆▆▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▄▂▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▅▇▇█</td></tr><tr><td>Validation - top-10</td><td>▁▅▇██</td></tr><tr><td>Validation - top-5</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.17065</td></tr><tr><td>Train - top-1</td><td>0.7324</td></tr><tr><td>Train - top-10</td><td>0.93063</td></tr><tr><td>Train - top-5</td><td>0.90248</td></tr><tr><td>Validation - loss</td><td>1.21256</td></tr><tr><td>Validation - top-1</td><td>0.72795</td></tr><tr><td>Validation - top-10</td><td>0.92509</td></tr><tr><td>Validation - top-5</td><td>0.89587</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/ai3x7sks</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_122208-ai3x7sks/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkCZPRrjf6KJ",
        "outputId": "b2de753c-4957-4797-daef-d52fe797c92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.04919%) \t Le beurre est le meilleur garçon du monde ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "1. (0.01414%) \t Le pain est le meilleur garçon du monde ?\n",
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "2. (0.01374%) \t Le beurre est le meilleur garçon du monde du monde.\n",
            "BLEU: 3.965294799986402e-78\n",
            "ROUGE-1: 0.4210526315789474\n",
            "ROUGE-2: 0.23529411764705882\n",
            "ROUGE-L: 0.4210526315789474\n",
            "METEOR: 0.45731707317073167\n",
            "\n",
            "3. (0.01289%) \t Le blâmer est le meilleur garçon du monde ?\n",
            "BLEU: 4.335118471269586e-78\n",
            "ROUGE-1: 0.4444444444444445\n",
            "ROUGE-2: 0.25\n",
            "ROUGE-L: 0.4444444444444445\n",
            "METEOR: 0.46296296296296297\n",
            "\n",
            "4. (0.01022%) \t Le beurre est le meilleur garçon du monde.\n",
            "BLEU: 4.4646672960328985e-78\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.26666666666666666\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.3680555555555556\n",
            "\n",
            "Greedy search: \t Le beurre est le plus grand de la meilleure source du monde.\n",
            "BLEU: 5.233427736988301e-155\n",
            "ROUGE-1: 0.4\n",
            "ROUGE-2: 0.1111111111111111\n",
            "ROUGE-L: 0.4\n",
            "METEOR: 0.30423280423280424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the work is done\"\n",
        "reference = [\"le\", \"travail\", \"est\", \"fait\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZ_ln-Hodo1",
        "outputId": "534bd4eb-5c71-47e3-d506-c6d2c099a111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (2.46919%) \t Tout est le travail ?\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "1. (2.34901%) \t Tout est le travail.\n",
            "BLEU: 1.5319719891192393e-231\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.25\n",
            "\n",
            "2. (1.99484%) \t Le travail est le travail.\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.6666666666666665\n",
            "ROUGE-2: 0.5714285714285715\n",
            "ROUGE-L: 0.6666666666666665\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "3. (1.65739%) \t Le travail est fait.\n",
            "BLEU: 9.53091075863908e-155\n",
            "ROUGE-1: 1.0\n",
            "ROUGE-2: 1.0\n",
            "ROUGE-L: 1.0\n",
            "METEOR: 0.7361111111111112\n",
            "\n",
            "4. (1.23024%) \t Tout est le travail   ?\n",
            "BLEU: 9.283142785759642e-155\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.3333333333333333\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.6233062330623306\n",
            "\n",
            "Greedy search: \t Le travail est fait du travail.\n",
            "BLEU: 5.775353993361614e-78\n",
            "ROUGE-1: 0.8\n",
            "ROUGE-2: 0.7499999999999999\n",
            "ROUGE-L: 0.8\n",
            "METEOR: 0.9449404761904763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "EvGynA5MCY5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YwDbJVPmarJrGhXBDAffl8jF_BVgcWKJ\" alt=\"RNN\" width=\"700\"/>\n"
      ],
      "metadata": {
        "id": "ze0Ec_P-Clpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1tdgNIwexsdyd8aSAf_4fF9tBdRup19Eg\" alt=\"RNN\" width=\"700\"/>"
      ],
      "metadata": {
        "id": "rocP6A4rCzB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU seems to be a better model for translation. However the model struggles to generalize because the translations of the instances are not well translated."
      ],
      "metadata": {
        "id": "8A0bMTJiC_w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architectures\n",
        "This section presents a few of the best architectures I created."
      ],
      "metadata": {
        "id": "18bYK_8IRhM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First architecture"
      ],
      "metadata": {
        "id": "TF8HYEqIUlab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Xnr3idFeByP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c741d0-d9ea-4108-a982-938bf932ce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "├─Embedding: 1-2                                   [128, 60, 196]            100,352\n",
              "├─Embedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "├─Embedding: 1-4                                   [128, 60, 196]            100,352\n",
              "├─Transformer: 1-5                                 [128, 60, 196]            --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-1                        --                        256,036\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-2                        --                        410,876\n",
              "├─Linear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 10,457,420\n",
              "Trainable params: 10,457,420\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.34\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1435.24\n",
              "Params size (MB): 41.83\n",
              "Estimated Total Size (MB): 1477.19\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='first architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sFdByui95wEM",
        "outputId": "a5d38db7-62f6-416e-fba6-f09c80c348c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_222559-eyc47829</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">first architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "Eval -    loss: 1.74     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You never asked what I wanted.\n",
            "Tu n'as jamais demandé ce que je voulais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.60     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Have you read this book already?\n",
            "Avez-vous déjà lu ce livre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.43     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.31     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "He is playing in his room.\n",
            "Il joue dans sa chambre.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.35     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.23     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I wish my girlfriend would spend more time with me.\n",
            "J'aimerais que ma petite amie passe davantage de temps avec moi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.24     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.18     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "He was so sad that he almost went mad.\n",
            "Il était si triste qu'il est parti.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "You can get a loan from a bank.\n",
            "Tu peux obtenir un prêt de banque.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "I swear I didn't make this up.\n",
            "Je jure que je ne faisais pas ça.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.11     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I had to stop.\n",
            "J'ai dû arrêter.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.08     top-1: 0.76    top-5: 0.91    top-10: 0.94\n",
            "CEO's of American corporations are paid several times their Japanese counterparts.\n",
            "Les habitants de l'immobilier a été payé plusieurs fois leurs japonaise.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom is trying to make sure that everything is ready.\n",
            "Tom essaie d'essayer de assurer que tout est prêt.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▄▄▅▆▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████▇█████████</td></tr><tr><td>Train - top-10</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▄▅▅▇▇▇▇▇▇▇▇▇▇█▇███████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.08052</td></tr><tr><td>Train - top-1</td><td>0.74934</td></tr><tr><td>Train - top-10</td><td>0.94146</td></tr><tr><td>Train - top-5</td><td>0.91612</td></tr><tr><td>Validation - loss</td><td>1.06362</td></tr><tr><td>Validation - top-1</td><td>0.75878</td></tr><tr><td>Validation - top-10</td><td>0.94018</td></tr><tr><td>Validation - top-5</td><td>0.91688</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">first architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/eyc47829</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_222559-eyc47829/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "CUPXLcm5iezz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598aadfc-a862-4863-a3c8-9d39d10bfd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (2.13144%) \t Le fer est le meilleur groupe du monde.\n",
            "BLEU: 0.345720784641941\n",
            "ROUGE-1: 0.625\n",
            "ROUGE-2: 0.42857142857142855\n",
            "ROUGE-L: 0.625\n",
            "METEOR: 0.49609375\n",
            "\n",
            "1. (1.38691%) \t Le café est le meilleur groupe du monde.\n",
            "BLEU: 0.345720784641941\n",
            "ROUGE-1: 0.625\n",
            "ROUGE-2: 0.42857142857142855\n",
            "ROUGE-L: 0.625\n",
            "METEOR: 0.49609375\n",
            "\n",
            "2. (0.69926%) \t Le fer est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.3896604938271605\n",
            "\n",
            "3. (0.54538%) \t Le fer est le meilleur groupe du monde dans le monde.\n",
            "BLEU: 0.24808415001701817\n",
            "ROUGE-1: 0.5263157894736842\n",
            "ROUGE-2: 0.3529411764705882\n",
            "ROUGE-L: 0.5263157894736842\n",
            "METEOR: 0.4481927710843373\n",
            "\n",
            "4. (0.50393%) \t Le fer est le meilleur groupe au monde.\n",
            "BLEU: 0.5169731539571706\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.7142857142857143\n",
            "ROUGE-L: 0.75\n",
            "METEOR: 0.6225\n",
            "\n",
            "Greedy search: \t Le fer est le meilleur groupe de la Terre.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.47058823529411764\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.47058823529411764\n",
            "METEOR: 0.48996913580246915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second architecture"
      ],
      "metadata": {
        "id": "CC8bFeacvCvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 14,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 6,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bejQMT75LY-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acce5911-b94e-472f-bbc3-95cfa02354b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "TranslationTransformer                                  [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                        [128, 60, 196]            2,382,184\n",
              "├─Embedding: 1-2                                        [128, 60, 196]            100,352\n",
              "├─Embedding: 1-3                                        [128, 60, 196]            3,594,640\n",
              "├─Embedding: 1-4                                        [128, 60, 196]            100,352\n",
              "├─Transformer: 1-5                                      [128, 60, 196]            --\n",
              "│    └─TransformerEncoder: 2-1                          [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-1                             --                        1,536,216\n",
              "│    └─TransformerDecoder: 2-2                          [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-2                             --                        2,465,256\n",
              "├─Linear: 1-6                                           [128, 60, 18340]          3,612,980\n",
              "=========================================================================================================\n",
              "Total params: 13,791,980\n",
              "Trainable params: 13,791,980\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.77\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 2736.54\n",
              "Params size (MB): 55.17\n",
              "Estimated Total Size (MB): 2791.83\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='second architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wZejgZTb_rRO",
        "outputId": "ef9e4cf3-b4b1-472b-a769-b61ae107901d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_224843-q0ioo85p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">second architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.66     top-1: 0.67    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.50     top-1: 0.69    top-5: 0.86    top-10: 0.90\n",
            "Turn on the TV.\n",
            "Éteins la télévision.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.33     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.22     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I was overweight.\n",
            "J'ai été en surpoids.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.22     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "Don't give him any ideas.\n",
            "Ne lui donne pas d'idées.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.09     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Are you leaving now?\n",
            "Allez-vous partir maintenant ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "A party of scientists were on board with them.\n",
            "Une fête de scientifiques étaient au bord d'eux.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 0.99     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm sick of your excuses.\n",
            "J'en ai marre de vos excuses.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.96     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm really glad to hear it.\n",
            "Je suis très heureux de l'entendre.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.90     top-1: 0.78    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.91     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "I have to do something else first.\n",
            "Je dois faire quelque chose d'autre.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.87     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.89     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "I think it's impossible for him to solve the problem.\n",
            "Je pense qu'il est impossible de lui résoudre le problème.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.86     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.88     top-1: 0.79    top-5: 0.94    top-10: 0.95\n",
            "What makes you laugh?\n",
            "Qu'est-ce qui te fait rire ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█████</td></tr><tr><td>Train - top-10</td><td>▁▂▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇█▇▇███████████████████</td></tr><tr><td>Train - top-5</td><td>▁▅▅▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.85579</td></tr><tr><td>Train - top-1</td><td>0.7909</td></tr><tr><td>Train - top-10</td><td>0.95955</td></tr><tr><td>Train - top-5</td><td>0.94017</td></tr><tr><td>Validation - loss</td><td>0.88038</td></tr><tr><td>Validation - top-1</td><td>0.79193</td></tr><tr><td>Validation - top-10</td><td>0.95467</td></tr><tr><td>Validation - top-5</td><td>0.93557</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">second architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/q0ioo85p</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_224843-q0ioo85p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GiW8ipY7Jl_",
        "outputId": "ca7c7237-d27f-4bb7-bdf3-0844c390975e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.60861%) \t Le meilleur groupe est le meilleur groupe au monde.\n",
            "BLEU: 0.44632361378533286\n",
            "ROUGE-1: 0.7058823529411765\n",
            "ROUGE-2: 0.6666666666666666\n",
            "ROUGE-L: 0.7058823529411765\n",
            "METEOR: 0.6148148148148148\n",
            "\n",
            "1. (0.55541%) \t Le meilleur groupe est le meilleur groupe du monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.48996913580246915\n",
            "\n",
            "2. (0.46842%) \t Le meilleur groupe est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2626909894424158\n",
            "ROUGE-1: 0.5555555555555556\n",
            "ROUGE-2: 0.375\n",
            "ROUGE-L: 0.5555555555555556\n",
            "METEOR: 0.38490853658536583\n",
            "\n",
            "3. (0.20985%) \t Le meilleur pays est le meilleur groupe du monde.\n",
            "BLEU: 0.2984745896009823\n",
            "ROUGE-1: 0.5882352941176471\n",
            "ROUGE-2: 0.39999999999999997\n",
            "ROUGE-L: 0.5882352941176471\n",
            "METEOR: 0.48996913580246915\n",
            "\n",
            "4. (0.18202%) \t Le meilleur groupe est le meilleur groupe dans le monde du monde.\n",
            "BLEU: 0.22416933501922293\n",
            "ROUGE-1: 0.5\n",
            "ROUGE-2: 0.33333333333333326\n",
            "ROUGE-L: 0.5\n",
            "METEOR: 0.44285714285714284\n",
            "\n",
            "Greedy search: \t Le meilleur groupe est le meilleur groupe dans le monde.\n",
            "BLEU: 0.2626909894424158\n",
            "ROUGE-1: 0.5555555555555556\n",
            "ROUGE-2: 0.375\n",
            "ROUGE-L: 0.5555555555555556\n",
            "METEOR: 0.38490853658536583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third architecture"
      ],
      "metadata": {
        "id": "-8So852v6qel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 256,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 6,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "warmup_steps = 200\n",
        "scheduler = CustomLRScheduler(config['optimizer'], config['dim_embedding'], warmup_steps)\n",
        "\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOJ-wcio6qGq",
        "outputId": "22267e20-94f8-405e-c4d0-9870bef7caae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "TranslationTransformer                                  [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                        [128, 60, 256]            3,111,424\n",
              "├─Embedding: 1-2                                        [128, 60, 256]            131,072\n",
              "├─Embedding: 1-3                                        [128, 60, 256]            4,695,040\n",
              "├─Embedding: 1-4                                        [128, 60, 256]            131,072\n",
              "├─Transformer: 1-5                                      [128, 60, 256]            --\n",
              "│    └─TransformerEncoder: 2-1                          [128, 60, 256]            --\n",
              "│    │    └─ModuleList: 3-1                             --                        2,374,656\n",
              "│    └─TransformerDecoder: 2-2                          [128, 60, 256]            --\n",
              "│    │    └─ModuleList: 3-2                             --                        3,956,736\n",
              "├─Linear: 1-6                                           [128, 60, 18340]          4,713,380\n",
              "=========================================================================================================\n",
              "Total params: 19,113,380\n",
              "Trainable params: 19,113,380\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 2.45\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 3171.53\n",
              "Params size (MB): 76.45\n",
              "Estimated Total Size (MB): 3248.11\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='third architecture',\n",
        "    ):\n",
        "    train_model(model, config, scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GKPYlUqhJglS",
        "outputId": "971be404-f300-4b4c-c60f-6bb377cb577f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_223858-egy0a07w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">third architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "May I ask another question?\n",
            "idée arpenteuse médicaments Vérifiez langue fainéant accordez Devinez fainéant fainéant accordez Devinez fainéant accordez attrapée chienne canettes éteigne Vérifiez misérable hospitalisées fainéant accordez bâtie détritus reposons fainéant fainéant comportent voulions figuré astucieux payerons décodé détritus reviendrons figure allégation découvertes croyait accordez Vérifiez excitée F Devinez prisonnière prisonnière remporter embarras traversâmes croyait couvercle traîné collation sucre serais épais principale shérif\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He hesitated for a while.\n",
            "séries suspendit quant préfèrerais Présentez accordez accordez Devinez prisonnière épais bâclé accordez cygnes pseudo-science programmes recontacterai concept éclaira posant accordez cygnes cygnes accordez climatiseur accordez Vide éclaira tombai moulins fainéant égaré sous-sol supplémentaire continuera accordez cygnes conducteur occupés boursier suspendit Devinez fainéant accordez portèrent accordez circonstance dangereuse travailleur continua accordez joyeuses pension recontacterai décideront accordez savais tondue Vérifiez score\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I know I made a mistake.\n",
            "séries suspendit sous-sol incompétents emménager allusion chiqué troupeau prisonnière épais bâclé Cendrillon croyait accordez indienne éclaira envoyer exemples marches insinuer su correcte karaté incorrecte détritus embauchées décliné débarrasserai complètement accrochait Vérifiez agréablement dormi effraction puisqu'levais conducteur puisqu'Vérifiez Coupe accordez Vérifiez Cambodge portèrent astucieux Saisissez sous-sol cordes injuste empilables décodé décidai recontacterai décideront accordez Vérifiez brésilienne oubliez obéira\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I think you ought to pay more attention in class.\n",
            "syllabe résidentiel chiqué éruption Hong fainéant accordez Devinez fainéant fainéant accordez Devinez fainéant accordez indienne reproduisent modestes refaire quatre-vingt-dix-sept accordez cygnes yoga négociations tennis signatures puisqu'pique-niques résonné devrais rompu résidentiel Nombre musicien croyait accordez cygnes empilables accordez canettes accordez Devinez fainéant accordez saisit déballer chronomètre Quelques cordes crédule programmes enleva fassions négociations inspecteur considérer Cookie prévisions forcés goûte\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Did I hurt you?\n",
            "séries suspendit sous-sol incompétents emménager allusion chiqué troupeau prisonnière épais bâclé Cendrillon croyait accordez cygnes pseudo-science extraordinaire accordez cygnes vénère cygnes yoga empilables rompu exemples Jetons moment souhaiterions croyait incorrect shérif volent souffrez réglée leurs décodé conducteur prisonnière remarquablement furent hommes cinémas commençait cuisinez décodé thon leurs originales plais changements également tasses leurs idéogrammes insinué cahiers éclaira enregistrée devenez\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I'd be disappointed if I saw you doing that.\n",
            "séries changements Donne-le-moi résidentiel emménager registre Hong Signez casser Davantage bâclé accordez servez sponsors épais fromage Parlez réfugiés joyeuses chapeau entendis Mieux traça adressée joyeuses traités Change écoutâmes ordres enseigna résidentiel Nombre musicien croyait accordez variété coupez Vérifiez payai croyait accordez saisit inconsidéré traversent fatigant considérer menacez Question injuste empilables mangue passionnante possession formellement croyait bouchés tragédie sursaut retrouve\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Why did you guys break up?\n",
            "séries suspendit armées épais bras parvint accordez rougi prépara épais bâclé Cendrillon croyait accordez attrapée fers organise partagèrent compromette découvrira impressionnées joui bachoté marches mention cow persistant tempêtes spray traversâmes précision écoles traditionnel croyait ferme décodé criait-t apprécieriez adoptèrent déplacer chenille excitée remplis décodé thon mini-jupe détritus embauchées confiants funérailles entêtant shérif expulsé accordez étouffait prévisions forcés prîmes\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "The room was very untidy.\n",
            "étatsunien lessive européens souffrez langue fainéant accordez rougi prépara épais bâclé accordez Reconnais accordez cygnes pseudo-science extraordinaire accordez cygnes chapeau figuré mesure concerne honoré détritus embauchées résidentiel torturez Entrez fainéant programmeur shérif arpenteuse essayez compliment sous-sol supplémentaire insoutenable sous-estimée traduit accordez saisit conservateur remplis décodé thon leurs cerisiers injuste empilables honoré oubliez emprunté interrompue allusion musées accordez oubliez obéira\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He didn't say anything.\n",
            "séries suspendit occupés type contentez frôlé accordez tenterai industrielle accordez joyeuses chapeau Constitution cinquantaine attrapée pleurniches joyeuses chapeau contentez enseigna égaux concevoir accordez climatiseur accordez conducteur résidentiel craignez moulins Apollo goûte Reconnaissez musicien douleurs accordez savais leva suspendit gorge Bravo Devinez fainéant accordez portèrent difficile débrouille allonge roula shérif vieillissait croyait couvercle recontacterai décideront accordez savais tondue rusé fêtes\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 9.88     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "After work, I go right home.\n",
            "séries changements Donne-le-moi sac pleurait dissuader accordez Devinez prisonnière gâchis nocturne Retenez conducteur journées fâche chienne fleuves croyait émeute accordez cygnes yoga négociations tennis détritus embauchées décliné débarrasserai complètement accrochait Vérifiez humilité embauchées boursier puisqu'levais conducteur puisqu'baguette pleura puisqu'refuse puisqu'Service jean emménager calice mémoriser canettes empilables funérailles comptais échec bambin marcherons jalouses astucieux étouffa idioties\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▅▄▄▄▇▅▅▄▆▄▄█▁▇▆▄▅▆▃▅▄▂▅▇▃▄▄▆▃▅▄▆▂▆▅▄▆▆▆▅</td></tr><tr><td>Train - top-1</td><td>▂▂▂▂▇▃▂▅▂▁▅▇▁▅▄▄▁▂▂▄▂▅▅▅▁▅▄█▁▃▆▂▂▂▅▁▂▂▂▃</td></tr><tr><td>Train - top-10</td><td>▃▅▂▄▄▅▃▄▃█▃▄▃▅█▃▄▁▅▂▄▂▃▃▂▄▁▅▃▄▃▅▄▂▃▂▅▃▂▃</td></tr><tr><td>Train - top-5</td><td>▅▄▁▅▅▃█▁▃▅▆▃▅▅▅▆▃▄▅▆█▇▅▄▆▄▇▄▄▅▅▅▄▄▄▄▆▅▆▅</td></tr><tr><td>Validation - loss</td><td>▅▁▇▄▆▄▃▃██</td></tr><tr><td>Validation - top-1</td><td>▁▅▇▄▄▄▆█▁▄</td></tr><tr><td>Validation - top-10</td><td>▂▁█▃▁▁▇▆▂▅</td></tr><tr><td>Validation - top-5</td><td>▁▆▄▆▆▅█▇▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>9.87689</td></tr><tr><td>Train - top-1</td><td>2e-05</td></tr><tr><td>Train - top-10</td><td>0.00056</td></tr><tr><td>Train - top-5</td><td>0.00019</td></tr><tr><td>Validation - loss</td><td>9.87546</td></tr><tr><td>Validation - top-1</td><td>0.0</td></tr><tr><td>Validation - top-10</td><td>0.00018</td></tr><tr><td>Validation - top-5</td><td>5e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">third architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/egy0a07w</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_223858-egy0a07w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL3AfEPI0lOM",
        "outputId": "f6e2d77c-3f38-4cb5-8c36-561abb80dc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t séries changements Donne-le-moi résidentiel gencives épais accordez Devinez fainéant documentée enseigna accordez dernièrement accordez attrapée conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde décliné perdez plais shérif arpenteuse infinies programmes perdez bambin portail exercice attrapés luxueux Libérez concept éclaira joyeuses chapeau exercice cordes injuste empilables funérailles comptais échec bambin précipiter bourrer enquise prêteriez virent\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t séries changements Donne-le-moi résidentiel gencives épais accordez Devinez fainéant documentée enseigna accordez dernièrement accordez attrapée conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde décliné perdez plais shérif arpenteuse infinies programmes perdez bambin portail exercice attrapés luxueux Libérez concept éclaira joyeuses chapeau exercice cordes injuste empilables funérailles comptais échec bambin précipiter bourrer enquise prêteriez bicyclette\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t séries changements Donne-le-moi résidentiel gencives épais accordez Devinez fainéant documentée enseigna accordez dernièrement accordez attrapée conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde décliné perdez plais shérif arpenteuse infinies programmes perdez bambin portail exercice attrapés luxueux Libérez concept éclaira joyeuses chapeau exercice cordes injuste empilables funérailles comptais échec bambin précipiter bourrer enquise prêteriez détritus\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.02380952380952381\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.02380952380952381\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t séries changements Donne-le-moi résidentiel gencives épais accordez Devinez fainéant documentée enseigna accordez dernièrement accordez attrapée conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde décliné perdez plais shérif arpenteuse infinies programmes perdez bambin portail exercice attrapés luxueux Libérez concept éclaira joyeuses chapeau exercice cordes injuste empilables funérailles comptais échec bambin précipiter bourrer enquise prêteriez bloquée\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.02380952380952381\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.02380952380952381\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t séries changements Donne-le-moi résidentiel gencives épais accordez Devinez fainéant documentée enseigna accordez dernièrement accordez attrapée conter fleuves exemples marches-t allusion Mieux devenez sous-sol exemples Jetons moment absurde décliné perdez plais shérif arpenteuse infinies programmes perdez bambin portail exercice attrapés luxueux Libérez concept éclaira joyeuses chapeau exercice cordes injuste empilables funérailles comptais échec bambin précipiter bourrer enquise prêteriez score\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.024096385542168676\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.024096385542168676\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t séries suspendit chienne outil professionnels prévenus crédule rougi prépara épais bâclé accordez dernièrement sermonna râteau mourons joyeuses chapeau contentez Hong nuage luxueux dettes payerons tripe hibernent trahis fainéant dénégations enseigna résidentiel raconterez arpenteuse infinies embauchées résidentiel empilables accordez canettes veniez emprunté cinémas commençait cuisinez absurde tasses leurs girafe accordez empilables universel fantastiques recontacterai décideront cimetière immobile Sale offre jouons Rencontrez\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is the result of this work ?\"\n",
        "reference = [\"Quel\", \"est\", \"le\", \"résultat\", \"de\", \"ce\", \"travail\", \"?\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "YcTJiPnk8ziK",
        "outputId": "1d53522a-892f-4138-f4cd-af3fb374aa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t séries changements Donne-le-moi résidentiel nié épais accordez Devinez fainéant documentée enseigna accordez croyait accordez indienne espion excitée remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables épargnée Baissez score virgule programmes décodé immergée rattraperai Reprenons refuse Devinez fainéant documentée indienne espion tasses leurs cerisiers melons fainéant chanceuse commençait traversent retraité croyait talons enquise Attache obéira\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04819277108433735\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04819277108433735\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t séries changements Donne-le-moi résidentiel nié épais accordez Devinez fainéant documentée enseigna accordez croyait accordez indienne espion excitée remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables épargnée Baissez score virgule programmes décodé immergée rattraperai Reprenons refuse Devinez fainéant documentée indienne espion tasses leurs cerisiers melons fainéant chanceuse commençait traversent retraité croyait talons enquise Attache saigneras\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t séries changements Donne-le-moi résidentiel nié épais accordez Devinez fainéant documentée enseigna accordez croyait accordez indienne espion excitée remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables épargnée Baissez score virgule programmes décodé immergée rattraperai Reprenons refuse Devinez fainéant documentée indienne espion tasses leurs cerisiers melons fainéant chanceuse commençait traversent retraité croyait talons enquise Attache estime\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t séries changements Donne-le-moi résidentiel nié épais accordez Devinez fainéant documentée enseigna accordez croyait accordez indienne espion excitée remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables épargnée Baissez score virgule programmes décodé immergée rattraperai Reprenons refuse Devinez fainéant documentée indienne espion tasses leurs cerisiers melons fainéant chanceuse commençait traversent retraité croyait talons enquise Attache organisons\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t séries changements Donne-le-moi résidentiel nié épais accordez Devinez fainéant documentée enseigna accordez croyait accordez indienne espion excitée remplis empilables Autrefois allusion Mieux devenez sous-sol exemples puisqu'rythme pension tasses empilables épargnée Baissez score virgule programmes décodé immergée rattraperai Reprenons refuse Devinez fainéant documentée indienne espion tasses leurs cerisiers melons fainéant chanceuse commençait traversent retraité croyait talons enquise Attache applaudissait\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.04878048780487805\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.04878048780487805\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t séries changements Donne-le-moi résidentiel torturez aimeras chiqué nia croyait mémoriser dauphins accordez croyait accordez indienne espion excitée remplis empilables Autrefois épais bras accordez devenez détritus embauchées décliné débarrasserai industrielle accordez Vérifiez vengé embauchées croyait puisqu'refuse prospectus ravaler roses veniez emprunté poupon Devinez fous joyeuses tasses leurs score éducateur empilables universel Tes croyait dix-neuf-cent-cinquante-sept manteaux cimetière enquise imprimer choisis cordes\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.047058823529411764\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.047058823529411764\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth architecture\n",
        "Big architecture. Values of the hyperparameters taken from the article Attention is all you need (name in the article : big)."
      ],
      "metadata": {
        "id": "od4A6GETEamU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "warmup_steps = 200\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 256,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 8,\n",
        "    'dim_embedding': 512,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.2,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "scheduler = CustomLRScheduler(config['optimizer'], config['dim_embedding'], warmup_steps)\n",
        "\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "dL8AVTo18zVS",
        "outputId": "df02c01a-65db-41fe-e67b-995580ca8e1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [256, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [256, 60, 512]            6,222,848\n",
              "├─Embedding: 1-2                                   [256, 60, 512]            262,144\n",
              "├─Embedding: 1-3                                   [256, 60, 512]            9,390,080\n",
              "├─Embedding: 1-4                                   [256, 60, 512]            262,144\n",
              "├─Transformer: 1-5                                 [256, 60, 512]            --\n",
              "│    └─TransformerEncoder: 2-1                     [256, 60, 512]            --\n",
              "│    │    └─ModuleList: 3-1                        --                        3,946,752\n",
              "│    └─TransformerDecoder: 2-2                     [256, 60, 512]            --\n",
              "│    │    └─ModuleList: 3-2                        --                        7,101,696\n",
              "├─Linear: 1-6                                      [256, 60, 18340]          9,408,420\n",
              "====================================================================================================\n",
              "Total params: 36,594,084\n",
              "Trainable params: 36,594,084\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 9.37\n",
              "====================================================================================================\n",
              "Input size (MB): 0.25\n",
              "Forward/backward pass size (MB): 6280.15\n",
              "Params size (MB): 146.38\n",
              "Estimated Total Size (MB): 6426.77\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='fourth architecture',\n",
        "    ):\n",
        "    train_model(model, config,scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xkIY6luZPZKs",
        "outputId": "9a196ff0-445b-42fa-c0ef-bc6f4d346f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_231610-zaztjkmh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">fourth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Show me where it happened.\n",
            "toqué abattre accusez jurés remise aigre amenés paris souffre repassait rébellion allonger douche marque triomphe amusais assurance radio ravalé Profite venait Renvoie publicitaire intérieure sortirais satisfaisants test déclencher arrêtèrent achats enjeux handicapé du profond diapo biologiste Achetez auto-stoppeurs Informez ferons serrurier moment détritus essayai lycéenne hypocondriaque subir embrassais amie abuse affamée Arménie tenez techniques féminine ratons équitable aimons soulignés\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I've been waiting all day for you.\n",
            "affilié J’ allonger détourna envolé courant tristes pistolets Hey opposants radios allonger douche marque triomphe Seize essaya repassait anodin Bas humiliation visions débarrasser découragez crocodiles témoignerai graves prier musicien pigeon sabots Comporte gomme château commode crédit Suisses trimestre choisi prouva intimidez louée attaquent thermomètre débutent colons lois améliorés chrétiens douche Mêle morts concentrés douche aise fers équitable aimons soulignés\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Where do you usually go fishing?\n",
            "amusèrent fis adoraient attirer athlètes aigre amenés elle atome fichez dangereux allonger douche marque triomphe venait répondeur attaque végétariennes robe toqué van Servez vacances fortement serrées divin brut musicien clôture précisément Ton négociations leçons habitons biologiste Achetez auto-stoppeurs Informez maline capitaine désolées attaquent thermomètre débutent colons lapins mourant foutu fierté Seconde surestimez apprécierais modifié emmène Chicago attendirent excuse nettoies\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "The doctor says that I'll never be able to swim again.\n",
            "parlera épris amenés Indiens toqué Effectuez expliquerais buissons manipulateur entrant dangereux allonger douche marque triomphe venait répondeur attaque végétariennes robe toqué exposés Épluche compléter repart Qu'sources négatifs Mêle morts La obsession coréen connaissance commère biologiste Achetez auto-stoppeurs Informez ferons nettoyés originale approcha anéantie sida reçue dira embrassais amie connaissances plaisantent gagnions descendue ma emmène souviens équitable van Marseille\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "I'll let you know when it's done.\n",
            "parlera épris accusez jurés remise aigre amenés elle atome fichez dangereux allonger douche marque triomphe amusais assurance bougie influent sauter additionnelle écouler rénové vacances fortement patiemment veiller satisfait gorge clôture puzzle Bois retirée Partez faufilé partit doctorat contusionné préparerai perdrai chatte chamade Autre mourrez sauter acupuncture repos récupéré Prendre brûlé Serre débris préparerait modifié emmène souviens soucies van Marseille\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "She taught him the tricks of the trade.\n",
            "amusèrent fis pardonné entouré date toqué amenés elle atome fichez dangereux allonger douche marque triomphe venait répondeur attaque végétariennes robe toqué exposés distribution significations invasion rendras reparler portier chanceuses musicien Barrière amie majeur canons coincé anthropologie acheté abandonne colombe prisonnières changerons nourrissent cuisine toucher cirer sous-estimée quelques admira préoccuperais écouté potentielles handball CD chassé délicate enlèvera équitable aimons soulignés\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "We eat to live, not live to eat.\n",
            "affilié inquiétudes accusez survenait toqué nerveuse améliorer déçu Hey opposants radios allonger douche marque triomphe amusais assurance bougie trahirai consommez pilules dangereux aventures rentre décède plombs avalé munitions bruiner vint répéta apprécieras insiste serra présidentielle biologiste Achetez auto-stoppeurs Informez ferons nettoyés épreuves gaspiller spectateurs changée antécédents désorganisée Jetons Pelez évacué Personnellement remarquable prouve distribuer dissuada Bell équitable aimons soulignés\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "You are taller than me.\n",
            "toqué monopoles prenait critiqué entraînons aigre amenés perdante hydrogène occuperai radios allonger douche marque triomphe Seize hypoxie repassait anodin libèrerai chanter J’ Payez vacances fortement contrôle su donation musicien agonie Ils étudies majeur dussiez désintoxication avocat employé mélangé 60 perçante dormit soleil diffère souciait biologiste pointait chauffard batte égayé avariée tenez matchs existent spaghetti délicate enlèvera équitable van Marseille\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Tom got fired.\n",
            "amusèrent fis adoraient attirer athlètes aigre amenés elle atome fichez dangereux allonger douche marque effronterie choisi hypoxie journée montiez mec tondrai reposé siffler réglée prier abandonnera veiller production anglaise Pardonne explosion fantasque terrifiée douche aise fromage bienvenu tante agonie cowboy avisés désolées attaquent thermomètre débutent colons tourné descendants amie connaissances divertissante teinturier maigrichon Ferme symphonies laverie persuada Retourne mentir\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 9.99     top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "Eval -    loss: 10.00    top-1: 0.00    top-5: 0.00    top-10: 0.00\n",
            "He died a few days before his hundredth birthday.\n",
            "parlera épris suspendis deviendras flic Apportez-le-moi Sécession bloquait verglacé propriété dangereux allonger douche marque disons troisième épais tienne J’ fantasque traîné athlètes fermes vacances fortement informer résonna avoua morceau Interdit moment Ton négociations leçons corneilles ironique instrument assise intérieure mah lèche rédaction hérita vieillissons locuteurs dormais grenouille attaquent Voyager pardonné éprouve fanfare examiner traîtresse cirer sous-estimée arrêtions posent bordent\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▂▃▃▂▁█▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▄▃▂▁</td></tr><tr><td>Train - top-1</td><td>▁▂▂▂▂▃▂▂▁▃▃▂▂▃▃▂▂▂▂▁▄▃▃▂▃█▁▂▃▃▁▂▃▂▂▂▂▂▂▃</td></tr><tr><td>Train - top-10</td><td>▂▂▁▃▂▃▃▂▂▃▃▁▁▂▄▂▁▂▃▂▃▂▂▂▂▂▃▂▁▁▂▂▂▃▃▂█▂▂▂</td></tr><tr><td>Train - top-5</td><td>▄▄▄▅▆▄▅▅▄▅▆▅▆▅▅▄▄▄▅▅█▄▄▄▄▅▁▄▅▆▄▅▅▄▄▄▃▆▄▅</td></tr><tr><td>Validation - loss</td><td>▁▁▂▃█▆▇▂▁▃</td></tr><tr><td>Validation - top-1</td><td>█▇▂▇▁▃▅▇▁▂</td></tr><tr><td>Validation - top-10</td><td>▇█▂▅▁▅▆▇▂█</td></tr><tr><td>Validation - top-5</td><td>▄▆▁▄▁█▄█▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>9.99004</td></tr><tr><td>Train - top-1</td><td>7e-05</td></tr><tr><td>Train - top-10</td><td>0.00048</td></tr><tr><td>Train - top-5</td><td>0.00025</td></tr><tr><td>Validation - loss</td><td>10.00034</td></tr><tr><td>Validation - top-1</td><td>2e-05</td></tr><tr><td>Validation - top-10</td><td>0.00042</td></tr><tr><td>Validation - top-5</td><td>0.00018</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fourth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/zaztjkmh</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_231610-zaztjkmh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iron Maiden is the best band in the world\"\n",
        "reference = [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "get_metrics(model, sentence, reference, config)"
      ],
      "metadata": {
        "id": "7GU_eHJXilmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bf5a47-23de-46c7-eb66-a8e25f4b42d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search : \n",
            "0. (0.00000%) \t toqué procéder réimprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisième épais tienne J’ fantasque terrifiée exposés Épluche compléter Serait bowling J’ envoient musicien indiens coincés dureront remarquable écharde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons méprise paisible déclare dira formellement exagéré sortirais irréparable adhésif appuyer piqûre délicate enlèvera équitable aimons soulignés\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "1. (0.00000%) \t toqué procéder réimprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisième épais tienne J’ fantasque terrifiée exposés Épluche compléter Serait bowling J’ envoient musicien indiens coincés dureront remarquable écharde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons méprise paisible déclare dira formellement exagéré sortirais irréparable adhésif appuyer piqûre délicate enlèvera équitable aimons soupé\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "2. (0.00000%) \t toqué procéder réimprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisième épais tienne J’ fantasque terrifiée exposés Épluche compléter Serait bowling J’ envoient musicien indiens coincés dureront remarquable écharde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons méprise paisible déclare dira formellement exagéré sortirais irréparable adhésif appuyer piqûre délicate enlèvera équitable aimons dose\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "3. (0.00000%) \t toqué procéder réimprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisième épais tienne J’ fantasque terrifiée exposés Épluche compléter Serait bowling J’ envoient musicien indiens coincés dureront remarquable écharde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons méprise paisible déclare dira formellement exagéré sortirais irréparable adhésif appuyer piqûre délicate enlèvera équitable aimons viendront\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "4. (0.00000%) \t toqué procéder réimprimer demi permettra rapide tristes pistolets Hey opposants radios allonger douche marque triomphe troisième épais tienne J’ fantasque terrifiée exposés Épluche compléter Serait bowling J’ envoient musicien indiens coincés dureront remarquable écharde retournez biologiste Achetez auto-stoppeurs Informez maline capitaine parts Supposons méprise paisible déclare dira formellement exagéré sortirais irréparable adhésif appuyer piqûre délicate enlèvera équitable aimons deviens\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n",
            "\n",
            "Greedy search: \t toqué procéder pointez souvient concrets perdus soleil agence corrélation échouée essayai sécurité chaleureusement accepterais survivrons noix anorexique survivantes bancale flottant chanter dangereux aventures elle aptitude vint opposent envoient musicien indiens coincés dureront remarquable écharde couper ironique instrument assise intérieure nettoya magazine grossiers réélu refroidit comédie essayiez oignons intéressais prévention obscur Rome explique spécialisée jardinage angles échangèrent confectionné tue Égypte poussaient\n",
            "BLEU: 0\n",
            "ROUGE-1: 0.0\n",
            "ROUGE-2: 0.0\n",
            "ROUGE-L: 0.0\n",
            "METEOR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the third and fourth architecture have a loss that is too high. The main possible reason is the number of parameters. Also the learning rate scheduler doesn't help either. Let's stick to smaller architectures."
      ],
      "metadata": {
        "id": "i2JLYsUvZw4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fifth architecture"
      ],
      "metadata": {
        "id": "reg8cpu0aJbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 14,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 4,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZP1Sqx9aAcN",
        "outputId": "300b9315-ad91-4f25-d73f-b153cf1745fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "├─Embedding: 1-2                                   [128, 60, 196]            100,352\n",
              "├─Embedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "├─Embedding: 1-4                                   [128, 60, 196]            100,352\n",
              "├─Transformer: 1-5                                 [128, 60, 196]            --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-1                        --                        1,024,144\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-2                        --                        1,643,504\n",
              "├─Linear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 12,458,156\n",
              "Trainable params: 12,458,156\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.59\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 2216.02\n",
              "Params size (MB): 49.83\n",
              "Estimated Total Size (MB): 2265.97\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='fifth architecture',\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3IjYl67saVwc",
        "outputId": "aa441d14-6827-4d2c-95ba-2a3b262edf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250318_133659-merbp2h8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">fifth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.65     top-1: 0.67    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.51     top-1: 0.69    top-5: 0.86    top-10: 0.89\n",
            "He wrote to me from time to time.\n",
            "Il m'a écrit de temps à temps.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.35     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.25     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Have you ever gone skinny dipping?\n",
            "Avez-vous jamais été aussi maigre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.20     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "Tom believed it.\n",
            "Tom le croyait.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.11     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.05     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "What was the weather like yesterday?\n",
            "Quel temps était le temps, hier ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.07     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Who saw me?\n",
            "Qui m'a vu ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.01     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "They are talking about what they will sing.\n",
            "Ils parlent de ce qu'ils vont chanter.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Comfort Tom.\n",
            "<unk> Tom.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm starting to get bored.\n",
            "Je commence à m'ennuyer.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.90     top-1: 0.78    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "What did you think I'd do?\n",
            "Qu'avez-vous pensé que je ferais ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.87     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.92     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "Tom didn't get home until after 2:30 this morning.\n",
            "Tom n'est pas arrivé jusqu'à la maison ce matin.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>██▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▄▄▅▆▆▆▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇█████</td></tr><tr><td>Train - top-10</td><td>▁▄▄▆▆▆▇▇▇▇▇▇█▇▇▇▇██▇▇▇▇███▇█████████████</td></tr><tr><td>Train - top-5</td><td>▁▂▃▃▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██████████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▃▂▂▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.86598</td></tr><tr><td>Train - top-1</td><td>0.78602</td></tr><tr><td>Train - top-10</td><td>0.95928</td></tr><tr><td>Train - top-5</td><td>0.93873</td></tr><tr><td>Validation - loss</td><td>0.91573</td></tr><tr><td>Validation - top-1</td><td>0.78578</td></tr><tr><td>Validation - top-10</td><td>0.95304</td></tr><tr><td>Validation - top-5</td><td>0.93345</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fifth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/merbp2h8</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250318_133659-merbp2h8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sixth architecture"
      ],
      "metadata": {
        "id": "PWXizdV2fOyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous architectures were big architectures. Let's try with a smaller architecture"
      ],
      "metadata": {
        "id": "qssBVVz4f-Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 10,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 3,\n",
        "    'dim_embedding': 30,\n",
        "    'dim_hidden': 64,\n",
        "    'n_layers': 2,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'Transformer',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoKLQQqGleHf",
        "outputId": "72a37b1d-f47f-463a-e276-09321ab30404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [128, 60, 30]             364,620\n",
              "├─Embedding: 1-2                                   [128, 60, 30]             15,360\n",
              "├─Embedding: 1-3                                   [128, 60, 30]             550,200\n",
              "├─Embedding: 1-4                                   [128, 60, 30]             15,360\n",
              "├─Transformer: 1-5                                 [128, 60, 30]             --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 30]             --\n",
              "│    │    └─ModuleList: 3-1                        --                        15,548\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 30]             --\n",
              "│    │    └─ModuleList: 3-2                        --                        23,108\n",
              "├─Linear: 1-6                                      [128, 60, 18340]          568,540\n",
              "====================================================================================================\n",
              "Total params: 1,552,736\n",
              "Trainable params: 1,552,736\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 198.75\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1219.95\n",
              "Params size (MB): 6.21\n",
              "Estimated Total Size (MB): 1226.29\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Transformers',\n",
        "        save_code=True,\n",
        "        name='sixth architecture',\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m6BbA3bqfdX-",
        "outputId": "f45a8645-4f81-4e4f-d6f1-98e01ef9d4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250318_141218-yx2opww2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">sixth architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 3.45     top-1: 0.42    top-5: 0.59    top-10: 0.65\n",
            "Eval -    loss: 3.28     top-1: 0.44    top-5: 0.61    top-10: 0.67\n",
            "She arranged the flowers beautifully.\n",
            "Elle sait le français.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.93     top-1: 0.48    top-5: 0.67    top-10: 0.73\n",
            "Eval -    loss: 2.71     top-1: 0.51    top-5: 0.69    top-10: 0.75\n",
            "Beef is very expensive.\n",
            "Peu.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.61     top-1: 0.53    top-5: 0.72    top-10: 0.77\n",
            "Eval -    loss: 2.40     top-1: 0.55    top-5: 0.74    top-10: 0.79\n",
            "He decided to submit his resignation.\n",
            "Il a décidé de sa fille.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.43     top-1: 0.55    top-5: 0.74    top-10: 0.79\n",
            "Eval -    loss: 2.19     top-1: 0.58    top-5: 0.77    top-10: 0.81\n",
            "The cats are safe.\n",
            "Les chats sont différents.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.26     top-1: 0.57    top-5: 0.77    top-10: 0.81\n",
            "Eval -    loss: 2.06     top-1: 0.60    top-5: 0.79    top-10: 0.83\n",
            "I was on my way to work.\n",
            "J'étais sur mon chemin de travail.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.15     top-1: 0.59    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 1.96     top-1: 0.62    top-5: 0.80    top-10: 0.84\n",
            "No one knows what to say.\n",
            "Personne ne sait ce qui veut dire.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.06     top-1: 0.60    top-5: 0.79    top-10: 0.84\n",
            "Eval -    loss: 1.88     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "If for some reason that happened, what would you do?\n",
            "Si vous trouve une raison de quoi que tu ferais   ?\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.99     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.82     top-1: 0.63    top-5: 0.82    top-10: 0.86\n",
            "What do you consider your greatest achievement?\n",
            "Que penses-tu ton album ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.77     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "Many students go to Europe for the purpose of studying music.\n",
            "Beaucoup d'étudiants en Europe pour apprendre la musique.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.89     top-1: 0.62    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.73     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You didn't keep your word.\n",
            "Vous n'avez pas tenu votre mot.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Train - top-5</td><td>▁▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▅▆▆▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.89182</td></tr><tr><td>Train - top-1</td><td>0.6234</td></tr><tr><td>Train - top-10</td><td>0.85933</td></tr><tr><td>Train - top-5</td><td>0.81553</td></tr><tr><td>Validation - loss</td><td>1.73038</td></tr><tr><td>Validation - top-1</td><td>0.64691</td></tr><tr><td>Validation - top-10</td><td>0.86973</td></tr><tr><td>Validation - top-5</td><td>0.83049</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sixth architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/yx2opww2</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250318_141218-yx2opww2/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A smaller architecture fails to do good translation"
      ],
      "metadata": {
        "id": "2SlPH5CFqHAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "rloGZXI0FfQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_Qx5gffKckj0IY959hygnMm0f-KWrCX-\" alt=\"bayes_net\" width=\"900\"/>"
      ],
      "metadata": {
        "id": "bn2lo-UeFgnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=12h360IOUV1cg7hjlGF9lNK0CbjA_rciC\" alt=\"bayes_net\" width=\"900\"/>\n"
      ],
      "metadata": {
        "id": "Kze4nGtqs7ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best architecture\n",
        "The best architecture seems to be the second transformer architecture (based on its loss).\n",
        "But the first architecture seems to generalize better. Because the translation of the sentence \"Iron Maiden is the best band in the world\" is better with the first architecture. So the second architecture may be overfitting. Increase the number of heads doesn't necessarily lead to better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qcE_JAk9Vq8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "0sAIeu7FWARJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 12,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UmGgeBGOD4Js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70776e86-343d-48ea-b8a0-285116189baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 18340]          --\n",
              "├─Embedding: 1-1                                   [128, 60, 196]            2,382,184\n",
              "├─Embedding: 1-2                                   [128, 60, 196]            100,352\n",
              "├─Embedding: 1-3                                   [128, 60, 196]            3,594,640\n",
              "├─Embedding: 1-4                                   [128, 60, 196]            100,352\n",
              "├─Transformer: 1-5                                 [128, 60, 196]            --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-1                        --                        256,036\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 196]            --\n",
              "│    │    └─ModuleList: 3-2                        --                        410,876\n",
              "├─Linear: 1-6                                      [128, 60, 18340]          3,612,980\n",
              "====================================================================================================\n",
              "Total params: 10,457,420\n",
              "Trainable params: 10,457,420\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.34\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1435.24\n",
              "Params size (MB): 41.83\n",
              "Estimated Total Size (MB): 1477.19\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225',  # Title of your project\n",
        "        group='Best architecture',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='first architecture',\n",
        "    ):\n",
        "    train_model(model, config)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rLH1ryQjAcYg",
        "outputId": "f808abc4-b08f-428f-9d0e-a67d82d129a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250323_123743-pjp8tc3r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">first architecture</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.96     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "Eval -    loss: 1.74     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "You never asked what I wanted.\n",
            "Tu n'as jamais demandé ce que je voulais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.60     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Have you read this book already?\n",
            "Avez-vous déjà lu ce livre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.43     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.31     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "He is playing in his room.\n",
            "Il joue dans sa chambre.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.35     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.23     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I wish my girlfriend would spend more time with me.\n",
            "J'aimerais que ma petite amie passe davantage de temps avec moi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.24     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.18     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "He was so sad that he almost went mad.\n",
            "Il était si triste qu'il est parti.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "You can get a loan from a bank.\n",
            "Tu peux obtenir un prêt de banque.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "I swear I didn't make this up.\n",
            "Je jure que je ne faisais pas ça.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.11     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "I had to stop.\n",
            "J'ai dû arrêter.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.08     top-1: 0.76    top-5: 0.91    top-10: 0.94\n",
            "CEO's of American corporations are paid several times their Japanese counterparts.\n",
            "Les habitants de l'immobilier a été payé plusieurs fois leurs japonaise.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom is trying to make sure that everything is ready.\n",
            "Tom essaie d'essayer de assurer que tout est prêt.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.05     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I have to stop you from doing that. \"Stop me from doing what?\"\n",
            "«   Je dois t'empêcher de faire ça.   » «   Pas de quoi   ?   »\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I'm only going to show you once.\n",
            "Je ne vais te montrer tout de suite.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▆▆▅▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▄▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇█▇▇█▇████▇▇██████████</td></tr><tr><td>Train - top-10</td><td>▁▂▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▂▄▄▅▆▆▇▇▇▇▇▇▇▇▇█▇▇██▇██████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▆▇▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.0026</td></tr><tr><td>Train - top-1</td><td>0.76263</td></tr><tr><td>Train - top-10</td><td>0.94921</td></tr><tr><td>Train - top-5</td><td>0.92455</td></tr><tr><td>Validation - loss</td><td>1.03892</td></tr><tr><td>Validation - top-1</td><td>0.76393</td></tr><tr><td>Validation - top-10</td><td>0.94266</td></tr><tr><td>Validation - top-5</td><td>0.91919</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">first architecture</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225/runs/pjp8tc3r</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/INF8225</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250323_123743-pjp8tc3r/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1pd1UTt__FOzQNR2Py9jkqERuHNTOeStp\" alt=\"bayes_net\" width=\"700\"/>"
      ],
      "metadata": {
        "id": "H8eXrpKjKdjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1NqdYPzqIOTi1SVRBIDoJrDi88cnQ44Zn\" alt=\"bayes_net\" width=\"700\"/>\n"
      ],
      "metadata": {
        "id": "rEW8IhX_J0nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "PonKtwzKWBe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def evaluate_model_on_dataset(model, sentences, references, config, method='greedy', beam_width=10, k=5):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a list of sentences and references.\n",
        "\n",
        "    Args:\n",
        "        model: The translation/generation model.\n",
        "        sentences (List[str]): List of source sentences.\n",
        "        references (List[List[str]]): List of reference token lists.\n",
        "        config (dict): Configuration parameters.\n",
        "        method (str): Either 'greedy' or 'beam' search.\n",
        "        beam_width (int): Beam width if using beam search.\n",
        "        k (int): Number of beam candidates to consider.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of metrics dictionaries for each sentence.\n",
        "    \"\"\"\n",
        "    all_metrics = []\n",
        "    for sentence, reference in zip(sentences, references):\n",
        "        if method == 'greedy':\n",
        "            translation = greedy_search(\n",
        "                model,\n",
        "                sentence,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                config['device'],\n",
        "                max_sentence_length=config['max_sequence_length']\n",
        "            )\n",
        "            print(\"sentence : \", sentence)\n",
        "            print(\"translation : \", translation.split())\n",
        "            print(\"reference : \", reference)\n",
        "            print()\n",
        "            metrics = compute_metrics(reference, translation.split())\n",
        "        elif method == 'beam':\n",
        "            preds = beam_search(\n",
        "                model,\n",
        "                sentence,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                config['device'],\n",
        "                beam_width=beam_width,\n",
        "                max_target=100,\n",
        "                max_sentence_length=config['max_sequence_length']\n",
        "            )[:k]\n",
        "            # Pick the best translation (first candidate)\n",
        "            translation, likelihood = preds[0]\n",
        "            print(\"translation : \", translation.split())\n",
        "            print(\"reference : \", reference)\n",
        "            metrics = compute_metrics(reference, translation.split())\n",
        "        else:\n",
        "            raise ValueError(\"Invalid method: choose 'greedy' or 'beam'\")\n",
        "        all_metrics.append(metrics)\n",
        "    return all_metrics\n",
        "\n",
        "def aggregate_metrics(all_metrics):\n",
        "    \"\"\"\n",
        "    Aggregates metrics by computing the average for each metric.\n",
        "\n",
        "    Args:\n",
        "        all_metrics (List[dict]): List of metrics for each sentence.\n",
        "\n",
        "    Returns:\n",
        "        dict: Average metrics.\n",
        "    \"\"\"\n",
        "    avg_metrics = {}\n",
        "    keys = all_metrics[0].keys()\n",
        "    for key in keys:\n",
        "        avg_metrics[key] = np.mean([m[key] for m in all_metrics])\n",
        "    return avg_metrics\n",
        "\n",
        "def plot_metrics_distribution(all_metrics):\n",
        "    \"\"\"\n",
        "    Plots histograms for the distribution of each metric.\n",
        "\n",
        "    Args:\n",
        "        all_metrics (List[dict]): List of metrics for each sentence.\n",
        "    \"\"\"\n",
        "    metrics_names = list(all_metrics[0].keys())\n",
        "    data = {metric: [m[metric] for m in all_metrics] for metric in metrics_names}\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, metric in enumerate(metrics_names):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.hist(data[metric], bins=20, color='skyblue', edgecolor='black')\n",
        "        plt.title(f\"Distribution of {metric}\")\n",
        "        plt.xlabel(metric)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_average_metrics(avg_metrics):\n",
        "    \"\"\"\n",
        "    Plots a bar chart of average metrics.\n",
        "\n",
        "    Args:\n",
        "        avg_metrics (dict): Dictionary of average metrics.\n",
        "    \"\"\"\n",
        "    metrics = list(avg_metrics.keys())\n",
        "    values = list(avg_metrics.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(metrics, values, color='lightgreen', edgecolor='black')\n",
        "    plt.title(\"Average Metrics over the Dataset\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Average Score\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AGL9mjszWDYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "        {\n",
        "            \"sentence\": \"Iron Maiden is the best band in the world\",\n",
        "            \"reference\": [\"Iron\", \"Maiden\", \"est\", \"le\", \"meilleur\", \"groupe\", \"au\", \"monde\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"The quick brown fox jumps over the lazy dog\",\n",
        "            \"reference\": [\"Le\", \"renard\", \"brun\", \"rapide\", \"saute\", \"par-dessus\", \"le\", \"chien\", \"paresseux\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"Artificial intelligence is transforming our world\",\n",
        "            \"reference\": [\"L'intelligence\", \"artificielle\", \"transforme\", \"notre\", \"monde\"]\n",
        "        },\n",
        "        {\n",
        "            \"sentence\": \"Hello world\",\n",
        "            \"reference\": [\"Bonjour\", \"le\", \"monde\"]\n",
        "        },\n",
        "         {\n",
        "            \"sentence\": \"This is a test sentence\",\n",
        "            \"reference\": [\"Ceci\", \"est\", \"une\", \"phrase\", \"de\", \"test\"]\n",
        "        },\n",
        "        {\"sentence\": \"The weather is nice today.\", \"reference\": \"Le temps est agréable aujourd'hui.\".split()},\n",
        "        {\"sentence\": \"I enjoy reading books in the park.\", \"reference\": \"J'aime lire des livres dans le parc.\".split()},\n",
        "        {\"sentence\": \"She loves to play the piano.\", \"reference\": \"Elle aime jouer du piano.\".split()},\n",
        "        {\"sentence\": \"The children are playing in the garden.\", \"reference\": \"Les enfants jouent dans le jardin.\".split()},\n",
        "        {\"sentence\": \"He is working on a new project.\", \"reference\": \"Il travaille sur un nouveau projet.\".split()},\n",
        "        {\"sentence\": \"They visited the museum last weekend.\", \"reference\": \"Ils ont visité le musée le week-end dernier.\".split()},\n",
        "        {\"sentence\": \"Our team won the championship.\", \"reference\": \"Notre équipe a gagné le championnat.\".split()},\n",
        "        {\"sentence\": \"The food at that restaurant is delicious.\", \"reference\": \"La nourriture dans ce restaurant est délicieuse.\".split()},\n",
        "        {\"sentence\": \"I will travel to France next summer.\", \"reference\": \"Je voyagerai en France l'été prochain.\".split()},\n",
        "        {\"sentence\": \"Technology is changing the world rapidly.\", \"reference\": \"La technologie change le monde rapidement.\".split()},\n",
        "        {\"sentence\": \"She is learning to speak Spanish.\", \"reference\": \"Elle apprend à parler espagnol.\".split()},\n",
        "        {\"sentence\": \"Music brings joy to our lives.\", \"reference\": \"La musique apporte de la joie à nos vies.\".split()},\n",
        "        {\"sentence\": \"He enjoys hiking in the mountains.\", \"reference\": \"Il aime faire de la randonnée dans les montagnes.\".split()},\n",
        "        {\"sentence\": \"The movie was both entertaining and thought-provoking.\", \"reference\": \"Le film était à la fois divertissant et stimulant.\".split()},\n",
        "        {\"sentence\": \"They are planning a surprise party for their friend.\", \"reference\": \"Ils prévoient une fête surprise pour leur ami.\".split()}\n",
        "\n",
        "    ]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4eK_V0yKXOZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sample_data\n",
        "sentences = [item[\"sentence\"] for item in dataset]\n",
        "references = [item[\"reference\"] for item in dataset]\n",
        "\n",
        "# Evaluate the model on the dataset using greedy search.\n",
        "all_metrics = evaluate_model_on_dataset(model, sentences, references, config, method='greedy')\n",
        "avg_metrics = aggregate_metrics(all_metrics)\n",
        "print(\"Average Metrics over the Dataset:\")\n",
        "for metric, value in avg_metrics.items():\n",
        "  print(f\"{metric}: {value}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6fR2Zl5HkiL",
        "outputId": "2af69435-ed31-4b48-c769-f3e7e44d9d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence :  Iron Maiden is the best band in the world\n",
            "translation :  ['Le', 'Le', 'vin', 'est', 'le', 'meilleur', 'groupe', 'dans', 'le', 'monde.']\n",
            "reference :  ['Iron', 'Maiden', 'est', 'le', 'meilleur', 'groupe', 'au', 'monde']\n",
            "\n",
            "sentence :  The quick brown fox jumps over the lazy dog\n",
            "translation :  ['Le', 'centre-ville', 'marron', 'le', 'renard.']\n",
            "reference :  ['Le', 'renard', 'brun', 'rapide', 'saute', 'par-dessus', 'le', 'chien', 'paresseux']\n",
            "\n",
            "sentence :  Artificial intelligence is transforming our world\n",
            "translation :  [\"L'intelligence\", 'artificielle', 'est', 'plus', 'grande', 'que', 'le', 'monde', 'soit', 'au', 'monde.']\n",
            "reference :  [\"L'intelligence\", 'artificielle', 'transforme', 'notre', 'monde']\n",
            "\n",
            "sentence :  Hello world\n",
            "translation :  ['Salut', 'le', 'monde.']\n",
            "reference :  ['Bonjour', 'le', 'monde']\n",
            "\n",
            "sentence :  This is a test sentence\n",
            "translation :  [\"C'est\", 'un', 'examen', 'd’', 'une', 'phrase.']\n",
            "reference :  ['Ceci', 'est', 'une', 'phrase', 'de', 'test']\n",
            "\n",
            "sentence :  The weather is nice today.\n",
            "translation :  ['Le', 'temps', 'est', 'beau', \"aujourd'hui.\"]\n",
            "reference :  ['Le', 'temps', 'est', 'agréable', \"aujourd'hui.\"]\n",
            "\n",
            "sentence :  I enjoy reading books in the park.\n",
            "translation :  [\"J'aime\", 'lire', 'des', 'livres', 'dans', 'le', 'parc.']\n",
            "reference :  [\"J'aime\", 'lire', 'des', 'livres', 'dans', 'le', 'parc.']\n",
            "\n",
            "sentence :  She loves to play the piano.\n",
            "translation :  ['Elle', 'aime', 'jouer', 'du', 'piano.']\n",
            "reference :  ['Elle', 'aime', 'jouer', 'du', 'piano.']\n",
            "\n",
            "sentence :  The children are playing in the garden.\n",
            "translation :  ['Les', 'enfants', 'jouent', 'dans', 'le', 'jardin.']\n",
            "reference :  ['Les', 'enfants', 'jouent', 'dans', 'le', 'jardin.']\n",
            "\n",
            "sentence :  He is working on a new project.\n",
            "translation :  ['Il', 'travaille', 'sur', 'un', 'nouveau', 'projet.']\n",
            "reference :  ['Il', 'travaille', 'sur', 'un', 'nouveau', 'projet.']\n",
            "\n",
            "sentence :  They visited the museum last weekend.\n",
            "translation :  ['Ils', 'ont', 'visité', 'le', 'musée', 'le', 'week-end', 'dernier.']\n",
            "reference :  ['Ils', 'ont', 'visité', 'le', 'musée', 'le', 'week-end', 'dernier.']\n",
            "\n",
            "sentence :  Our team won the championship.\n",
            "translation :  ['Notre', 'équipe', 'a', 'gagné', 'le', 'championnat.']\n",
            "reference :  ['Notre', 'équipe', 'a', 'gagné', 'le', 'championnat.']\n",
            "\n",
            "sentence :  The food at that restaurant is delicious.\n",
            "translation :  ['La', 'nourriture', 'au', 'restaurant', 'est', 'délicieuse.']\n",
            "reference :  ['La', 'nourriture', 'dans', 'ce', 'restaurant', 'est', 'délicieuse.']\n",
            "\n",
            "sentence :  I will travel to France next summer.\n",
            "translation :  ['Je', 'vais', 'voyager', 'à', 'la', 'France', \"l'été\", 'prochain.']\n",
            "reference :  ['Je', 'voyagerai', 'en', 'France', \"l'été\", 'prochain.']\n",
            "\n",
            "sentence :  Technology is changing the world rapidly.\n",
            "translation :  ['La', 'technologie', 'change', 'du', 'monde', 'est', 'rapidement.']\n",
            "reference :  ['La', 'technologie', 'change', 'le', 'monde', 'rapidement.']\n",
            "\n",
            "sentence :  She is learning to speak Spanish.\n",
            "translation :  ['Elle', 'apprend', 'à', 'parler', 'espagnol.']\n",
            "reference :  ['Elle', 'apprend', 'à', 'parler', 'espagnol.']\n",
            "\n",
            "sentence :  Music brings joy to our lives.\n",
            "translation :  ['La', 'musique', 'classique', 'de', 'notre', 'vie', 'à', 'notre', 'vies.']\n",
            "reference :  ['La', 'musique', 'apporte', 'de', 'la', 'joie', 'à', 'nos', 'vies.']\n",
            "\n",
            "sentence :  He enjoys hiking in the mountains.\n",
            "translation :  ['Il', 'aime', 'la', 'randonnée', 'dans', 'les', 'montagnes.']\n",
            "reference :  ['Il', 'aime', 'faire', 'de', 'la', 'randonnée', 'dans', 'les', 'montagnes.']\n",
            "\n",
            "sentence :  The movie was both entertaining and thought-provoking.\n",
            "translation :  ['Le', 'film', 'était', 'à', 'la', 'fois,', 'et', 'pensais', '«', '»']\n",
            "reference :  ['Le', 'film', 'était', 'à', 'la', 'fois', 'divertissant', 'et', 'stimulant.']\n",
            "\n",
            "sentence :  They are planning a surprise party for their friend.\n",
            "translation :  ['Ils', 'ont', \"l'intention\", \"d'une\", 'surprise', 'pour', 'leur', 'amie.']\n",
            "reference :  ['Ils', 'prévoient', 'une', 'fête', 'surprise', 'pour', 'leur', 'ami.']\n",
            "\n",
            "Average Metrics over the Dataset:\n",
            "BLEU: 0.41025996480920907\n",
            "ROUGE-1: 0.7648611111111111\n",
            "ROUGE-2: 0.61386322011322\n",
            "ROUGE-L: 0.7586111111111111\n",
            "METEOR: 0.6578715069174248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of metrics and the average metrics.\n",
        "plot_metrics_distribution(all_metrics)\n",
        "plot_average_metrics(avg_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4CHAx6hWH17c",
        "outputId": "8dc4341d-2dde-4d98-9d5e-22bb49976a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABdIAAAPeCAYAAAAI5OjmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAoW9JREFUeJzs3XecFfXZN+B7Kbt0RIqAUlYkiiJ2fRUUjSgiWGNHA8QWg1HEWDAaYsWK2KLGAsbey2NsqChq9FFRY4miKAoqKCjSWSnz/mE4D+suI1vPYfe6Pp/548yZ+c09c87uPee7s3PykiRJAgAAAAAAKFWdbBcAAAAAAAC5TJAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA7l9Ne//jXy8vKqZVu77rpr7LrrrpnHL7zwQuTl5cUDDzxQLdsfPHhwdO7cuVq2VV4LFiyIY445Jtq2bRt5eXkxbNiwbJcEQAXptblFrwWgNPp1btGvoeoI0iEixo0bF3l5eZmpQYMG0b59++jbt29cffXVMX/+/ErZztdffx1//etf45133qmU8SpTLte2Ji666KIYN25cnHDCCXH77bfHUUcdtdplO3fuXOL17tq1a5x22mnx/fffF1t25Unh7NmzVzveypO31U333HNPZtm8vLw48cQTSx3ngQceiLy8vHjhhRfKtvMAawG9NrdrWxMV6bWNGzeO7bffPv7xj3+sdp1p06bF73//++jcuXMUFBREmzZtYv/9949XXnmlxLIr309vvvlmqWMNGDCg1KCjqKgorrnmmujVq1e0aNEi8vPzo3379rHvvvvG3XffHcuXL88s+/nnn6f294svvjjlaP3kmWeeiaOPPjq6d+8edevWzfnwBUC/zu3a1oR+XbZ+vWjRorjuuutizz33jHbt2kXTpk1jq622iuuvv77YdiAiol62C4Bcct5550VhYWEsXbo0Zs6cGS+88EIMGzYsRo8eHY899lj06NEjs+zZZ58dZ555ZpnG//rrr+Pcc8+Nzp07x5ZbbrnG6z3zzDNl2k55pNV20003xYoVK6q8hop4/vnn4//9v/8XI0eOXKPlt9xyyzj11FMjImLJkiUxadKkGDNmTLz44ovx+uuvl6uGk046KbbbbrsS83fcccdyjQdQE+m1tbPXzpgxI26++eYYNGhQFBUVxbHHHlts2VdeeSX23nvviIg45phjYtNNN42ZM2fGuHHjYuedd46rrroq/vjHP1ao/lmzZkW/fv1i0qRJ0bdv3zj77LNj3XXXjZkzZ8azzz4bRxxxREyZMiXOOeecYusdfvjhmdpWtdVWW/3iNu+666649957Y+utt4727dtXqH6A6qRf69e1pV9/9tln8cc//jF23333GD58eDRr1iyefvrp+MMf/hCvvfZa3HbbbRXaH2oWQTqsol+/frHttttmHo8YMSKef/75GDBgQOy7777x4YcfRsOGDSMiol69elGvXtX+CC1atCgaNWoU+fn5VbqdX1K/fv2sbn9NfPvtt7Hpppuu8fLrr79+HHnkkZnHxxxzTDRp0iQuv/zy+OSTT6Jr165lrmHnnXeOgw46qMzrAdQmem3pakOvHTx4cGy44YZx5ZVXFvtgPmfOnDjooIOiYcOG8corr0SXLl0yzw0fPjz69u0bw4YNi2222SZ22mmnctd/1FFHxdtvvx0PPvhgHHjggcWeGzFiRLz55psxefLkEuttvfXWxfajLC666KK46aabon79+jFgwIB4//33yzUOQHXTr0unX9e8ft22bdt47733YrPNNsvMO/744+N3v/tdjB07Ns4555zYaKONyr4j1Ehu7QK/4Ne//nWcc8458cUXX8Qdd9yRmV/afeDGjx8fvXr1inXWWSeaNGkSG2+8cZx11lkR8dPtP1ZerTxkyJDMvxmNGzcuIn6611v37t1j0qRJscsuu0SjRo0y6/78PnArLV++PM4666xo27ZtNG7cOPbdd9+YPn16sWU6d+4cgwcPLrHuqmP+Um2l3Qdu4cKFceqpp0aHDh2ioKAgNt5447j88ssjSZJiy628lckjjzwS3bt3j4KCgthss83iqaeeKv2A/8y3334bRx99dKy33nrRoEGD2GKLLYr9RXjlbVWmTp0a//znPzO1f/7552s0/qratm0bEVHlJ4EAFKfX1o5e27p169hkk03i008/LTb/xhtvjJkzZ8Zll11W7EN5RETDhg3jtttui7y8vDjvvPPKtL1Vvfrqq/H000/HcccdV+JD+UrbbrttDBw4sNzbKE379u3XitAFYE3o1/p1TezXrVq1Khair3TAAQdERMSHH35Yadti7SctgjVw1FFHxVlnnRXPPPNMiX9tWumDDz6IAQMGRI8ePeK8886LgoKCmDJlSuY+Yd26dYvzzjsv/vKXv8Rxxx0XO++8c0REsb/Ufvfdd9GvX7847LDD4sgjj4z11lsvta4LL7ww8vLy4owzzohvv/02xowZE3369Il33nknc3XAmliT2laVJEnsu+++MWHChDj66KNjyy23jKeffjpOO+20+Oqrr+LKK68stvzLL78cDz30UPzhD3+Ipk2bxtVXXx2/+c1vYtq0adGyZcvV1rV48eLYddddY8qUKXHiiSdGYWFh3H///TF48OD44Ycf4uSTT45u3brF7bffHqecckpssMEGmX9Ja926deo+L126NHPf8yVLlsTbb78do0ePjl122SUKCwvX+Nitav78+aXeS71ly5bV9uU7AGsrvba4mtBrf27ZsmXx5ZdfRosWLYrN/5//+Z9o0KBBHHLIIaWuV1hYGL169Yrnn38+Fi9eXKbjvuo2IqJcV6otWrSo1P6+zjrr+OM7UOvo18Xp1/+npvXrmTNnRsRPQTtkJEAyduzYJCKSN954Y7XLNG/ePNlqq60yj0eOHJms+iN05ZVXJhGRzJo1a7VjvPHGG0lEJGPHji3xXO/evZOISG644YZSn+vdu3fm8YQJE5KISNZff/1k3rx5mfn33XdfEhHJVVddlZnXqVOnZNCgQb84ZlptgwYNSjp16pR5/MgjjyQRkVxwwQXFljvooIOSvLy8ZMqUKZl5EZHk5+cXm/fvf/87iYjkmmuuKbGtVY0ZMyaJiOSOO+7IzPvxxx+THXfcMWnSpEmxfe/UqVPSv3//1PFWXTYiSkw9e/ZMZs+eXWzZla9z2uu68vVY3TRjxoxix2Po0KGljnP//fcnEZFMmDBhjfYDYG2i19a+Xrvnnnsms2bNSmbNmpW89957yVFHHVVqH1xnnXWSLbbYInW8k046KYmI5N13302S5JffT/379y92PA844IAkIpIffvih2HKLFy/O1Dhr1qxkzpw5meemTp2a2t9fffXVNToWq6sJIBfp1/p1be/XSZIkRUVFyaabbpoUFhYmS5cuLfP61Fxu7QJrqEmTJqnfUL7OOutERMSjjz5a7i8fKSgoiCFDhqzx8r/97W+jadOmmccHHXRQtGvXLp544olybX9NPfHEE1G3bt046aSTis0/9dRTI0mSePLJJ4vN79OnT7F//erRo0c0a9YsPvvss1/cTtu2bePwww/PzKtfv36cdNJJsWDBgnjxxRfLvQ877LBDjB8/PsaPHx+PP/54XHjhhfHBBx/EvvvuG4sXLy7XmH/5y18yY646rbvuuuWuE6A20Wv/T03otc8880y0bt06WrduHZtvvnncfvvtMWTIkLjsssuKLTd//vxix7g0K5+fN29euWpZuV6TJk2Kzb/hhhsyNbZu3Tp69epVYt3jjjuu1P5elvvPAtQk+vX/0a+Lqyn9+sQTT4z//Oc/ce211/rvM4rxboA1tGDBgmjTps1qnz/00EPj5ptvjmOOOSbOPPPM2H333ePAAw+Mgw46KOrUWbO/Wa2//vpl+vKUn38hZl5eXmy00Ubluj94WXzxxRfRvn37Ek20W7dumedX1bFjxxJjtGjRIubMmfOL2+natWuJ47e67ZRFq1atok+fPpnH/fv3j4033jgOOuiguPnmm8v1TeObb755sTHLy21ggNpKr/0/NaHX7rDDDnHBBRfE8uXL4/33348LLrgg5syZU+L4N23aNDWQiYjM87/0AX5Vq/bTlestWLAgmjdvnpn/m9/8Jrp37x4RP4Uey5cvLzFO165dU/v73Llzi/0RPj8/3x/RgRpNv/4/+nVxNaFfX3bZZXHTTTfF+eefH3vvvfca7we1gyvSYQ18+eWXMXfu3NRvam7YsGFMnDgxnn322TjqqKPi3XffjUMPPTT22GOPUn/Jr26Myra6UHZNa6oMdevWLXV+8rMvX8m23XffPSIiJk6cWGXbKCgoWO0V74sWLYqIiAYNGlTZ9gFylV5bMbnYa1f+0bpv375x6qmnxh133BGPPPJIXHXVVcWW69atW0yePDmKiopWO9a7774b9evXzwQlK3tlWk9dtZ9usskmERHx/vvvF1uuQ4cO0adPn+jTp0+Je8GuqZNPPjnatWuXmVb35WgANYF+XTH6dXG51q/HjRsXZ5xxRvz+97+Ps88+u1zboWYTpMMauP322yMiom/fvqnL1alTJ3bfffcYPXp0/Oc//4kLL7wwnn/++ZgwYUJEVP6Vxp988kmxx0mSxJQpU4p9i3iLFi3ihx9+KLHuz/9iXZbaOnXqFF9//XWJv0Z/9NFHmecrQ6dOneKTTz4p8e+Alb2dlZYtWxYRP/31u6p06tQpJk+eXOpzK+dX9n4BrA302uJqYq/t379/9O7dOy666KJYuHBhZv6AAQNiyZIlcf/995e63ueffx4vvfRS/PrXv84EKyvrWl1P/fjjj4vVPmDAgIiIuPPOOytlX1Z1+umnF/sX8iuuuKLStwGQK/Tr4vTr/7O29+tHH300jjnmmDjwwAPjuuuuq/TtUzMI0uEXPP/883H++edHYWFhDBw4cLXLff/99yXmbbnllhERmb/YNm7cOCKi1OZdHv/4xz+KNewHHnggZsyYEf369cvM69KlS7z22mvx448/ZuY9/vjjMX369GJjlaW2vffeO5YvXx7XXnttsflXXnll5OXlFdt+Rey9994xc+bMuPfeezPzli1bFtdcc000adIkevfuXSnbWWnlN4RvscUWlTruqvbee+947bXXYtKkScXm//DDD3HnnXfGlltuGW3btq2y7QPkIr22pJraa88444z47rvv4qabbsrMO/7446NNmzZx2mmnlbhH7JIlS2LIkCGRJEn85S9/yczfZpttok2bNnHzzTeXuDLukUceia+++qrYMerZs2fsscce8fe//z0effTRUmsr79WAm266aeYquT59+sQ222xTrnEAcp1+XZJ+/ZO1vV9PnDgxDjvssNhll13izjvvXONbEFH7uEc6rOLJJ5+Mjz76KJYtWxbffPNNPP/88zF+/Pjo1KlTPPbYY6m33DjvvPNi4sSJ0b9//+jUqVN8++238be//S022GCDzBdhdOnSJdZZZ5244YYbomnTptG4cePYYYcdorCwsFz1rrvuutGrV68YMmRIfPPNNzFmzJjYaKON4thjj80sc8wxx8QDDzwQe+21VxxyyCHx6aefxh133FHsC07KWts+++wTu+22W/z5z3+Ozz//PLbYYot45pln4tFHH41hw4aVGLu8jjvuuLjxxhtj8ODBMWnSpOjcuXM88MAD8corr8SYMWPKdN+1n/vqq6/ijjvuiIiIH3/8Mf7973/HjTfeGK1atSr1/uijR4+ORo0aFZtXp06dOOusszKPX3rppViyZEmJdXv06BE9evSIiIgzzzwz7r///thll13i+OOPj0022SS+/vrrGDduXMyYMSPGjh1b7n0CWBvotbWn15amX79+0b179xg9enQMHTo06tevHy1btowHHngg+vfvH1tvvXUcc8wxsemmm8bMmTNj3LhxMWXKlLjqqqtip512yoyTn58fl19+eQwaNCi22267OPTQQ6Nly5bx9ttvx6233ho9evSI4447rti277jjjthrr71i//33j379+mX+PXzmzJnx7LPPxsSJE0sNPN56663MOcOqunTpEjvuuGPq/r777rvx2GOPRUTElClTYu7cuXHBBRdExE9/uN9nn33KfAwBqoN+rV/Xln79xRdfxL777ht5eXlx0EEHlbjiftXP8xAJkIwdOzaJiMyUn5+ftG3bNtljjz2Sq666Kpk3b16JdUaOHJms+iP03HPPJfvtt1/Svn37JD8/P2nfvn1y+OGHJx9//HGx9R599NFk0003TerVq5dERDJ27NgkSZKkd+/eyWabbVZqfb1790569+6deTxhwoQkIpK77747GTFiRNKmTZukYcOGSf/+/ZMvvviixPpXXHFFsv766ycFBQVJz549kzfffLPEmGm1DRo0KOnUqVOxZefPn5+ccsopSfv27ZP69esnXbt2TS677LJkxYoVxZaLiGTo0KElaurUqVMyaNCgUvd3Vd98800yZMiQpFWrVkl+fn6y+eabZ+r6+Xj9+/f/xfFWLrvq612nTp2kTZs2yeGHH55MmTKl2LIrX+fSprp16yZJ8n+vx+qmkSNHFhvzyy+/TI455phk/fXXT+rVq5esu+66yYABA5LXXnttjeoHWBvptem11cReu7plx40bV2zfV5o6dWpy7LHHJh07dkzq16+ftGrVKtl3332Tl156abXbefLJJ5PddtstadasWVK/fv2ksLAwGT58eDJnzpxSl1+8eHEyZsyYZMcdd0yaNWuW1KtXL2nbtm0yYMCA5M4770yWLVtWrJ60/r4mx/bn7/uyrg9Q3fTr9Nr065rXr8v6eZ7aLS9Jcuzb/gAAAAAAIIe46Q8AAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAECKetkuoKqtWLEivv7662jatGnk5eVluxwAqLAkSWL+/PnRvn37qFOnZvxNXL8GoKbRrwEg95WlX9f4IP3rr7+ODh06ZLsMAKh006dPjw022CDbZVQK/RqAmkq/BoDctyb9usYH6U2bNo2Inw5Gs2bNslwNAFTcvHnzokOHDpkeVxPo1wDUNPo1AOS+svTrGh+kr/x3s2bNmmn0ANQoNelfqvVrAGoq/RoAct+a9OuacaM2AAAAAACoIoJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASJHVIH3ixImxzz77RPv27SMvLy8eeeSRYs8nSRJ/+ctfol27dtGwYcPo06dPfPLJJ9kpFgBYrc6dO0deXl6JaejQodkuDQD4L/0aAMovq0H6woULY4sttojrrruu1OcvvfTSuPrqq+OGG26I//3f/43GjRtH3759Y8mSJdVcKQCQ5o033ogZM2ZkpvHjx0dExMEHH5zlygCAlfRrACi/etnceL9+/aJfv36lPpckSYwZMybOPvvs2G+//SIi4h//+Eest9568cgjj8Rhhx1WnaUCAClat25d7PHFF18cXbp0id69e2epIgDg5/RrACi/nL1H+tSpU2PmzJnRp0+fzLzmzZvHDjvsEK+++moWKwMA0vz4449xxx13xO9+97vIy8vLdjkAQCn0awAom6xekZ5m5syZERGx3nrrFZu/3nrrZZ4rTVFRURQVFWUez5s3r9JrmzZtWsyePbvSxmvVqlV07Nix0sYDgGx65JFH4ocffojBgwevdpnq6NcA1Hw+m5Wffg1Adakp/Tpng/TyGjVqVJx77rlVNv60adNik27dYvGiRZU2ZsNGjeKjDz+sNSdsANRst9xyS/Tr1y/at2+/2mWqul8DUPP5bFYx+jUA1aEm9eucDdLbtm0bERHffPNNtGvXLjP/m2++iS233HK1640YMSKGDx+eeTxv3rzo0KFDpdU1e/bsWLxoURxywfXRprBrhcf7duoncd/ZJ8Ts2bNrxckaADXbF198Ec8++2w89NBDqctVdb8GoObz2az89GsAqktN6tc5G6QXFhZG27Zt47nnnssE5/PmzYv//d//jRNOOGG16xUUFERBQUGV19emsGus322LKt8OAKxNxo4dG23atIn+/funLldd/RqAms9ns7LTrwGobjWhX2c1SF+wYEFMmTIl83jq1KnxzjvvxLrrrhsdO3aMYcOGxQUXXBBdu3aNwsLCOOecc6J9+/ax//77Z69oAKBUK1asiLFjx8agQYOiXr2c/Vs9ANRq+jUAlE9Wu+abb74Zu+22W+bxyn8ZGzRoUIwbNy5OP/30WLhwYRx33HHxww8/RK9eveKpp56KBg0aZKtkAGA1nn322Zg2bVr87ne/y3YpAMBq6NcAUD5ZDdJ33XXXSJJktc/n5eXFeeedF+edd141VgUAlMeee+6Z2tcBgOzTrwGgfOpkuwAAAAAAAMhlgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdACgwr766qs48sgjo2XLltGwYcPYfPPN480338x2WQDAKvRrACi/etkuAABYu82ZMyd69uwZu+22Wzz55JPRunXr+OSTT6JFixbZLg0A+C/9GgAqRpAOAFTIJZdcEh06dIixY8dm5hUWFmaxIgDg5/RrAKgYt3YBACrksccei2233TYOPvjgaNOmTWy11VZx0003ZbssAGAV+jUAVIwgHQCokM8++yyuv/766Nq1azz99NNxwgknxEknnRS33XbbatcpKiqKefPmFZsAgKqjXwNAxbi1CwBQIStWrIhtt902LrroooiI2GqrreL999+PG264IQYNGlTqOqNGjYpzzz23OssEgFpNvwaAinFFOgBQIe3atYtNN9202Lxu3brFtGnTVrvOiBEjYu7cuZlp+vTpVV0mANRq+jUAVIwr0gGACunZs2dMnjy52LyPP/44OnXqtNp1CgoKoqCgoKpLAwD+S78GgIpxRToAUCGnnHJKvPbaa3HRRRfFlClT4q677oq///3vMXTo0GyXBgD8l34NABUjSAcAKmS77baLhx9+OO6+++7o3r17nH/++TFmzJgYOHBgtksDAP5LvwaAinFrFwCgwgYMGBADBgzIdhkAQAr9GgDKzxXpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKXI6SF++fHmcc845UVhYGA0bNowuXbrE+eefH0mSZLs0AAAAAABqiXrZLiDNJZdcEtdff33cdtttsdlmm8Wbb74ZQ4YMiebNm8dJJ52U7fIAAAAAAKgFcjpI/9e//hX77bdf9O/fPyIiOnfuHHfffXe8/vrrWa4MAAAAAIDaIqdv7bLTTjvFc889Fx9//HFERPz73/+Ol19+Ofr167fadYqKimLevHnFJgAAAAAAKK+cviL9zDPPjHnz5sUmm2wSdevWjeXLl8eFF14YAwcOXO06o0aNinPPPbcaqwQAAAAAoCbL6SvS77vvvrjzzjvjrrvuirfeeituu+22uPzyy+O2225b7TojRoyIuXPnZqbp06dXY8UAAAAAANQ0OX1F+mmnnRZnnnlmHHbYYRERsfnmm8cXX3wRo0aNikGDBpW6TkFBQRQUFFRnmQAAAAAA1GA5fUX6okWLok6d4iXWrVs3VqxYkaWKAAAAAACobXL6ivR99tknLrzwwujYsWNsttlm8fbbb8fo0aPjd7/7XbZLAwAAAACglsjpIP2aa66Jc845J/7whz/Et99+G+3bt4/jjz8+/vKXv2S7NAAAAAAAaomcDtKbNm0aY8aMiTFjxmS7FAAAAAAAaqmcvkc6AAAAAABkmyAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQCosL/+9a+Rl5dXbNpkk02yXRYAsAr9GgDKr162CwAAaobNNtssnn322czjevWcZgBArtGvAaB8dEwAoFLUq1cv2rZtm+0yAIAU+jUAlI9buwAAleKTTz6J9u3bx4YbbhgDBw6MadOmrXbZoqKimDdvXrEJAKh6+jUAlI8gHQCosB122CHGjRsXTz31VFx//fUxderU2HnnnWP+/PmlLj9q1Kho3rx5ZurQoUM1VwwAtY9+DQDlJ0gHACqsX79+cfDBB0ePHj2ib9++8cQTT8QPP/wQ9913X6nLjxgxIubOnZuZpk+fXs0VA0Dto18DQPm5RzoAUOnWWWed+NWvfhVTpkwp9fmCgoIoKCio5qoAgFXp1wCw5lyRDgBUugULFsSnn34a7dq1y3YpAMBq6NcAsOYE6QBAhf3pT3+KF198MT7//PP417/+FQcccEDUrVs3Dj/88GyXBgD8l34NAOXn1i4AUIpp06bF7NmzK228Vq1aRceOHSttvFzz5ZdfxuGHHx7fffddtG7dOnr16hWvvfZatG7dOtulAQD/pV8DQPkJ0gHgZ6ZNmxabdOsWixctqrQxGzZqFB99+GGNDdPvueeebJcAAPwC/RoAyk+QDgA/M3v27Fi8aFEccsH10aawa4XH+3bqJ3Hf2SfE7Nmza2yQDgAAADWZIB0AVqNNYddYv9sW2S4DAAAAyDJfNgoAAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAAClyPkj/6quv4sgjj4yWLVtGw4YNY/PNN48333wz22UBAAAAAFBLlCtI/+yzzyq7jlLNmTMnevbsGfXr148nn3wy/vOf/8QVV1wRLVq0qJbtA0BNV109HQAoP/0aALKvXEH6RhttFLvttlvccccdsWTJksquKeOSSy6JDh06xNixY2P77bePwsLC2HPPPaNLly5Vtk0AqE2qq6cDAOWnXwNA9pUrSH/rrbeiR48eMXz48Gjbtm0cf/zx8frrr1d2bfHYY4/FtttuGwcffHC0adMmttpqq7jpppsqfTsAUFtVV08HAMpPvwaA7CtXkL7lllvGVVddFV9//XXceuutMWPGjOjVq1d07949Ro8eHbNmzaqU4j777LO4/vrro2vXrvH000/HCSecECeddFLcdtttq12nqKgo5s2bV2wCAEpXXT0dACg//RoAsq9CXzZar169OPDAA+P++++PSy65JKZMmRJ/+tOfokOHDvHb3/42ZsyYUaHiVqxYEVtvvXVcdNFFsdVWW8Vxxx0Xxx57bNxwww2rXWfUqFHRvHnzzNShQ4cK1QAAtUFV93QAoOL0awDIngoF6W+++Wb84Q9/iHbt2sXo0aPjT3/6U3z66acxfvz4+Prrr2O//farUHHt2rWLTTfdtNi8bt26xbRp01a7zogRI2Lu3LmZafr06RWqAQBqg6ru6QBAxenXAJA99cqz0ujRo2Ps2LExefLk2HvvveMf//hH7L333lGnzk+5fGFhYYwbNy46d+5coeJ69uwZkydPLjbv448/jk6dOq12nYKCgigoKKjQdgGgtqiung4AlJ9+DQDZV64g/frrr4/f/e53MXjw4GjXrl2py7Rp0yZuueWWChV3yimnxE477RQXXXRRHHLIIfH666/H3//+9/j73/9eoXEBgJ9UV08HAMpPvwaA7CtXkP7JJ5/84jL5+fkxaNCg8gyfsd1228XDDz8cI0aMiPPOOy8KCwtjzJgxMXDgwAqNCwD8pLp6OgBQfvo1AGRfuYL0sWPHRpMmTeLggw8uNv/++++PRYsWVWrzHjBgQAwYMKDSxgMA/k919nQAoHz0awDIvnJ92eioUaOiVatWJea3adMmLrroogoXBQBUDz0dAHKffg0A2VeuIH3atGlRWFhYYn6nTp1i2rRpFS4KAKgeVdHTL7744sjLy4thw4ZVsDoAIEK/BoBcUK4gvU2bNvHuu++WmP/vf/87WrZsWeGiAIDqUdk9/Y033ogbb7wxevToURnlAQChXwNALihXkH744YfHSSedFBMmTIjly5fH8uXL4/nnn4+TTz45DjvssMquEQCoIpXZ0xcsWBADBw6Mm266KVq0aFFFFQNA7aNfA0D2levLRs8///z4/PPPY/fdd4969X4aYsWKFfHb3/7W/dkAYC1SmT196NCh0b9//+jTp09ccMEFqcsWFRVFUVFR5vG8efPKXjwA1BL6NQBkX7mC9Pz8/Lj33nvj/PPPj3//+9/RsGHD2HzzzaNTp06VXR8AUIUqq6ffc8898dZbb8Ubb7yxRsuPGjUqzj333PKUDAC1jn4NANlXriB9pV/96lfxq1/9qrJqAQCypCI9ffr06XHyySfH+PHjo0GDBmu0zogRI2L48OGZx/PmzYsOHTqUa/sAUFvo1wCQPeUK0pcvXx7jxo2L5557Lr799ttYsWJFseeff/75SikOAKhaldHTJ02aFN9++21svfXWxcadOHFiXHvttVFUVBR169Yttk5BQUEUFBRUzk4AQA2nXwNA9pUrSD/55JNj3Lhx0b9//+jevXvk5eVVdl0AQDWojJ6+++67x3vvvVds3pAhQ2KTTTaJM844o8SHcgCgbPRrAMi+cgXp99xzT9x3332x9957V3Y9AEA1qoye3rRp0+jevXuxeY0bN46WLVuWmA8AlJ1+DQDZV6c8K+Xn58dGG21U2bUAANVMTweA3KdfA0D2lStIP/XUU+Oqq66KJEkqux4AoBpVVU9/4YUXYsyYMZU6JgDUVvo1AGRfuW7t8vLLL8eECRPiySefjM022yzq169f7PmHHnqoUooDAKqWng4AuU+/BoDsK1eQvs4668QBBxxQ2bUAANVMTweA3KdfA0D2lStIHzt2bGXXAQBkgZ4OALlPvwaA7CvXPdIjIpYtWxbPPvts3HjjjTF//vyIiPj6669jwYIFlVYcAFD19HQAyH36NQBkV7muSP/iiy9ir732imnTpkVRUVHsscce0bRp07jkkkuiqKgobrjhhsquEwCoAno6AOQ+/RoAsq9cV6SffPLJse2228acOXOiYcOGmfkHHHBAPPfcc5VWHABQtfR0AMh9+jUAZF+5rkh/6aWX4l//+lfk5+cXm9+5c+f46quvKqUwAKDq6ekAkPv0awDIvnJdkb5ixYpYvnx5iflffvllNG3atMJFAQDVQ08HgNynXwNA9pUrSN9zzz1jzJgxmcd5eXmxYMGCGDlyZOy9996VVRsAUMX0dADIffo1AGRfuW7tcsUVV0Tfvn1j0003jSVLlsQRRxwRn3zySbRq1Sruvvvuyq4RAKgiejoA5D79GgCyr1xB+gYbbBD//ve/45577ol33303FixYEEcffXQMHDiw2BefAAC5TU8HgNynXwNA9pUrSI+IqFevXhx55JGVWQsAkAV6OgDkPv0aALKrXEH6P/7xj9Tnf/vb35arGACgeunpAJD79GsAyL5yBeknn3xyscdLly6NRYsWRX5+fjRq1EgTB4C1hJ4OALlPvwaA7KtTnpXmzJlTbFqwYEFMnjw5evXq5YtOAGAtoqcDQO7TrwEg+8oVpJema9eucfHFF5f4SzkAsHbR0wEg9+nXAFC9Ki1Ij/jpy0++/vrryhwSAMgCPR0Acp9+DQDVp1z3SH/ssceKPU6SJGbMmBHXXntt9OzZs1IKAwCqnp4OALlPvwaA7CtXkL7//vsXe5yXlxetW7eOX//613HFFVdURl0AQDXQ0wEg9+nXAJB95QrSV6xYUdl1AABZoKcDQO7TrwEg+yr1HukAAAAAAFDTlOuK9OHDh6/xsqNHjy7PJgCAaqCnA0Du068BIPvKFaS//fbb8fbbb8fSpUtj4403joiIjz/+OOrWrRtbb711Zrm8vLzKqRIAqBJ6OgDkPv0aALKvXEH6PvvsE02bNo3bbrstWrRoERERc+bMiSFDhsTOO+8cp556aqUWCQBUDT0dAHKffg0A2Veue6RfccUVMWrUqEwDj4ho0aJFXHDBBb4xHADWIno6AOQ+/RoAsq9cQfq8efNi1qxZJebPmjUr5s+fX+GiAIDqoacDQO7TrwEg+8oVpB9wwAExZMiQeOihh+LLL7+ML7/8Mh588ME4+uij48ADD6zsGgGAKqKnA0Du068BIPvKdY/0G264If70pz/FEUccEUuXLv1poHr14uijj47LLrusUgsEAKqOng4AuU+/BoDsK1eQ3qhRo/jb3/4Wl112WXz66acREdGlS5do3LhxpRYHAFQtPR0Acp9+DQDZV65bu6w0Y8aMmDFjRnTt2jUaN24cSZJUVl0AQDXS0wEg9+nXAJA95QrSv/vuu9h9993jV7/6Vey9994xY8aMiIg4+uij49RTT63UAgGAqqOnA0Du068BIPvKFaSfcsopUb9+/Zg2bVo0atQoM//QQw+Np556qtKKAwCqlp4OALlPvwaA7CvXPdKfeeaZePrpp2ODDTYoNr9r167xxRdfVEphAEDV09MBIPfp1wCQfeW6In3hwoXF/gq+0vfffx8FBQUVLgoAqB56OgDkPv0aALKvXEH6zjvvHP/4xz8yj/Py8mLFihVx6aWXxm677VZpxQEAVUtPB4Dcp18DQPaV69Yul156aey+++7x5ptvxo8//hinn356fPDBB/H999/HK6+8Utk1AgBVRE8HgNynXwNA9pXrivTu3bvHxx9/HL169Yr99tsvFi5cGAceeGC8/fbb0aVLl8quEQCoIno6AOQ+/RoAsq/MV6QvXbo09tprr7jhhhviz3/+c1XUBABUAz0dAHKffg0AuaHMV6TXr18/3n333aqoBQCoRno6AOQ+/RoAckO5bu1y5JFHxi233FLZtQAA1UxPB4Dcp18DQPaV68tGly1bFrfeems8++yzsc0220Tjxo2LPT969OhKKQ4AqFp6OgDkPv0aALKvTEH6Z599Fp07d473338/tt5664iI+Pjjj4stk5eXV3nVAQBVQk8HgNynXwNA7ihTkN61a9eYMWNGTJgwISIiDj300Lj66qtjvfXWq5LiAICqoacDQO7TrwEgd5TpHulJkhR7/OSTT8bChQsrtSAAoOrp6QCQ+/RrAMgd5fqy0ZV+3tQBgLWTng4AuU+/BoDsKVOQnpeXV+L+a+7HBgBrn8ru6ddff3306NEjmjVrFs2aNYsdd9wxnnzyyYqWCQC1mn4NALmjTPdIT5IkBg8eHAUFBRERsWTJkvj9739f4hvDH3roocqrEACodJXd0zfYYIO4+OKLo2vXrpEkSdx2222x3377xdtvvx2bbbZZpdcPALWBfg0AuaNMQfqgQYOKPT7yyCMrtRgAoHpUdk/fZ599ij2+8MIL4/rrr4/XXnvNB3MAKCf9GgByR5mC9LFjx1ZVHQBANarKnr58+fK4//77Y+HChbHjjjuWukxRUVEUFRVlHs+bN6/K6gGAtVVt6NfTpk2L2bNnV9p4rVq1io4dO1baeACwUpmCdACA1Xnvvfdixx13jCVLlkSTJk3i4Ycfjk033bTUZUeNGhXnnntuNVcIAORSv542bVps0q1bLF60qNLGbNioUXz04YfCdAAqnSAdAKgUG2+8cbzzzjsxd+7ceOCBB2LQoEHx4osvlvrhfMSIETF8+PDM43nz5kWHDh2qs1wAqJVyqV/Pnj07Fi9aFIdccH20Kexa4fG+nfpJ3Hf2CTF79mxBOgCVTpAOAFSK/Pz82GijjSIiYptttok33ngjrrrqqrjxxhtLLFtQUJD54jQAoPrkYr9uU9g11u+2RZVvBwAqok62CwAAaqYVK1YUu68qAJB79GsAWDOuSAcAKmzEiBHRr1+/6NixY8yfPz/uuuuueOGFF+Lpp5/OdmkAwH/p1wBQfoJ0AKDCvv322/jtb38bM2bMiObNm0ePHj3i6aefjj322CPbpQEA/6VfA0D5CdIBgAq75ZZbsl0CAPAL9GsAKL+16h7pF198ceTl5cWwYcOyXQoAAAAAALXEWhOkv/HGG3HjjTdGjx49sl0KAAAAAAC1yFoRpC9YsCAGDhwYN910U7Ro0SLb5QAAAAAAUIusFUH60KFDo3///tGnT59fXLaoqCjmzZtXbAIAAAAAgPLK+S8bveeee+Ktt96KN954Y42WHzVqVJx77rlVXBUAAAAAALVFTl+RPn369Dj55JPjzjvvjAYNGqzROiNGjIi5c+dmpunTp1dxlQAAAAAA1GQ5fUX6pEmT4ttvv42tt946M2/58uUxceLEuPbaa6OoqCjq1q1bbJ2CgoIoKCio7lIBAAAAAKihcjpI33333eO9994rNm/IkCGxySabxBlnnFEiRAcAAAAAgMqW00F606ZNo3v37sXmNW7cOFq2bFliPgAAAAAAVIWcvkc6AAAAAABkW05fkV6aF154IdslAAAAAABQi7giHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AqLBRo0bFdtttF02bNo02bdrE/vvvH5MnT852WQDAKvRrACg/QToAUGEvvvhiDB06NF577bUYP358LF26NPbcc89YuHBhtksDAP5LvwaA8quX7QIAgLXfU089VezxuHHjok2bNjFp0qTYZZddslQVALAq/RoAyk+QDgBUurlz50ZExLrrrlvq80VFRVFUVJR5PG/evGqpK5dMmzYtZs+eXWnjtWrVKjp27Fhp45FbKvP9kuvvlcr+2SgqKoqCgoJKGy/Xjx+UhX7Nzzk/qbm8tlBxgnQAoFKtWLEihg0bFj179ozu3buXusyoUaPi3HPPrebKcse0adNik27dYvGiRZU2ZsNGjeKjDz/0gaYGquz3Sy6/V6riZyOvTp1IVqyotPFy+fhBWejX/Jzzk5rLawuVQ5AOAFSqoUOHxvvvvx8vv/zyapcZMWJEDB8+PPN43rx50aFDh+ooLyfMnj07Fi9aFIdccH20Kexa4fG+nfpJ3Hf2CTF79mwfZmqgyny/5Pp7pbJ/Nia/8lyM/9soP2tQCv2an3N+UnN5baFyCNIBgEpz4oknxuOPPx4TJ06MDTbYYLXLFRQUVOqtFtZWbQq7xvrdtsh2GawlatP7pbL29dupn1TqeFBT6Nek8Tuz5vLaQsUI0gGACkuSJP74xz/Gww8/HC+88EIUFhZmuyQA4Gf0awAoP0E6AFBhQ4cOjbvuuiseffTRaNq0acycOTMiIpo3bx4NGzbMcnUAQIR+DQAVUSfbBQAAa7/rr78+5s6dG7vuumu0a9cuM917773ZLg0A+C/9GgDKzxXpAECFJUmS7RIAgF+gXwNA+bkiHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACBFTgfpo0aNiu222y6aNm0abdq0if333z8mT56c7bIAAAAAAKhFcjpIf/HFF2Po0KHx2muvxfjx42Pp0qWx5557xsKFC7NdGgAAAAAAtUS9bBeQ5qmnnir2eNy4cdGmTZuYNGlS7LLLLlmqCgAAAACA2iSnr0j/ublz50ZExLrrrpvlSgAAAAAAqC1y+or0Va1YsSKGDRsWPXv2jO7du692uaKioigqKso8njdvXnWUBwAAAABADbXWXJE+dOjQeP/99+Oee+5JXW7UqFHRvHnzzNShQ4dqqhAAAAAAgJporQjSTzzxxHj88cdjwoQJscEGG6QuO2LEiJg7d25mmj59ejVVCQAAAABATZTTt3ZJkiT++Mc/xsMPPxwvvPBCFBYW/uI6BQUFUVBQUA3VAQAAAABQG+R0kD506NC466674tFHH42mTZvGzJkzIyKiefPm0bBhwyxXBwAAAABAbZDTt3a5/vrrY+7cubHrrrtGu3btMtO9996b7dIAAAAAAKglcvqK9CRJsl0CAAAAAAC1XE5fkQ4AAAAAANkmSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAoMImTpwY++yzT7Rv3z7y8vLikUceyXZJAMDP6NcAUH6CdACgwhYuXBhbbLFFXHfdddkuBQBYDf0aAMqvXrYLAADWfv369Yt+/fpluwwAIIV+DQDlJ0gHAKpdUVFRFBUVZR7Pmzev0rcxbdq0mD17dqWN16pVq+jYsWOljZfrKvv4FRUVRUFBQaWNV9mvh/cLNYX3MpWpOvp1rsv1n6nKrO/DDz+slHGqUq6fn+T6+U5tkus/u6ydBOkAQLUbNWpUnHvuuVU2/rRp02KTbt1i8aJFlTZmw0aN4qMPP6wVJ9BVcfzy6tSJZMWKShuvMl8P7xdqCu9lKltV9+tcl+s/U1VRXy5bG85Pcvl8pzbJ9Z9d1l6CdACg2o0YMSKGDx+eeTxv3rzo0KFDpY0/e/bsWLxoURxywfXRprBrhcf7duoncd/ZJ8Ts2bNrxclzZR+/ya88F+P/NipnXw/vF2oK72UqW1X361yX6z9TVdWvc1Wun5/k+vlObZLrP7usvQTpAEC1KygoqNR/e12dNoVdY/1uW1T5dmqqyjp+3079pFLHqyq5Xh+sKe9lKkt19etcl+s/U5Xdr3Ndrp6frC3nO7WJ14LKVifbBQAAAAAAQC5zRToAUGELFiyIKVOmZB5PnTo13nnnnVh33XX9+yMA5Aj9GgDKT5AOAFTYm2++Gbvttlvm8cr7qQ4aNCjGjRuXpaoAgFXp1wBQfoJ0AKDCdt1110iSJNtlAAAp9GsAKD/3SAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFPWyXQBU1LRp02L27NmVNl6rVq2iY8eOlTZeZcr1fc31+gAAAACgPATprNWmTZsWm3TrFosXLaq0MRs2ahQfffhhzgW4ub6vuV4fAAAAAJSXIJ212uzZs2PxokVxyAXXR5vCrhUe79upn8R9Z58Qs2fPzrnwNtf3NdfrAwAAAIDyEqRTI7Qp7Brrd9si22VUi1zf11yvDwAAAADKypeNAgAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkWCuC9Ouuuy46d+4cDRo0iB122CFef/31bJcEAPyMfg0AuU+/BoDyyfkg/d57743hw4fHyJEj46233ootttgi+vbtG99++222SwMA/ku/BoDcp18DQPnlfJA+evToOPbYY2PIkCGx6aabxg033BCNGjWKW2+9NdulAQD/pV8DQO7TrwGg/Oplu4A0P/74Y0yaNClGjBiRmVenTp3o06dPvPrqq6WuU1RUFEVFRZnHc+fOjYiIefPmVUpNCxYsiIiIrz58N35ctLDC48364tOIiJg0aVJm7IqqU6dOrFixolLGyvXxJk+eHBG14/XI9X3N9foicvu9bLzcGq+q3s8LFiyolH60cowkSSo8VmXQr8su139nVnp9n39SuePl+v7mcH253l9r23svIsePXy3dX/36/+jXuf0zVdt+Z+b8/ubw8cv193LE2tEfcnV/c328GtWvkxz21VdfJRGR/Otf/yo2/7TTTku23377UtcZOXJkEhEmk8lkMtX4afr06dXRjn+Rfm0ymUwm0+on/dpkMplMptyf1qRf5/QV6eUxYsSIGD58eObxihUr4vvvv4+WLVtGXl5ehcefN29edOjQIaZPnx7NmjWr8Hi1jeNXfo5dxTh+FeP4VUxlH78kSWL+/PnRvn37SqguO6q6X1dUbX/P2//au/+1ed8j7L/9169/Ltf7dVnU9vd3Njjm1c8xr36OefXLZr/O6SC9VatWUbdu3fjmm2+Kzf/mm2+ibdu2pa5TUFAQBQUFxeats846lV5bs2bN/IBUgONXfo5dxTh+FeP4VUxlHr/mzZtXyjiVIZf7dUXV9ve8/a+9+1+b9z3C/tt//XqltaVfl0Vtf39ng2Ne/Rzz6ueYV79s9Ouc/rLR/Pz82GabbeK5557LzFuxYkU899xzseOOO2axMgBgJf0aAHKffg0AFZPTV6RHRAwfPjwGDRoU2267bWy//fYxZsyYWLhwYQwZMiTbpQEA/6VfA0Du068BoPxyPkg/9NBDY9asWfGXv/wlZs6cGVtuuWU89dRTsd5662WlnoKCghg5cmSJf29jzTh+5efYVYzjVzGOX8XUhuOXa/26omrDa5bG/tfe/a/N+x5h/+1/zd//mtavy6I2vL65xjGvfo559XPMq182j3lekiRJtW8VAAAAAADWEjl9j3QAAAAAAMg2QToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkl+K6666Lzp07R4MGDWKHHXaI119/PXX5+++/PzbZZJNo0KBBbL755vHEE09UU6W5qSzH76abboqdd945WrRoES1atIg+ffr84vGuycr63lvpnnvuiby8vNh///2rtsAcV9bj98MPP8TQoUOjXbt2UVBQEL/61a9q9c9vWY/fmDFjYuONN46GDRtGhw4d4pRTToklS5ZUU7W5Y+LEibHPPvtE+/btIy8vLx555JFfXOeFF16IrbfeOgoKCmKjjTaKcePGVXmdlFTbf+eWZf/HjRsXeXl5xaYGDRpUY7WVq7b3i7Ls/6677lritc/Ly4v+/ftXY8WVq7b3u7Ls/9KlS+O8886LLl26RIMGDWKLLbaIp556qhqrrVx6ds3ns2j1q+3nU9lQ289jsqG2nztUt5zu1wnF3HPPPUl+fn5y6623Jh988EFy7LHHJuuss07yzTfflLr8K6+8ktStWze59NJLk//85z/J2WefndSvXz957733qrny3FDW43fEEUck1113XfL2228nH374YTJ48OCkefPmyZdfflnNlWdfWY/dSlOnTk3WX3/9ZOedd07222+/6ik2B5X1+BUVFSXbbrttsvfeeycvv/xyMnXq1OSFF15I3nnnnWquPDeU9fjdeeedSUFBQXLnnXcmU6dOTZ5++umkXbt2ySmnnFLNlWffE088kfz5z39OHnrooSQikocffjh1+c8++yxp1KhRMnz48OQ///lPcs011yR169ZNnnrqqeopmCRJ/M4t6/6PHTs2adasWTJjxozMNHPmzGquunLU9n5R1v3/7rvvir3u77//flK3bt1k7Nix1Vt4Jant/a6s+3/66acn7du3T/75z38mn376afK3v/0tadCgQfLWW29Vc+WVQ8+u2XwWrX61/XwqG2r7eUw21PZzh2zI5X4tSP+Z7bffPhk6dGjm8fLly5P27dsno0aNKnX5Qw45JOnfv3+xeTvssENy/PHHV2mduaqsx+/nli1bljRt2jS57bbbqqrEnFWeY7ds2bJkp512Sm6++eZk0KBBtfokpKzH7/rrr0823HDD5Mcff6yuEnNaWY/f0KFDk1//+tfF5g0fPjzp2bNnldaZ69akyZ9++unJZpttVmzeoYcemvTt27cKK+Pnavvv3LLu/9ixY5PmzZtXU3VVq7b3i4qeq1155ZVJ06ZNkwULFlRViVWqtve7su5/u3btkmuvvbbYvAMPPDAZOHBgldZZHfTsmsdn0epX28+nsqG2n8dkQ20/d8i2XOvXbu2yih9//DEmTZoUffr0ycyrU6dO9OnTJ1599dVS13n11VeLLR8R0bdv39UuX5OV5/j93KJFi2Lp0qWx7rrrVlWZOam8x+68886LNm3axNFHH10dZeas8hy/xx57LHbccccYOnRorLfeetG9e/e46KKLYvny5dVVds4oz/HbaaedYtKkSZl/afvss8/iiSeeiL333rtaal6b6RvZV9t/55Z3/xcsWBCdOnWKDh06xH777RcffPBBdZRbqWp7v6iMc7VbbrklDjvssGjcuHFVlVllanu/K8/+FxUVlbiNU8OGDePll1+u0lpzhZ699vBZtPrV9vOpbKjt5zHZUNvPHdYW1dmv61X6iGux2bNnx/Lly2O99dYrNn+99daLjz76qNR1Zs6cWeryM2fOrLI6c1V5jt/PnXHGGdG+ffsSPwA1XXmO3csvvxy33HJLvPPOO9VQYW4rz/H77LPP4vnnn4+BAwfGE088EVOmTIk//OEPsXTp0hg5cmR1lJ0zynP8jjjiiJg9e3b06tUrkiSJZcuWxe9///s466yzqqPktdrq+sa8efNi8eLF0bBhwyxVVnvU9t+55dn/jTfeOG699dbo0aNHzJ07Ny6//PLYaaed4oMPPogNNtigOsquFLW9X1T0XO3111+P999/P2655ZaqKrFK1fZ+V57979u3b4wePTp22WWX6NKlSzz33HPx0EMP1ZoARs9ee/gsWv1q+/lUNtT285hsqO3nDmuL6uzXrkgnZ1x88cVxzz33xMMPP7xWf4FZdZg/f34cddRRcdNNN0WrVq2yXc5aacWKFdGmTZv4+9//Httss00ceuih8ec//zluuOGGbJe2VnjhhRfioosuir/97W/x1ltvxUMPPRT//Oc/4/zzz892aVDp/M6N2HHHHeO3v/1tbLnlltG7d+946KGHonXr1nHjjTdmu7Qqp1/8n1tuuSU233zz2H777bNdSrWp7f3uqquuiq5du8Ymm2wS+fn5ceKJJ8aQIUOiTh0fI6lZfBates6nssN5TPWr7ecONZ0r0lfRqlWrqFu3bnzzzTfF5n/zzTfRtm3bUtdp27ZtmZavycpz/Fa6/PLL4+KLL45nn302evToUZVl5qSyHrtPP/00Pv/889hnn30y81asWBEREfXq1YvJkydHly5dqrboHFKe9167du2ifv36Ubdu3cy8bt26xcyZM+PHH3+M/Pz8Kq05l5Tn+J1zzjlx1FFHxTHHHBMREZtvvnksXLgwjjvuuPjzn//sA3aK1fWNZs2aubKtmtT237kV6dcr1a9fP7baaquYMmVKVZRYZWp7v6jIa79w4cK455574rzzzqvKEqtUbe935dn/1q1bxyOPPBJLliyJ7777Ltq3bx9nnnlmbLjhhtVRctbp2WsPn0WrX20/n8qG2n4ekw21/dxhbVGd/dqrt4r8/PzYZptt4rnnnsvMW7FiRTz33HOx4447lrrOjjvuWGz5iIjx48evdvmarDzHLyLi0ksvjfPPPz+eeuqp2Hbbbauj1JxT1mO3ySabxHvvvRfvvPNOZtp3331jt912i3feeSc6dOhQneVnXXneez179owpU6ZkTt4iIj7++ONo165drTuZKM/xW7RoUYkTgJUnZz99Hwiro29kX23/nVvefr2q5cuXx3vvvRft2rWrqjKrRG3vFxV57e+///4oKiqKI488sqrLrDK1vd9V5PVv0KBBrL/++rFs2bJ48MEHY7/99qvqcnOCnr328Fm0+tX286lsqO3nMdlQ288d1hbV2q8r/etL13L33HNPUlBQkIwbNy75z3/+kxx33HHJOuusk8ycOTNJkiQ56qijkjPPPDOz/CuvvJLUq1cvufzyy5MPP/wwGTlyZFK/fv3kvffey9YuZFVZj9/FF1+c5OfnJw888EAyY8aMzDR//vxs7ULWlPXY/Vxt/8bzsh6/adOmJU2bNk1OPPHEZPLkycnjjz+etGnTJrnggguytQtZVdbjN3LkyKRp06bJ3XffnXz22WfJM888k3Tp0iU55JBDsrULWTN//vzk7bffTt5+++0kIpLRo0cnb7/9dvLFF18kSZIkZ555ZnLUUUdllv/ss8+SRo0aJaeddlry4YcfJtddd11St27d5KmnnsrWLtRKtf13bln3/9xzz02efvrp5NNPP00mTZqUHHbYYUmDBg2SDz74IFu7UG61vV+U973fq1ev5NBDD63ucitdbe93Zd3/1157LXnwwQeTTz/9NJk4cWLy61//OiksLEzmzJmTpT2oGD27ZvNZtPrV9vOpbKjt5zHZUNvPHbIhl/u1IL0U11xzTdKxY8ckPz8/2X777ZPXXnst81zv3r2TQYMGFVv+vvvuS371q18l+fn5yWabbZb885//rOaKc0tZjl+nTp2SiCgxjRw5svoLzwFlfe+tyklI2Y/fv/71r2SHHXZICgoKkg033DC58MILk2XLllVz1bmjLMdv6dKlyV//+tekS5cuSYMGDZIOHTokf/jDH9baD9YVMWHChFJ/j608XoMGDUp69+5dYp0tt9wyyc/PTzbccMNk7Nix1V43fueWZf+HDRuWWXa99dZL9t577+Stt97KQtWVo7b3i7Lu/0cffZRERPLMM89Uc6VVo7b3u7Ls/wsvvJB069YtKSgoSFq2bJkcddRRyVdffZWFqiuHnl3z+Sxa/Wr7+VQ21PbzmGyo7ecO1S2X+3Vekvi/AgAAAAAAWB33SAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBzIGDx4ceXl5mally5ax1157xbvvvptZJi8vLx555JFS13/hhReKrb/qNHPmzMw29t9//9Wu+8MPP1TBngFA7lm179avXz8KCwvj9NNPjyVLlhRb7vHHH4/evXtH06ZNo1GjRrHddtvFuHHjii2T1kc7d+4cY8aMKTZvwoQJMWDAgGjdunU0aNAgunTpEoceemhMnDixxJhpfb00EydOjH322Sfat2+fet4AAGuDmtqvR40aFdttt100bdo02rRpE/vvv39Mnjy5zMcHahNBOlDMXnvtFTNmzIgZM2bEc889F/Xq1YsBAwaUaYzJkydnxlg5tWnTpooqBoC118q++9lnn8WVV14ZN954Y4wcOTLz/DXXXBP77bdf9OzZM/73f/833n333TjssMPi97//ffzpT38q1zb/9re/xe677x4tW7aMe++9NyZPnhwPP/xw7LTTTnHKKaeUWL6sfX3hwoWxxRZbxHXXXVeu+gAg19TEfv3iiy/G0KFD47XXXovx48fH0qVLY88994yFCxeWq16oDepluwAgtxQUFETbtm0jIqJt27Zx5plnxs477xyzZs2K1q1br9EYbdq0iXXWWacKqwSAmmHVvtuhQ4fo06dPjB8/Pi655JKYPn16nHrqqTFs2LC46KKLMuuceuqpkZ+fHyeddFIcfPDBscMOO6zx9qZNmxbDhg2LYcOGxejRo4s916NHjzjppJNKrFPWvt6vX7/o16/fGi8PALmuJvbrp556qtjjcePGRZs2bWLSpEmxyy67rPE4UJu4Ih1YrQULFsQdd9wRG220UbRs2TLb5QBAjfb+++/Hv/71r8jPz4+IiAceeCCWLl1a6pVsxx9/fDRp0iTuvvvuMm3jwQcfjKVLl8bpp59e6vN5eXllLxwAapGa2q/nzp0bERHrrrtupY8NNYUr0oFiHn/88WjSpElE/PSv2e3atYvHH3886tRZ87+7bbDBBsUed+rUKT744INKrRMAaoKVfXfZsmVRVFQUderUiWuvvTYiIj7++ONo3rx5tGvXrsR6+fn5seGGG8bHH39cpu19/PHH0axZs8xVdRE/fVgfNGhQ5vGrr74am2++eeaxvg5AbVfT+/WKFSti2LBh0bNnz+jevXuZaoXaRJAOFLPbbrvF9ddfHxERc+bMib/97W/Rr1+/eP3116NTp05rNMZLL70UTZs2zTyuX79+ldQKAGu7lX134cKFceWVV0a9evXiN7/5TZVu8+dXsfXt2zfeeeed+Oqrr2LXXXeN5cuXF3t+dX39pZdeKnYLlxtvvDEGDhxYhZUDQHbU9H49dOjQeP/99+Pll1+u7N2AGkWQDhTTuHHj2GijjTKPb7755mjevHncdNNNccEFF6zRGIWFhau9N1uzZs3iiy++KDH/hx9+iLp160bjxo3LVTcArI1W7bu33nprbLHFFnHLLbfE0UcfHb/61a9i7ty58fXXX0f79u2Lrffjjz/Gp59+GrvttltE/NRfI376t+yf9+AffvghmjdvHhERXbt2jblz58bMmTMzV7k1adIkNtpoo6hXr/SPBqvr69tuu2288847mcfrrbdemfcfANYGNblfn3jiifH444/HxIkTS1zVDhTnHulAqry8vKhTp04sXry4UsbbeOON44MPPoiioqJi8996660oLCx09ToAtVadOnXirLPOirPPPjsWL14cv/nNb6J+/fpxxRVXlFj2hhtuiIULF8bhhx8eET994K5Tp05MmjSp2HKfffZZzJ07N371q19FRMRBBx0U9evXj0suuaTC9TZs2DA22mijzLTqVXAAUFPVlH6dJEmceOKJ8fDDD8fzzz8fhYWFFd4W1HSuSAeKKSoqipkzZ0bET7d2ufbaa2PBggWxzz77ZJaZOnVqsb9oR/x0QrDSt99+G0uWLCn2fMuWLaN+/foxcODAOO+88+K3v/1tnH766dG8efOYOHFijBkzJi699NKq2zEAWAscfPDBcdppp8V1110Xf/rTn+LSSy+NU089NRo0aBBHHXVU1K9fPx599NE466yz4tRTT40ddtghIiKaNm0axxxzTJx66qlRr1692HzzzWP69OlxxhlnxP/7f/8vdtppp4iI6NixY1xxxRVx8sknx/fffx+DBw+OwsLC+P777+OOO+6IiIi6desWqymtr5dmwYIFMWXKlMzjlecN6667bnTs2LHSjhUAZEtN6NdDhw6Nu+66Kx599NFo2rRpJgdo3rx5NGzYsFKPF9QYCcB/DRo0KImIzNS0adNku+22Sx544IHMMqs+v+r00ksvJRMmTFjt86+++mpmjMmTJycHHHBA0r59+6Rx48bJFltskdx0003JihUrsrHbAJAVgwYNSvbbb78S80eNGpW0bt06WbBgQZIkSfLoo48mO++8c9K4ceOkQYMGyTbbbJPceuutJdZbvHhxMnLkyGSTTTZJGjZsmBQWFibHHXdcMmvWrBLLjh8/PunXr1+y7rrrJvXq1UvWW2+9ZP/990+eeuqpzDJr2td/bnXrDRo0qOwHCQCyrKb269WtM3bs2LIfJKgl8pIkSaoyqAcAAAAAgLWZe6QDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6tdZf//rXyMvLq5Zt7brrrrHrrrtmHr/wwguRl5cXDzzwQLVsf/DgwdG5c+dq2VZ5LViwII455pho27Zt5OXlxbBhw7JdEgA5Ru/OLXo3AAC1iSCdGmHcuHGRl5eXmRo0aBDt27ePvn37xtVXXx3z58+vlO18/fXX8de//jXeeeedShmvMuVybWvioosuinHjxsUJJ5wQt99+exx11FGrXbZz587FXu/GjRvH9ttvH//4xz9Wu860adPi97//fXTu3DkKCgqiTZs2sf/++8crr7xSYtmV76c333yz1LEGDBhQarhRVFQU11xzTfTq1StatGgR+fn50b59+9h3333j7rvvjuXLl2eW/fzzz4vtw8+niy++OOVo/d9xGDBgwC8uB5CL9O7crm1NlKd39+nTp9Tnb7rppsx7YdX+u/KPJ6ubZs6cGbvuumvqMiunv/71r8VqKW3aa6+9StT2yiuvxAEHHBDrrbdeFBQUROfOneP444+PadOmlVj25/XWr18/OnfuHCeddFL88MMPZTvAAADklHrZLgAq03nnnReFhYWxdOnSmDlzZrzwwgsxbNiwGD16dDz22GPRo0ePzLJnn312nHnmmWUa/+uvv45zzz03OnfuHFtuueUar/fMM8+UaTvlkVbbTTfdFCtWrKjyGiri+eefj//3//5fjBw5co2W33LLLePUU0+NiIgZM2bEzTffHIMGDYqioqI49thjiy37yiuvxN577x0REcccc0xsuummMXPmzBg3blzsvPPOcdVVV8Uf//jHCtU/a9as6NevX0yaNCn69u0bZ599dqy77roxc+bMePbZZ+OII46IKVOmxDnnnFNsvcMPPzxT26q22mqrCtUDsLbQu2tP727QoEFMmDAhZs6cGW3bti323J133hkNGjSIJUuWlLru9ddfH02aNCkxf5111ok///nPccwxx2TmvfHGG3H11VfHWWedFd26dcvMX/W9tOp5xKrat29f7PE111wTJ598cmy44Ybxxz/+Mdq1axcffvhh3HzzzXHvvffGE088ETvttNNq6124cGE899xzcc0118Rbb70VL7/88mqODgAAuU6QTo3Sr1+/2HbbbTOPR4wYEc8//3wMGDAg9t133/jwww+jYcOGERFRr169qFevan8EFi1aFI0aNYr8/Pwq3c4vqV+/fla3vya+/fbb2HTTTdd4+fXXXz+OPPLIzOPBgwfHhhtuGFdeeWWxIH3OnDlx0EEHRcOGDeOVV16JLl26ZJ4bPnx49O3bN4YNGxbbbLNNqR+E19RRRx0Vb7/9djz44INx4IEHFntuxIgR8eabb8bkyZNLrLf11lsX2w+A2kbvLl1N7N09e/aMN954I+699944+eSTM/O//PLLeOmll+KAAw6IBx98sNR1DzrooGjVqlWpz+2xxx7FHjdo0CCuvvrq2GOPPYrdnmdVPz+PKM0rr7wSw4YNi169esVTTz0VjRo1yjx3wgknRM+ePeOggw6KDz74IFq0aLHaeo8//vg47LDD4t57743XX389tt9++9TtAgCQm9zahRrv17/+dZxzzjnxxRdfxB133JGZX9p9VsePHx+9evWKddZZJ5o0aRIbb7xxnHXWWRHx071Rt9tuu4iIGDJkSOZfdseNGxcRP91LtXv37jFp0qTYZZddolGjRpl1f36f1ZWWL18eZ511VrRt2zYaN24c++67b0yfPr3YMp07d47BgweXWHfVMX+pttLus7pw4cI49dRTo0OHDlFQUBAbb7xxXH755ZEkSbHl8vLy4sQTT4xHHnkkunfvHgUFBbHZZpvFU089VfoB/5lvv/02jj766FhvvfWiQYMGscUWW8Rtt92WeX7lPWenTp0a//znPzO1f/7552s0/kqtW7eOTTbZJD799NNi82+88caYOXNmXHbZZcVC9IiIhg0bxm233RZ5eXlx3nnnlWl7q3r11Vfj6aefjuOOO65EiL7StttuGwMHDiz3NgBqE727ZvbuBg0axIEHHhh33XVXsfl33313tGjRIvr27btG9VWX888/P/Ly8uK2224rFqJHRHTp0iUuvfTSmDFjRtx4442/ONbOO+8cEVHiPAUAgLWHIJ1aYeU9O9P+TfuDDz6IAQMGRFFRUZx33nlxxRVXxL777pu5h3a3bt0yYetxxx0Xt99+e9x+++2xyy67ZMb47rvvol+/frHlllvGmDFjYrfddkut68ILL4x//vOfccYZZ8RJJ50U48ePjz59+sTixYvLtH9rUtuqkiSJfffdN6688srYa6+9YvTo0bHxxhvHaaedFsOHDy+x/Msvvxx/+MMf4rDDDotLL700lixZEr/5zW/iu+++S61r8eLFseuuu8btt98eAwcOjMsuuyyaN28egwcPjquuuipT++233x6tWrWKLbfcMlN769aty3QMli1bFl9++WWJK8L+53/+Jxo0aBCHHHJIqesVFhZGr1694vnnny/zcV91GxFRrivLFy1aFLNnzy4xLVu2rFy1ANQUendxNaV3H3HEEfH6668XC5TvuuuuOOigg1Kvwv/+++9L9MqK3HN86dKlpfbfla/jokWL4rnnnoudd945CgsLSx3j0EMPjYKCgnj88cd/cXsr/8jw8/MUAADWIgnUAGPHjk0iInnjjTdWu0zz5s2TrbbaKvN45MiRyao/AldeeWUSEcmsWbNWO8Ybb7yRREQyduzYEs/17t07iYjkhhtuKPW53r17Zx5PmDAhiYhk/fXXT+bNm5eZf9999yURkVx11VWZeZ06dUoGDRr0i2Om1TZo0KCkU6dOmcePPPJIEhHJBRdcUGy5gw46KMnLy0umTJmSmRcRSX5+frF5//73v5OISK655poS21rVmDFjkohI7rjjjsy8H3/8Mdlxxx2TJk2aFNv3Tp06Jf37908db9Vl99xzz2TWrFnJrFmzkvfeey856qijkohIhg4dWmzZddZZJ9liiy1SxzvppJOSiEjefffdJEl++f3Uv3//YsfzgAMOSCIi+eGHH4ott3jx4kyNs2bNSubMmZN5burUqUlErHZ69dVX1+g4rOkxA8g1enft6939+/dPli1blrRt2zY5//zzkyRJkv/85z9JRCQvvvhiqe+Jla95adPGG29c6rbuv//+JCKSCRMmrLaW1Y05atSoJEmS5J133kkiIjn55JNT96tHjx7JuuuuW6LeyZMnJ7NmzUo+//zz5NZbb00aNmyYtG7dOlm4cOEaHS8AAHKPK9KpNZo0aRLz589f7fPrrLNOREQ8+uij5f5yr4KCghgyZMgaL//b3/42mjZtmnl80EEHRbt27eKJJ54o1/bX1BNPPBF169aNk046qdj8U089NZIkiSeffLLY/D59+hS7LUqPHj2iWbNm8dlnn/3idtq2bRuHH354Zl79+vXjpJNOigULFsSLL75Y7n145plnonXr1tG6devYfPPN4/bbb48hQ4bEZZddVmy5+fPnFzvGpVn5/Lx588pVy8r1fv4laDfccEOmxtatW0evXr1KrHvcccfF+PHjS0xluecsQE2ld/+fmtC7IyLq1q0bhxxySNx9990R8dOXjHbo0CFz65PVefDBB0v0yrFjx5a7jh122KHU/rtyv1e+79bkHKK084eNN944WrduHZ07d47f/e53sdFGG8WTTz5Z4hYxAACsPXzZKLXGggULok2bNqt9/tBDD42bb745jjnmmDjzzDNj9913jwMPPDAOOuigqFNnzf7mtP7665fpy8m6du1a7HFeXl5stNFGZb4/eFl98cUX0b59+xIfDrt165Z5flUdO3YsMUaLFi1izpw5v7idrl27ljh+q9tOWeywww5xwQUXxPLly+P999+PCy64IObMmVPi+Ddt2jQ1hIlY8w/Lq1r1Hr0r11uwYEE0b948M/83v/lNdO/ePSJ+CjqWL19eYpyuXbtGnz59VruduXPnFrtdQH5+fqy77rprXCfA2kzv/j81oXevdMQRR8TVV18d//73v+Ouu+6Kww47rMS9739ul112We2XjZZHq1atUvvvyuO8JucQpZ0/PPjgg9GsWbOYNWtWXH311TF16tTMl+YCALB2ckU6tcKXX34Zc+fOjY022mi1yzRs2DAmTpwYzz77bBx11FHx7rvvxqGHHhp77LFHqQHo6saobKv7YLmmNVWGunXrljo/+dmXm1WnlR+A+/btG6eeemrccccd8cgjj2Tu37pSt27dYvLkyVFUVLTasd59992oX79+Jhxp0KBBRMRq73e7aNGizDIREZtssklERLz//vvFluvQoUP06dMn+vTpU+57op588snRrl27zLS6LzMFqGn07orJxd690g477BBdunSJYcOGxdSpU+OII47IdkklbLTRRlGvXr149913V7tMUVFRTJ48udT/Ittll12iT58+cfjhh8f48eOjYcOGMXDgwHL/5wQAANknSKdWuP322yMiom/fvqnL1alTJ3bfffcYPXp0/Oc//4kLL7wwnn/++ZgwYUJErP6DcXl98sknxR4nSRJTpkyJzp07Z+a1aNGi1C/T+vkVYWWprVOnTvH111+XuMrqo48+yjxfGTp16hSffPJJiQ+Nlb2diIj+/ftH796946KLLoqFCxdm5g8YMCCWLFkS999/f6nrff755/HSSy/Fr3/960yYsrKuyZMnl7rOxx9/XKz2AQMGRMRP/55e2U4//fRi/3J+xRVXVPo2AHKR3l1cTevdhx9+eLzwwgvRrVu32HLLLStlzMrUuHHj2G233WLixImrvQr/vvvui6Kiosx5wOo0adIkRo4cGe+8807cd999VVEuAADVQJBOjff888/H+eefH4WFhTFw4MDVLvf999+XmLfyg93Kq5kbN24cEVHqh+Py+Mc//lHsA/EDDzwQM2bMiH79+mXmdenSJV577bX48ccfM/Mef/zxmD59erGxylLb3nvvHcuXL49rr7222Pwrr7wy8vLyim2/Ivbee++YOXNm3HvvvZl5y5Yti2uuuSaaNGkSvXv3rpTtrHTGGWfEd999FzfddFNm3vHHHx9t2rSJ0047rcR9YZcsWRJDhgyJJEniL3/5S2b+NttsE23atImbb765xJXsjzzySHz11VfFjlHPnj1jjz32iL///e/x6KOPllpbea8A3HTTTTNXtffp0ye22Wabco0DsDbRu0uqab37mGOOiZEjR+b0H4jPPvvsSJIkBg8eXOK/1KZOnRqnn356tGvXLo4//vhfHGvgwIGxwQYbxCWXXFJV5QIAUMXcI50a5cknn4yPPvooli1bFt988008//zzMX78+OjUqVM89thjxW7H8XPnnXdeTJw4Mfr37x+dOnWKb7/9Nv72t7/FBhtskPmSyC5dusQ666wTN9xwQzRt2jQaN24cO+ywQxQWFpar3nXXXTd69eoVQ4YMiW+++SbGjBkTG220URx77LGZZY455ph44IEHYq+99opDDjkkPv3007jjjjuKfYFYWWvbZ599Yrfddos///nP8fnnn8cWW2wRzzzzTDz66KMxbNiwEmOX13HHHRc33nhjDB48OCZNmhSdO3eOBx54IF555ZUYM2ZMme5Jvib69esX3bt3j9GjR8fQoUOjfv360bJly3jggQeif//+sfXWW8cxxxwTm266acycOTPGjRsXU6ZMiauuuip22mmnzDj5+flx+eWXx6BBg2K77baLQw89NFq2bBlvv/123HrrrdGjR4847rjjim37jjvuiL322iv233//6NevX+Z2LjNnzoxnn302Jk6cWGrI8dZbb8Udd9xRYn6XLl1ixx13/MV9njJlSlxwwQUl5m+11VbRv3//NTlsAFmld9fO3t2pU6f461//usbLP/DAAyW+1DsiYo899oj11luvzNv/6quvSu2/TZo0if333z8ifro9y+WXXx7Dhw+PHj16xODBg6Ndu3bx0UcfxU033RQrVqyIJ554Yo1u31a/fv04+eST47TTTounnnoq9tprrzLXDABAliVQA4wdOzaJiMyUn5+ftG3bNtljjz2Sq666Kpk3b16JdUaOHJms+iPw3HPPJfvtt1/Svn37JD8/P2nfvn1y+OGHJx9//HGx9R599NFk0003TerVq5dERDJ27NgkSZKkd+/eyWabbVZqfb1790569+6deTxhwoQkIpK77747GTFiRNKmTZukYcOGSf/+/ZMvvviixPpXXHFFsv766ycFBQVJz549kzfffLPEmGm1DRo0KOnUqVOxZefPn5+ccsopSfv27ZP69esnXbt2TS677LJkxYoVxZaLiGTo0KElaurUqVMyaNCgUvd3Vd98800yZMiQpFWrVkl+fn6y+eabZ+r6+Xj9+/f/xfF+adlx48YV2/eVpk6dmhx77LFJx44dk/r16yetWrVK9t133+Sll15a7XaefPLJZLfddkuaNWuW1K9fPyksLEyGDx+ezJkzp9TlFy9enIwZMybZcccdk2bNmiX16tVL2rZtmwwYMCC58847k2XLlhWrZ9X37M+nNTm2nTp1Wu36Rx999C+uD5BNend6bbWpd6+08j3xxhtvZOatfM1XN02YMKHEOPfff/9qn1tZy+rG+/kxT5IkmThxYrLffvslrVq1SurXr5907NgxOfbYY5PPP/+8xLIr6501a1aJ5+bOnZs0b968xHsAAIC1Q16S5MA3DgEAAAAAQI5yj3QAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEhRL9sFVLUVK1bE119/HU2bNo28vLxslwMAFZYkScyfPz/at28fderUjL+J69cA1DQ1sV8DQG1W44P0r7/+Ojp06JDtMgCg0k2fPj022GCDbJdRKfRrAGqqmtSvAaA2q/FBetOmTSPip5OXZs2aZbkaAKi4efPmRYcOHTI9ribQrwGoaWpivwaA2qzGB+kr/z28WbNmPpgDUKPUpFug6NcA1FQ1qV8DQG3mRm0AAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApcj5I79y5c+Tl5ZWYhg4dmu3SAID/0q8BAACoyeplu4Bf8sYbb8Ty5cszj99///3YY4894uCDD85iVQDAqvRrAAAAarKcD9Jbt25d7PHFF18cXbp0id69e2epIgDg5/RrAAAAarKcD9JX9eOPP8Ydd9wRw4cPj7y8vFKXKSoqiqKioszjefPmVVd5ANQg06ZNi9mzZ1faeK1atYqOHTtW2ni5TL8GoLro1wBAdVmrgvRHHnkkfvjhhxg8ePBqlxk1alSce+651VcUADXOtGnTYpNu3WLxokWVNmbDRo3iow8/rBUfzvVrAKqDfg0AVKe8JEmSbBexpvr27Rv5+fnxP//zP6tdprQr3Dp06BBz586NZs2aVUeZAKzl3nrrrdhmm23ikAuujzaFXSs83rdTP4n7zj4hJk2aFFtvvXWFx5s3b140b948Z3ubfg1AddCvAYDqtNZckf7FF1/Es88+Gw899FDqcgUFBVFQUFBNVQFQk7Up7Brrd9si22WsVfRrAKqbfg0AVIc62S5gTY0dOzbatGkT/fv3z3YpAMBq6NcAAADURGtFkL5ixYoYO3ZsDBo0KOrVW2suogeAWkW/BgAAoKZaK4L0Z599NqZNmxa/+93vsl0KALAa+jUAAAA11Vpxudiee+4Za9F3ogJAraRfAwAAUFOtFVekAwAAAABAtgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABS5HyQ/tVXX8WRRx4ZLVu2jIYNG8bmm28eb775ZrbLAgBWoV8DAABQk9XLdgFp5syZEz179ozddtstnnzyyWjdunV88skn0aJFi2yXBgD8l34NAABATZfTQfoll1wSHTp0iLFjx2bmFRYWZrEiAODn9GsAAABqupy+tctjjz0W2267bRx88MHRpk2b2GqrreKmm25KXaeoqCjmzZtXbAIAqo5+DQAAQE2X00H6Z599Ftdff3107do1nn766TjhhBPipJNOittuu22164waNSqaN2+emTp06FCNFQNA7aNfAwAAUNPldJC+YsWK2HrrreOiiy6KrbbaKo477rg49thj44YbbljtOiNGjIi5c+dmpunTp1djxQBQ++jXAAAA1HQ5HaS3a9cuNt1002LzunXrFtOmTVvtOgUFBdGsWbNiEwBQdfRrAAAAarqcDtJ79uwZkydPLjbv448/jk6dOmWpIgDg5/RrAAAAarqcDtJPOeWUeO211+Kiiy6KKVOmxF133RV///vfY+jQodkuDQD4L/0aAACAmi6ng/TtttsuHn744bj77ruje/fucf7558eYMWNi4MCB2S4NAPgv/RoAAICarl62C/glAwYMiAEDBmS7DAAghX4NAABATZbTV6QDAAAAAEC2CdIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB2A/9/enUdJUZ77A3+GZYbFAZUdBUUWNSqu0YC7YlC8XtQkoqIiksWICQouIeYGjQtqlOiNAl6DkNwkokZMcozBhUiMUWNEiRuigAGMg4gYNmVApn5/eJ0fI1AzDd3TTc/nc06fM11T/dbz9vZUf091NQAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApCjpIv/rqq6OkpKTGZa+99sp3WQDA5+jZAAAAFLMm+S6gNvvss0888cQT1debNCn4kgGgQdKzAQAAKFYF/wm3SZMm0bFjx3yXAQDUQs8GAACgWBX0qV0iIt56663o3Llz7LHHHjF48OBYtGhRvksCADZDzwYAAKBYFfQR6YcddlhMmTIl9txzz6ioqIhrrrkmjjzyyHj11VejvLx8s7eprKyMysrK6usrV66sr3IBoMHKtGfr1wAAAGxPCjpIP+mkk6r/7t27dxx22GGx2267xf333x/Dhg3b7G3Gjh0b11xzTX2VCABE5j1bvwYAAGB7UvCndtnYjjvuGL169Yp58+ZtcZ3Ro0fHihUrqi+LFy+uxwoBgIjae7Z+DQAAwPZkuwrSV69eHfPnz49OnTptcZ2ysrJo1apVjQsAUL9q69n6NQAAANuTgg7SL7vssvjzn/8c//znP+OZZ56J0047LRo3bhxnnXVWvksDADaiZwMAAFDMCvoc6e+8806cddZZ8cEHH0S7du3iiCOOiOeeey7atWuX79IAgI3o2QAAABSzgg7Sp06dmu8SAIA60LMBAAAoZgV9ahcAAAAAAMg3QToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABACkE6AAAAAACkEKQDAAAAAEAKQToAAAAAAKQQpAMAAAAAQApBOgAAAAAApBCkAwAAAABAipwF6QsWLMjV0ABAlujXAAAAULucBek9evSIY489Nn75y1/G2rVrc7UZAGAb6NcAAABQu5wF6S+++GL07t07Ro4cGR07doxvfetb8fzzz+dqcwDAVtCvAQAAoHY5C9IPOOCAuP322+Pdd9+Ne+65JyoqKuKII46IfffdN8aNGxfvv/9+rjYNANSRfg0AAAC1y/mPjTZp0iROP/30eOCBB+Kmm26KefPmxWWXXRZdunSJ8847LyoqKnJdAgBQC/0aAAAAtiznQfoLL7wQF110UXTq1CnGjRsXl112WcyfPz8ef/zxePfdd2PgwIG5LgEAqIV+DQAAAFvWJFcDjxs3LiZPnhxz586NAQMGxC9+8YsYMGBANGr0aXbfrVu3mDJlSuy+++65KgEAqIV+DQAAALXLWZA+YcKEuOCCC+L888+PTp06bXad9u3bx6RJk3JVAgBQC/0aAAAAapezIP2tt96qdZ3S0tIYMmRIrkoAAGqhXwMAAEDtcnaO9MmTJ8cDDzywyfIHHnggfv7zn+dqswBABvRrAAAAqF3OgvSxY8dG27ZtN1nevn37uOGGG7ZqzBtvvDFKSkrikksu2cbqAIAI/RoAAADqImdB+qJFi6Jbt26bLN9tt91i0aJFGY/397//Pe66667o3bt3NsoDAEK/BgAAgLrIWZDevn37ePnllzdZ/o9//CPatGmT0VirV6+OwYMHx9133x077bRTtkoEgAZPvwYAAIDa5SxIP+uss+K73/1uPPnkk7Fhw4bYsGFD/OlPf4oRI0bEmWeemdFYw4cPj5NPPjn69euXo2oBoGHSrwEAAKB2TXI18LXXXhv//Oc/4/jjj48mTT7dTFVVVZx33nkZnXN16tSp8eKLL8bf//73Oq1fWVkZlZWV1ddXrlyZWeEA0IDo1wAAAFC7nAXppaWlcd9998W1114b//jHP6J58+ax3377xW677VbnMRYvXhwjRoyIxx9/PJo1a1an24wdOzauueaarS0bABoU/RoAAABql7Mg/TO9evWKXr16bdVtZ82aFUuXLo2DDjqoetmGDRviqaeeijvuuCMqKyujcePGNW4zevToGDlyZPX1lStXRpcuXbaueABoIPRrAAAA2LKcBekbNmyIKVOmxIwZM2Lp0qVRVVVV4/9/+tOfah3j+OOPj1deeaXGsqFDh8Zee+0VV1555SYfyiMiysrKoqysbNuKB4AGQr8GAACA2uUsSB8xYkRMmTIlTj755Nh3332jpKQk4zHKy8tj3333rbGsZcuW0aZNm02WAwCZ068BAACgdjkL0qdOnRr3339/DBgwIFebAAC2kX4NAAAAtcvpj4326NEj6+POnDkz62MCQEOlXwMAAEDtGuVq4FGjRsXtt98eSZLkahMAwDbSrwEAAKB2OTsi/emnn44nn3wy/vjHP8Y+++wTTZs2rfH/adOm5WrTAEAd6dcAAABQu5wF6TvuuGOcdtppuRoeAMgC/RoAAABql7MgffLkybkaGgDIEv0aAAAAapezc6RHRHzyySfxxBNPxF133RWrVq2KiIh33303Vq9encvNAgAZ0K8BAAAgXc6OSF+4cGGceOKJsWjRoqisrIwTTjghysvL46abborKysqYOHFirjYNANSRfg0AAAC1y9kR6SNGjIhDDjkkPvzww2jevHn18tNOOy1mzJiRq80CABnQrwEAAKB2OTsi/S9/+Us888wzUVpaWmP57rvvHv/6179ytVkAIAP6NQAAANQuZ0ekV1VVxYYNGzZZ/s4770R5eXmuNgsAZEC/BgAAgNrlLEj/8pe/HLfddlv19ZKSkli9enWMGTMmBgwYkKvNAgAZ0K8BAACgdjk7tcutt94a/fv3jy984Quxdu3aOPvss+Ott96Ktm3bxr333purzQIAGdCvAQAAoHY5C9J33XXX+Mc//hFTp06Nl19+OVavXh3Dhg2LwYMH1/gxMwAgf/RrAAAAqF3OgvSIiCZNmsQ555yTy00AANtIvwYAAIB0OQvSf/GLX6T+/7zzzsvVpgGAOtKvAQAAoHY5C9JHjBhR4/r69evjo48+itLS0mjRooUP5gBQAPRrAAAAqF2jXA384Ycf1risXr065s6dG0cccYQfLwOAAqFfAwAAQO1yFqRvTs+ePePGG2/c5Og3AKBw6NcAAABQU70G6RGf/qDZu+++W9+bBQAyoF8DAADA/5ezc6T//ve/r3E9SZKoqKiIO+64Iw4//PBcbRYAyIB+DQAAALXLWZB+6qmn1rheUlIS7dq1i+OOOy5uvfXWXG0WAMiAfg0AAAC1y1mQXlVVlauhAYAs0a8BAACgdvV+jnQAAAAAANie5OyI9JEjR9Z53XHjxuWqDAAghX4NAAAAtctZkP7SSy/FSy+9FOvXr48999wzIiLefPPNaNy4cRx00EHV65WUlOSqBACgFvo1AAAA1C5nQfopp5wS5eXl8fOf/zx22mmniIj48MMPY+jQoXHkkUfGqFGjcrVpAKCO9GsAAACoXc7OkX7rrbfG2LFjqz+UR0TstNNOcd1118Wtt96aq80CABnQrwEAAKB2OQvSV65cGe+///4my99///1YtWpVrjYLAGRAvwYAAIDa5SxIP+2002Lo0KExbdq0eOedd+Kdd96JBx98MIYNGxann356rjYLAGRAvwYAAIDa5ewc6RMnTozLLrsszj777Fi/fv2nG2vSJIYNGxY//vGPc7VZACAD+jUAAADULmdBeosWLWL8+PHx4x//OObPnx8REd27d4+WLVvmapMAQIb0awAAAKhdzk7t8pmKioqoqKiInj17RsuWLSNJklxvEgDIkH4NAAAAW5azIP2DDz6I448/Pnr16hUDBgyIioqKiIgYNmxYjBo1KlebBQAyoF8DAABA7XIWpF966aXRtGnTWLRoUbRo0aJ6+aBBg2L69Om52iwAkAH9GgAAAGqXs3OkP/bYY/Hoo4/GrrvuWmN5z549Y+HChbnaLACQAf0aAAAAapezI9LXrFlT48i2zyxfvjzKyspytVkAIAP6NQAAANQuZ0H6kUceGb/4xS+qr5eUlERVVVXcfPPNceyxx+ZqswBABvRrAAAAqF3OTu1y8803x/HHHx8vvPBCrFu3Lq644op47bXXYvny5fHXv/41V5sFADKgXwMAAEDtcnZE+r777htvvvlmHHHEETFw4MBYs2ZNnH766fHSSy9F9+7dc7VZACAD+jUAAADULidHpK9fvz5OPPHEmDhxYlx11VW52AQAsI30awAAAKibnByR3rRp03j55ZdzMTQAkCX6NQAAANRNzk7tcs4558SkSZNyNTwAkAX6NQAAANQuZz82+sknn8Q999wTTzzxRBx88MHRsmXLGv8fN25crjYNANSRfg0AAAC1y3qQvmDBgth9993j1VdfjYMOOigiIt58880a65SUlGR7swBABvRrAAAAqLusB+k9e/aMioqKePLJJyMiYtCgQfHf//3f0aFDh2xvCgDYSvo1AAAA1F3Wz5GeJEmN63/84x9jzZo12d4MALAN9GsAAACou5z92OhnPv9BPRMTJkyI3r17R6tWraJVq1bRp0+f+OMf/5jF6gCAiG3r1xF6NgAAAMUt60F6SUnJJudU3dpzrO66665x4403xqxZs+KFF16I4447LgYOHBivvfZaNkoFgAYrm/06Qs8GAACguGX9HOlJksT5558fZWVlERGxdu3auPDCC6Nly5Y11ps2bVqtY51yyik1rl9//fUxYcKEeO6552KfffbJXtEA0MBks19H6NkAAAAUt6wH6UOGDKlx/ZxzzsnKuBs2bIgHHngg1qxZE3369MnKmADQUOWqX0fo2QAAABSfrAfpkydPzup4r7zySvTp0yfWrl0bO+ywQzz00EPxhS98YYvrV1ZWRmVlZfX1lStXZrUeACgG2e7XEZn1bP0aAACA7UnOf2x0W+25554xe/bs+Nvf/hbf/va3Y8iQIfH6669vcf2xY8dG69atqy9dunSpx2oBoOHKpGfr1wAAAGxPCj5ILy0tjR49esTBBx8cY8eOjf333z9uv/32La4/evToWLFiRfVl8eLF9VgtADRcmfRs/RoAAIDtSdZP7ZJrVVVVNb4K/nllZWXVP5wGAORPWs/WrwEAANieFHSQPnr06DjppJOia9eusWrVqvj1r38dM2fOjEcffTTfpQEAG9GzAQAAKGYFHaQvXbo0zjvvvKioqIjWrVtH796949FHH40TTjgh36UBABvRswEAAChmBR2kT5o0Kd8lAAB1oGcDAABQzAr+x0YBAAAAACCfBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkE6QAAAAAAkEKQDgAAAAAAKQTpAAAAAACQQpAOAAAAAAApBOkAAAAAAJBCkA4AAAAAACkKOkgfO3ZsfPGLX4zy8vJo3759nHrqqTF37tx8lwUAfI6eDQAAQDEr6CD9z3/+cwwfPjyee+65ePzxx2P9+vXx5S9/OdasWZPv0gCAjejZAAAAFLMm+S4gzfTp02tcnzJlSrRv3z5mzZoVRx11VJ6qAgA+T88GAACgmBX0Eemft2LFioiI2HnnnfNcCQCQRs8GAACgmBT0Eekbq6qqiksuuSQOP/zw2Hfffbe4XmVlZVRWVlZfX7lyZX2URx4tWrQoli1blrXx2rZtG127ds3aeGy9Qn9ss11fZWVllJWVZW08z2XypS49uz76daG/h1DcPP+Kl8cWAKBh2m6C9OHDh8err74aTz/9dOp6Y8eOjWuuuaaeqiLfFi1aFHvtvXd8/NFHWRuzeYsW8cacOT7Q5FmhP7a5qK+kUaNIqqqyNp7nMvlSl56d635d6O8hFDfPv+LlsQUAaLi2iyD94osvjocffjieeuqp2HXXXVPXHT16dIwcObL6+sqVK6NLly65LpE8WbZsWXz80UdxxnUTon23nts83tK334r7f/DtWLZsmQ8zeVboj22265v71xnx+PixBTtfqKu69uxc9+tCfw+huHn+FS+PLQBAw1XQQXqSJPGd73wnHnrooZg5c2Z069at1tuUlZVl9dQIbB/ad+sZu+y9f77LIAcK/bHNVn1L334rq+NBfcu0Z9dXv/aaIp88/4qXxxYAoOEp6CB9+PDh8etf/zp+97vfRXl5eSxZsiQiIlq3bh3NmzfPc3UAwGf0bAAAAIpZo3wXkGbChAmxYsWKOOaYY6JTp07Vl/vuuy/fpQEAG9GzAQAAKGYFfUR6kiT5LgEAqAM9GwAAgGJW0EekAwAAAABAvgnSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSFHyQ/tRTT8Upp5wSnTt3jpKSkvjtb3+b75IAgM/RrwEAAChmBR+kr1mzJvbff/+48847810KALAF+jUAAADFrEm+C6jNSSedFCeddFK+ywAAUujXAAAAFLOCPyIdAAAAAADyqeCPSM9UZWVlVFZWVl9fuXJl1rexaNGiWLZsWdbGa9u2bXTt2jVr47Ht5syZk7Wxsvn4Zvu5V1lZGWVlZVkbz3OZTHgvbdjqo1/nQqH2h1wo9NdoodfXkBT6/kk2x8vme0CuxvVcBgDIjaIL0seOHRvXXHNNzsZftGhR7LX33vHxRx9lbczmLVrEG3Pm2OEtAKuWvRcljRrFOeeck7Uxs/X45uK5V9KoUSRVVVkbz3OZuvJeSq77dbYVcn/IhUJ/jRZ6fQ3J9rB/ku3xsqmhvbcAAGzPii5IHz16dIwcObL6+sqVK6NLly5ZG3/ZsmXx8UcfxRnXTYj23Xpu83hL334r7v/Bt2PZsmV2dgvAx6tWRlJVVZCPb7afe3P/OiMeHz+2IOdK8fNeSq77dbYVcn/IhUJ/jRZ6fQ1Joe+f5Gq8bGlo7y0AANuzogvSy8rKsvpV0C1p361n7LL3/jnfDvlRyI9vtmpb+vZbWR0PtobnX8NVX/062xrac7bQ51vo9TUkhbp/kqvxss1zGQCg8BV8kL569eqYN29e9fW33347Zs+eHTvvvLOjLACgQOjXAAAAFLOCD9JfeOGFOPbYY6uvf/Y18CFDhsSUKVPyVBUAsDH9GgAAgGJW8EH6McccE0mS5LsMACCFfg0AAEAxa5TvAgAAAAAAoJAJ0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUmwXQfqdd94Zu+++ezRr1iwOO+yweP755/NdEgDwOfo1AAAAxargg/T77rsvRo4cGWPGjIkXX3wx9t9//+jfv38sXbo036UBAP9HvwYAAKCYFXyQPm7cuPjGN74RQ4cOjS984QsxceLEaNGiRdxzzz35Lg0A+D/6NQAAAMWsoIP0devWxaxZs6Jfv37Vyxo1ahT9+vWLZ599No+VAQCf0a8BAAAodk3yXUCaZcuWxYYNG6JDhw41lnfo0CHeeOONzd6msrIyKisrq6+vWLEiIiJWrlyZlZpWr14dERH/mvNyrPtozTaP9/7C+RERMWvWrOqxt1WjRo2iqqoqK2MV+nhz586NiCw+Hv98K7vjZfHxbUhzjcjBfAu9vgJ/PCIK/LVb6I/v/9W3evXqrPSjz8ZIkmSbx8qGBtGvvUa3yfbyGm0o9RX0cyXbr7WGNl6Bv7dkezz9GgCoTyVJAXf1d999N3bZZZd45plnok+fPtXLr7jiivjzn/8cf/vb3za5zdVXXx3XXHNNfZYJAHmxePHi2HXXXfNdhn4NACkKpV8DANumoI9Ib9u2bTRu3Djee++9Gsvfe++96Nix42ZvM3r06Bg5cmT19aqqqli+fHm0adMmSkpKclpvNq1cuTK6dOkSixcvjlatWuW7nKwpxnmZ0/bBnLYfxTivbM8pSZJYtWpVdO7cOQvVbbuG3K/rQzG+JgqF+zZ33Le5477NnWLv1wDAtinoIL20tDQOPvjgmDFjRpx66qkR8ekH7RkzZsTFF1+82duUlZVFWVlZjWU77rhjjivNnVatWhXlDnIxzsuctg/mtP0oxnllc06tW7fOyjjZoF/Xj2J8TRQK923uuG9zx32bO8XarwGAbVPQQXpExMiRI2PIkCFxyCGHxKGHHhq33XZbrFmzJoYOHZrv0gCA/6NfAwAAUMwKPkgfNGhQvP/++/HDH/4wlixZEgcccEBMnz59kx80AwDyR78GAACgmBV8kB4RcfHFF2/xq+HFqqysLMaMGbPJ1963d8U4L3PaPpjT9qMY51WMc9qchtiv60NDef7kg/s2d9y3ueO+zR33LQCQpiRJkiTfRQAAAAAAQKFqlO8CAAAAAACgkAnSAQAAAAAghSAdAAAAAABSCNLz6M4774zdd989mjVrFocddlg8//zzW1x32rRpccghh8SOO+4YLVu2jAMOOCD+93//tx6rrbtM5rWxqVOnRklJSZx66qm5LXArZDKnKVOmRElJSY1Ls2bN6rHausn0cfr3v/8dw4cPj06dOkVZWVn06tUrHnnkkXqqtm4ymdMxxxyzyeNUUlISJ598cj1WXLtMH6fbbrst9txzz2jevHl06dIlLr300li7dm09VVt3mcxr/fr18aMf/Si6d+8ezZo1i/333z+mT59ej9Wme+qpp+KUU06Jzp07R0lJSfz2t7+t9TYzZ86Mgw46KMrKyqJHjx4xZcqUnNdJYcvkNXH33XfHkUceGTvttFPstNNO0a9fvzr32oaoGPdLCkUx7ksUimLt//mmZwMA2yQhL6ZOnZqUlpYm99xzT/Laa68l3/jGN5Idd9wxee+99za7/pNPPplMmzYtef3115N58+Ylt912W9K4ceNk+vTp9Vx5ukzn9Zm333472WWXXZIjjzwyGThwYP0UW0eZzmny5MlJq1atkoqKiurLkiVL6rnqdJnOqbKyMjnkkEOSAQMGJE8//XTy9ttvJzNnzkxmz55dz5VvWaZz+uCDD2o8Rq+++mrSuHHjZPLkyfVbeIpM5/SrX/0qKSsrS371q18lb7/9dvLoo48mnTp1Si699NJ6rjxdpvO64oorks6dOyd/+MMfkvnz5yfjx49PmjVrlrz44ov1XPnmPfLII8lVV12VTJs2LYmI5KGHHkpdf8GCBUmLFi2SkSNHJq+//nry05/+tCDfz6k/mb4mzj777OTOO+9MXnrppWTOnDnJ+eefn7Ru3Tp555136rnywleM+yWFohj3JQpFsfb/QqBnAwDbQpCeJ4ceemgyfPjw6usbNmxIOnfunIwdO7bOYxx44IHJD37wg1yUt9W2Zl6ffPJJ0rdv3+RnP/tZMmTIkIL7wJrpnCZPnpy0bt26nqrbOpnOacKECckee+yRrFu3rr5KzNi2vqZ+8pOfJOXl5cnq1atzVWLGMp3T8OHDk+OOO67GspEjRyaHH354TuvMVKbz6tSpU3LHHXfUWHb66acngwcPzmmdW6MuH8qvuOKKZJ999qmxbNCgQUn//v1zWBmFbFvfvz755JOkvLw8+fnPf56rErdbxbhfUiiKcV+iUBRr/y80ejYAkCmndsmDdevWxaxZs6Jfv37Vyxo1ahT9+vWLZ599ttbbJ0kSM2bMiLlz58ZRRx2Vy1IzsrXz+tGPfhTt27ePYcOG1UeZGdnaOa1evTp222236NKlSwwcODBee+21+ii3TrZmTr///e+jT58+MXz48OjQoUPsu+++ccMNN8SGDRvqq+xU2/qaioiYNGlSnHnmmdGyZctclZmRrZlT3759Y9asWdVf/16wYEE88sgjMWDAgHqpuS62Zl6VlZWbnB6pefPm8fTTT+e01lx59tlna8w/IqJ///51fq5SXLLx/vXRRx/F+vXrY+edd85VmdulYtwvKRTFuC9RKIq1/2+v9GwAYGNN8l1AQ7Rs2bLYsGFDdOjQocbyDh06xBtvvLHF261YsSJ22WWXqKysjMaNG8f48ePjhBNOyHW5dbY183r66adj0qRJMXv27HqoMHNbM6c999wz7rnnnujdu3esWLEibrnllujbt2+89tprseuuu9ZH2am2Zk4LFiyIP/3pTzF48OB45JFHYt68eXHRRRfF+vXrY8yYMfVRdqqtfU195vnnn49XX301Jk2alKsSM7Y1czr77LNj2bJlccQRR0SSJPHJJ5/EhRdeGN///vfro+Q62Zp59e/fP8aNGxdHHXVUdO/ePWbMmBHTpk3bbsOXJUuWbHb+K1eujI8//jiaN2+ep8rIh219/4qIuPLKK6Nz586bhD0NXTHulxSKYtyXKBTF2v+3V3o2ALAxR6RvR8rLy2P27Nnx97//Pa6//voYOXJkzJw5M99lbbVVq1bFueeeG3fffXe0bds23+VkTZ8+feK8886LAw44II4++uiYNm1atGvXLu666658l7bVqqqqon379vE///M/cfDBB8egQYPiqquuiokTJ+a7tKyYNGlS7LfffnHooYfmu5RtMnPmzLjhhhti/Pjx8eKLL8a0adPiD3/4Q1x77bX5Lm2b3H777dGzZ8/Ya6+9orS0NC6++OIYOnRoNGqkhcGNN94YU6dOjYceeqggf9h6e1Ks+yWFotj3JfKpWPs/AEChcUR6HrRt2zYaN24c7733Xo3l7733XnTs2HGLt2vUqFH06NEjIiIOOOCAmDNnTowdOzaOOeaYXJZbZ5nOa/78+fHPf/4zTjnllOplVVVVERHRpEmTmDt3bnTv3j23Rddiax+rjTVt2jQOPPDAmDdvXi5KzNjWzKlTp07RtGnTaNy4cfWyvffeO5YsWRLr1q2L0tLSnNZcm215nNasWRNTp06NH/3oR7ksMWNbM6f/+q//inPPPTe+/vWvR0TEfvvtF2vWrIlvfvObcdVVVxVE8Lw182rXrl389re/jbVr18YHH3wQnTt3ju9973uxxx571EfJWdexY8fNzr9Vq1aObGuAtuX965Zbbokbb7wxnnjiiejdu3cuy9wuFeN+SaEoxn2JQlGs/X97pWcDABuzV5UHpaWlcfDBB8eMGTOql1VVVcWMGTOiT58+dR6nqqoqKisrc1HiVsl0XnvttVe88sorMXv27OrLf/7nf8axxx4bs2fPji5dutRn+ZuVjcdqw4YN8corr0SnTp1yVWZGtmZOhx9+eMybN686UIiIePPNN6NTp04F8cF3Wx6nBx54ICorK+Occ87JdZkZ2Zo5ffTRR5t8WP4ssEiSJHfFZmBbHqtmzZrFLrvsEp988kk8+OCDMXDgwFyXmxN9+vSpMf+IiMcffzyj93+Kx9a+Jm6++ea49tprY/r06XHIIYfUR6nbnWLcLykUxbgvUSiKtf9vr/RsAKCGvP7UaQM2derUpKysLJkyZUry+uuvJ9/85jeTHXfcMVmyZEmSJEly7rnnJt/73veq17/hhhuSxx57LJk/f37y+uuvJ7fcckvSpEmT5O67787XFDYr03l93pAhQ5KBAwfWU7V1k+mcrrnmmuTRRx9N5s+fn8yaNSs588wzk2bNmiWvvfZavqawiUzntGjRoqS8vDy5+OKLk7lz5yYPP/xw0r59++S6667L1xQ2sbXPvSOOOCIZNGhQfZdbJ5nOacyYMUl5eXly7733JgsWLEgee+yxpHv37skZZ5yRrylsVqbzeu6555IHH3wwmT9/fvLUU08lxx13XNKtW7fkww8/zNMMalq1alXy0ksvJS+99FISEcm4ceOSl156KVm4cGGSJEnyve99Lzn33HOr11+wYEHSokWL5PLLL0/mzJmT3HnnnUnjxo2T6dOn52sK5Fmmr4kbb7wxKS0tTX7zm98kFRUV1ZdVq1blawoFqxj3SwpFMe5LFIpi7f+FQM8GALaFID2PfvrTnyZdu3ZNSktLk0MPPTR57rnnqv939NFHJ0OGDKm+ftVVVyU9evRImjVrluy0005Jnz59kqlTp+ah6tplMq/PK9QPrJnM6ZJLLqlet0OHDsmAAQOSF198MQ9Vp8v0cXrmmWeSww47LCkrK0v22GOP5Prrr08++eSTeq46XaZzeuONN5KISB577LF6rrTuMpnT+vXrk6uvvjrp3r170qxZs6RLly7JRRddVDCB88YymdfMmTOTvffeOykrK0vatGmTnHvuucm//vWvPFS9eU8++WQSEZtcPpvDkCFDkqOPPnqT2xxwwAFJaWlpssceeySTJ0+u97opLJm8JnbbbbfNPufGjBlT/4VvB4pxv6RQFOO+RKEo1v6fb3o2ALAtSpLE9/0AAAAAAGBLnCMdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0gEAAAAAIIUgHQAAAAAAUgjSAQAAAAAghSAdAAAAAABSCNIBAAAAACCFIB0AAAAAAFII0qEInX/++VFSUhIlJSXRtGnT6NatW1xxxRWxdu3aGus9/PDDcfTRR0d5eXm0aNEivvjFL8aUKVNqrDNz5swoKSmJf//735tsZ/fdd4/bbrutxrInn3wy/uM//iPatWsXzZo1i+7du8egQYPiqaee2mTMzV2WLFmSOq9TTz0107sDAAreZ737wgsv3OR/w4cPj5KSkjj//PNrrPv5y4knnpjaYz+7zJw5M6ZMmbLZ/zVr1qzGthcvXhwXXHBBdO7cOUpLS2O33XaLESNGxAcffFBjvWOOOabGGL169YqxY8dGkiQ5u88AAKA+CdKhSJ144olRUVERCxYsiJ/85Cdx1113xZgxY6r//9Of/jQGDhwYhx9+ePztb3+Ll19+Oc4888y48MIL47LLLtuqbY4fPz6OP/74aNOmTdx3330xd+7ceOihh6Jv375x6aWXbrL+3Llzo6Kiosalffv2Wz1nANiedenSJaZOnRoff/xx9bK1a9fGr3/96+jatWuNdT/r8xtf7r333ujbt2+NZWecccYm6/bt2zciIlq1arXJGAsXLqzexoIFC+KQQw6Jt956K+69996YN29eTJw4MWbMmBF9+vSJ5cuX16jpG9/4RlRUVMTcuXNj9OjR8cMf/jAmTpyYw3sMAADqT5N8FwDkRllZWXTs2DEiPv1g3q9fv3j88cfjpptuisWLF8eoUaPikksuiRtuuKH6NqNGjYrS0tL47ne/G1/72tfisMMOq/P2Fi1aFJdccklccsklMW7cuBr/6927d3z3u9/d5Dbt27ePHXfccesmCABF5qCDDor58+fHtGnTYvDgwRERMW3atOjatWt069atxrob9/nP23h58+bNo7KycrPrlpSUbHGMiE+PhC8tLY3HHnssmjdvHhERXbt2jQMPPDC6d+8eV111VUyYMKF6/RYtWlSPN3To0Ljjjjvi8ccfj29/+9t1vAcAAKBwOSIdGoBXX301nnnmmSgtLY2IiN/85jexfv36zR55/q1vfSt22GGHuPfeezPaxoMPPhjr16+PK664YrP/LykpybxwAGhgLrjggpg8eXL19XvuuSeGDh1a73UsX748Hn300bjooouqQ/TPdOzYMQYPHhz33XffZk/dkiRJ/OUvf4k33nijet8DAAC2d4J0KFIPP/xw7LDDDtGsWbPYb7/9YunSpXH55ZdHRMSbb74ZrVu3jk6dOm1yu9LS0thjjz3izTffzGh7b775ZrRq1arGkW0PPvhg7LDDDtWXV155pcZtdt111xr/32effbZipgBQPM4555x4+umnY+HChbFw4cL461//Guecc84m633W5ze+bPwts7pYsWLFJmOcdNJJERHx1ltvRZIksffee2/2tnvvvXd8+OGH8f7771cvGz9+fOywww5RVlYWRx11VFRVVW32G2kAALA9cmoXKFLHHntsTJgwIdasWRM/+clPokmTJvGVr3wlp9v8/FHn/fv3j9mzZ8e//vWvOOaYY2LDhg01/v+Xv/wlysvLq683bdq0evlnH+QjIu66667qr7gDQDFr165dnHzyyTFlypRIkiROPvnkaNu27SbrfdbnN7bzzjtntK3y8vJ48cUXayz7/NHnmfxY6ODBg+Oqq66KDz/8MMaMGRN9+/atPh87AABs7wTpUKRatmwZPXr0iIhPvxa+//77x6RJk2LYsGHRq1evWLFiRbz77rvRuXPnGrdbt25dzJ8/P4499tiI+PSHyCI+PWrt8+cz//e//x2tW7eOiIiePXvGihUrYsmSJdVHpe+www7Ro0ePaNJk82813bp12+w50g855JCYPXt29fUOHTpkPH8A2F5dcMEFcfHFF0dExJ133rnZdTbu81urUaNGWxyjR48eUVJSEnPmzInTTjttk//PmTMndtppp2jXrl31statW1ePd//990ePHj3iS1/6UvTr12+b6gQAgELg1C7QADRq1Ci+//3vxw9+8IP4+OOP4ytf+Uo0bdo0br311k3WnThxYqxZsybOOuusiPg0IG/UqFHMmjWrxnoLFiyIFStWRK9evSIi4qtf/Wo0bdo0brrppm2ut3nz5tGjR4/qy8ZHrQNAsTvxxBNj3bp1sX79+ujfv39eamjTpk2ccMIJMX78+Pj4449r/G/JkiXxq1/9KgYNGrTF30DZYYcdYsSIEXHZZZdldFQ7AAAUKkekQwPxta99LS6//PK4884747LLLoubb745Ro0aFc2aNYtzzz03mjZtGr/73e/i+9//fowaNSoOO+ywiPj0a99f//rXY9SoUdGkSZPYb7/9YvHixXHllVfGl770peqvbHft2jVuvfXWGDFiRCxfvjzOP//86NatWyxfvjx++ctfRkRE48aNa9S0dOnSWLt2bY1lbdq0qT7Fy+asWLGixtHqn92mS5cu23oXAUBBaNy4ccyZM6f6782prKyMJUuW1FjWpEmTzZ4GZkuSJNlkjIiI9u3bR6NGjeKOO+6Ivn37Rv/+/eO6666Lbt26xWuvvRaXX3557LLLLnH99denjv+tb30rrr322njwwQfjq1/9ap3rAgCAQiRIhwaiSZMmcfHFF8fNN98c3/72t+OSSy6JPfbYI2655Za4/fbbY8OGDbHPPvvEhAkTYujQoTVue/vtt8eNN94YV155ZSxcuDA6duwYJ5xwQlx//fU1jkT7zne+E3vvvXeMGzcuvvrVr8bKlSujTZs20adPn5g+fXrst99+Ncbdc889N6nz2WefjS996UtbnMfMmTPjwAMPrLFs2LBh8bOf/Wxr7hYAKEifnVptS6ZPn77Jj4bvueee8cYbb9R5GytXrtzsD49XVFREx44do2fPnvHCCy/EmDFj4owzzojly5dHx44d49RTT40xY8bUek72nXfeOc4777y4+uqr4/TTT49GjXwZFgCA7VdJ4ruWAAAAAACwRQ4LAQAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASCFIBwAAAACAFIJ0AAAAAABIIUgHAAAAAIAUgnQAAAAAAEghSAcAAAAAgBSCdAAAAAAASPH/AIu/gWLAdu3jAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "L7EyXM4BH341"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can perform good translations of small sentences with few ambiguities. The model has some difficulties translating words like “at”, as they can have different translations and meanings in French. METEOR score is slight above 0.65 which indicates that the machine-generated translation is fairly close to alignment with human reference translations. However BLEU indicates that the translations are not so similar to the references."
      ],
      "metadata": {
        "id": "mD0BFuSXH58X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Conclusion"
      ],
      "metadata": {
        "id": "mJS7OsHXKoEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several types of architecture for translation. RNN does not seem to be sufficiently efficient for translation. GRU is an improvement on RNN, but still fails to generalize well. Transformers have potential for translation. But with a larger data set, they could lead to better performance."
      ],
      "metadata": {
        "id": "boMh0aq4KqdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Not Part of TP3, But A Potential Project Idea: Understanding the Architecture of a Decoder-Only Transformer\n",
        "\n",
        "Step 1: In a project group of 3-4 create a high level plan for a Decoder-Only model for how you would need to modify this code to implement a Decoder-Only Transformer. Key components of the implementation should be split up and each member of the group should present the pseudo-code (or actual code) for one component of the full model to one another, and in a report. Then create the working model and perform experiments comparing it with your TP3 encoder-decoder model.\n",
        "\n",
        "For more details on the Decoder-Only Transformer see [this blog post](https://medium.com/international-school-of-ai-data-science/building-custom-gpt-with-pytorch-59e5ba8102d4). The [first \"GPT\" paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), and the paper cited by this GPT-1 paper for the Decoder Only architecture used for GPT, [i.e. this paper](https://arxiv.org/abs/1801.10198)"
      ],
      "metadata": {
        "id": "U6-OpYG_hz2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxFnLwN1zI-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}